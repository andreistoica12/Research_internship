{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Replace the download directory of the NLTK tokenizer files with your preferred directory (I chose the root directory of the Research Internship project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/andreistoica12/research-internship...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "from sentistrength import PySentiStr\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', download_dir='/home/andreistoica12/research-internship')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "# Load the punkt tokenizer data from the local directory\n",
    "nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = '/home/andreistoica12/research-internship'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where we store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/andreistoica12/research-internship/data/covaxxy-csv-complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_jar = '/home/andreistoica12/research-internship/SentiStrength/SentiStrengthCom.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_language_folder = '/home/andreistoica12/research-internship/SentiStrength/LanguageFolder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_replies_opinion_changes = rootdir_path + '/replies_opinion_changes.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 subfolders to store important files and graphs, respectively. If they already existed (from previous runnings of the project), delete the folders and their contents and create empty folders to store the current files and graphs, relevant to the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = os.path.join(rootdir_path, 'files')\n",
    "if os.path.exists(files_path):\n",
    "   shutil.rmtree(files_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(files_path)\n",
    "\n",
    "graphs_path = os.path.join(rootdir_path, 'graphs')\n",
    "if os.path.exists(graphs_path):\n",
    "   shutil.rmtree(graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_graphs_path = os.path.join(graphs_path, 'covaxxy')\n",
    "if os.path.exists(covaxxy_graphs_path):\n",
    "   shutil.rmtree(covaxxy_graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(covaxxy_graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_longitudinal_analysis_graphs = os.path.join(covaxxy_graphs_path, 'longitudinal-analysis')\n",
    "if os.path.exists(covaxxy_longitudinal_analysis_graphs):\n",
    "   shutil.rmtree(covaxxy_longitudinal_analysis_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(covaxxy_longitudinal_analysis_graphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function remove_emoji() that takes a text string as input and uses a regular expression to match all Unicode characters that are classified as emojis. The regular expression includes different ranges of Unicode characters that represent different types of emojis, such as emoticons, symbols, and flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # # I can use the predefined list of stopwords provided by NLTK, but it's for general purpose\n",
    "    # # and the results when computing the sentiment are worse than expected, e.g. it considers\n",
    "    # # words, such as \"not\" and \"all\" to be stopwords in contexts where they are actually important.\n",
    "    \n",
    "    # nltk.download('stopwords')\n",
    "    # from nltk.corpus import stopwords\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    # To avoid this problem, I defined a custom list of stopwords, which I made sure doesn't contain wrong stopwords.\n",
    "    stop_words = {\"the\", \"and\", \"or\", \"a\", \"an\", \"in\", \"of\", \"on\", \"to\", \"that\", \"this\", \"is\", \"are\", \n",
    "                  \"was\", \"were\", \"am\", \"be\", \"been\", \"has\", \"have\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \n",
    "                  \"should\", \"can\", \"could\", \"may\", \"might\", \"must\", \"shall\", \"shouldn't\", \"wouldn't\", \"couldn't\", \n",
    "                  \"can't\", \"mustn't\", \"haven't\", \"hasn't\", \"hadn't\", \"didn't\", \"doesn't\", \"don't\"}\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove the stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):    \n",
    "    # 1. Lowercase all words in the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace the new line character with empty string\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # 2. Remove words starting with '@' - tags (most common noise in replies)\n",
    "    text = re.sub(r'@\\w+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 3. Remove words starting with 'http' - hyperlinks\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 4. Remove punctuation from the text using regular expressions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    import contractions\n",
    "    # 5. Remove contractions, such as you're => you are\n",
    "    contractions.fix(text)\n",
    "\n",
    "    # 6. Remove emojis\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    # 7. Remove stopwords in English\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_days(data_path):\n",
    "    # A list of the current data files need for my analysis.\n",
    "    file_list = os.listdir(data_path)\n",
    "\n",
    "    # For simplicity's and consistency's sake, I will store all data in chronological order, \n",
    "    # so we sort the list of file names from the start.\n",
    "    file_list.sort(key=lambda date: datetime.strptime(date, \"tweet_ids--%Y-%m-%d.csv\"))\n",
    "\n",
    "    # I parse the date of the tweets from the file names and transform them into datetime objects. \n",
    "    # This makes it easier to get the day/month/year, as they are already properties of such type of objects.\n",
    "    keys_datetime = [ datetime.strptime(key, \"tweet_ids--%Y-%m-%d.csv\") for key in file_list ]\n",
    "\n",
    "    # Ultimately, I will store each .csv file as a pandas DataFrame in a dictionary, where the keys represent a \n",
    "    # simplified form of the date. So, here, I will format the dates from the datetime objects into simple strings.\n",
    "    keys = [ f\"{key.day}-{key.month}-{key.year}\" for key in keys_datetime ]\n",
    "\n",
    "    # In order to read the data from the files, I need the paths of the files to be passed on to the read_csv() function. \n",
    "    # The order of the days in the file paths needs to be consistent with the order of the dates in the keys\n",
    "    paths = [ os.path.join(data_path, file) for file in file_list ]\n",
    "\n",
    "    # Here, I will build the dictionary where the keys represent the formatted simple date and \n",
    "    # the values are dataframes corresponding to each file.\n",
    "    days = dict()\n",
    "    for i in range(len(file_list)):\n",
    "        days[keys[i]] = pd.read_csv(paths[i], index_col=False)\n",
    "        days[keys[i]].drop('id', axis=1, inplace=True)\n",
    "\n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = create_days(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, I merged all data (from all available days) into a single dataframe (they have the same structure).\n",
    "# I did that because some replies to a tweet posted today can come some days after, so we need to take care\n",
    "# of the dataset as a whole.\n",
    "\n",
    "# concatenate the dataframes and reset the index\n",
    "merged_days = pd.concat([df for key, df in days.items()], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string column to datetime\n",
    "merged_days['created_at'] = pd.to_datetime(merged_days['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe based on datetime column\n",
    "# NOTE: I also reset the index so that I know later on which tweet was posted first based on the index (useful for opinion change)\n",
    "merged_days = merged_days.sort_values('created_at').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = merged_days[merged_days['reference_type'] == 'replied_to'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_replies = replies[replies.duplicated(subset=['author_id', 'in_reply_to_tweet_id'], keep=False)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple_replies_first_500 = multiple_replies.head(500).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_replies['in_reply_to_tweet_id'] = multiple_replies['in_reply_to_tweet_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the rows by the two columns\n",
    "grouped_df = multiple_replies.groupby(['author_id', 'in_reply_to_tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_of_replies = grouped_df.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([3424369, 3461244], dtype='int64')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups_of_replies[(5895742, 1368371467805728770)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath(path_to_sentistrength_jar)\n",
    "senti.setSentiStrengthLanguageFolderPath(path_to_sentistrength_language_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counter = 0\n",
    "progress = 0.01\n",
    "def opinion_change(groups_of_replies, group, rows_indices):\n",
    "    global group_counter\n",
    "    global progress\n",
    "    group_counter += 1\n",
    "\n",
    "    if (group_counter / len(groups_of_replies)) >= progress:\n",
    "        print(f\"Progress: {group_counter} / {len(groups_of_replies)} groups of replies processed.\")\n",
    "        progress += 0.01\n",
    "\n",
    "\n",
    "    texts = [ clean_text(replies.loc[index, 'text']) for index in rows_indices ]\n",
    "\n",
    "    sentiments = senti.getSentiment(texts, score='scale')\n",
    "    sentiments = np.array(sentiments)\n",
    "\n",
    "    positive = np.any(sentiments > 0)\n",
    "    negative = np.any(sentiments < 0)\n",
    "\n",
    "    return positive and negative, sentiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE:\n",
    "\n",
    "THERE IS NO NEED TO RUN THE CELLS BETWEEN THE LINES BELOW!!!!!!!\n",
    "It took more than 15 minutes when I ran it on the whole replies dataset...\n",
    "\n",
    "I saved the resulting dictionary into a JSON file, which can be found in the root directory of the project. This can be imported into a dictionary with ease (code can be found in the next parts of the notebook)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 56 / 5587 groups of replies processed.\n",
      "Progress: 112 / 5587 groups of replies processed.\n",
      "Progress: 168 / 5587 groups of replies processed.\n",
      "Progress: 224 / 5587 groups of replies processed.\n",
      "Progress: 280 / 5587 groups of replies processed.\n",
      "Progress: 336 / 5587 groups of replies processed.\n",
      "Progress: 392 / 5587 groups of replies processed.\n",
      "Progress: 447 / 5587 groups of replies processed.\n",
      "Progress: 503 / 5587 groups of replies processed.\n",
      "Progress: 559 / 5587 groups of replies processed.\n",
      "Progress: 615 / 5587 groups of replies processed.\n",
      "Progress: 671 / 5587 groups of replies processed.\n",
      "Progress: 727 / 5587 groups of replies processed.\n",
      "Progress: 783 / 5587 groups of replies processed.\n",
      "Progress: 839 / 5587 groups of replies processed.\n",
      "Progress: 894 / 5587 groups of replies processed.\n",
      "Progress: 950 / 5587 groups of replies processed.\n",
      "Progress: 1006 / 5587 groups of replies processed.\n",
      "Progress: 1062 / 5587 groups of replies processed.\n",
      "Progress: 1118 / 5587 groups of replies processed.\n",
      "Progress: 1174 / 5587 groups of replies processed.\n",
      "Progress: 1230 / 5587 groups of replies processed.\n",
      "Progress: 1286 / 5587 groups of replies processed.\n",
      "Progress: 1341 / 5587 groups of replies processed.\n",
      "Progress: 1397 / 5587 groups of replies processed.\n",
      "Progress: 1453 / 5587 groups of replies processed.\n",
      "Progress: 1509 / 5587 groups of replies processed.\n",
      "Progress: 1565 / 5587 groups of replies processed.\n",
      "Progress: 1621 / 5587 groups of replies processed.\n",
      "Progress: 1677 / 5587 groups of replies processed.\n",
      "Progress: 1732 / 5587 groups of replies processed.\n",
      "Progress: 1788 / 5587 groups of replies processed.\n",
      "Progress: 1844 / 5587 groups of replies processed.\n",
      "Progress: 1900 / 5587 groups of replies processed.\n",
      "Progress: 1956 / 5587 groups of replies processed.\n",
      "Progress: 2012 / 5587 groups of replies processed.\n",
      "Progress: 2068 / 5587 groups of replies processed.\n",
      "Progress: 2124 / 5587 groups of replies processed.\n",
      "Progress: 2179 / 5587 groups of replies processed.\n",
      "Progress: 2235 / 5587 groups of replies processed.\n",
      "Progress: 2291 / 5587 groups of replies processed.\n",
      "Progress: 2347 / 5587 groups of replies processed.\n",
      "Progress: 2403 / 5587 groups of replies processed.\n",
      "Progress: 2459 / 5587 groups of replies processed.\n",
      "Progress: 2515 / 5587 groups of replies processed.\n",
      "Progress: 2571 / 5587 groups of replies processed.\n",
      "Progress: 2626 / 5587 groups of replies processed.\n",
      "Progress: 2682 / 5587 groups of replies processed.\n",
      "Progress: 2738 / 5587 groups of replies processed.\n",
      "Progress: 2794 / 5587 groups of replies processed.\n",
      "Progress: 2850 / 5587 groups of replies processed.\n",
      "Progress: 2906 / 5587 groups of replies processed.\n",
      "Progress: 2962 / 5587 groups of replies processed.\n",
      "Progress: 3017 / 5587 groups of replies processed.\n",
      "Progress: 3073 / 5587 groups of replies processed.\n",
      "Progress: 3129 / 5587 groups of replies processed.\n",
      "Progress: 3185 / 5587 groups of replies processed.\n",
      "Progress: 3241 / 5587 groups of replies processed.\n",
      "Progress: 3297 / 5587 groups of replies processed.\n",
      "Progress: 3353 / 5587 groups of replies processed.\n",
      "Progress: 3409 / 5587 groups of replies processed.\n",
      "Progress: 3464 / 5587 groups of replies processed.\n",
      "Progress: 3520 / 5587 groups of replies processed.\n",
      "Progress: 3576 / 5587 groups of replies processed.\n",
      "Progress: 3632 / 5587 groups of replies processed.\n",
      "Progress: 3688 / 5587 groups of replies processed.\n",
      "Progress: 3744 / 5587 groups of replies processed.\n",
      "Progress: 3800 / 5587 groups of replies processed.\n",
      "Progress: 3856 / 5587 groups of replies processed.\n",
      "Progress: 3911 / 5587 groups of replies processed.\n",
      "Progress: 3967 / 5587 groups of replies processed.\n",
      "Progress: 4023 / 5587 groups of replies processed.\n",
      "Progress: 4079 / 5587 groups of replies processed.\n",
      "Progress: 4135 / 5587 groups of replies processed.\n",
      "Progress: 4191 / 5587 groups of replies processed.\n",
      "Progress: 4247 / 5587 groups of replies processed.\n",
      "Progress: 4302 / 5587 groups of replies processed.\n",
      "Progress: 4358 / 5587 groups of replies processed.\n",
      "Progress: 4414 / 5587 groups of replies processed.\n",
      "Progress: 4470 / 5587 groups of replies processed.\n",
      "Progress: 4526 / 5587 groups of replies processed.\n",
      "Progress: 4582 / 5587 groups of replies processed.\n",
      "Progress: 4638 / 5587 groups of replies processed.\n",
      "Progress: 4694 / 5587 groups of replies processed.\n",
      "Progress: 4749 / 5587 groups of replies processed.\n",
      "Progress: 4805 / 5587 groups of replies processed.\n",
      "Progress: 4861 / 5587 groups of replies processed.\n",
      "Progress: 4917 / 5587 groups of replies processed.\n",
      "Progress: 4973 / 5587 groups of replies processed.\n",
      "Progress: 5029 / 5587 groups of replies processed.\n",
      "Progress: 5085 / 5587 groups of replies processed.\n",
      "Progress: 5141 / 5587 groups of replies processed.\n",
      "Progress: 5196 / 5587 groups of replies processed.\n",
      "Progress: 5252 / 5587 groups of replies processed.\n",
      "Progress: 5308 / 5587 groups of replies processed.\n",
      "Progress: 5364 / 5587 groups of replies processed.\n",
      "Progress: 5420 / 5587 groups of replies processed.\n",
      "Progress: 5476 / 5587 groups of replies processed.\n",
      "Progress: 5532 / 5587 groups of replies processed.\n",
      "Progress: 5588 / 5587 groups of replies processed.\n",
      "Progress: 5643 / 5587 groups of replies processed.\n",
      "Progress: 5699 / 5587 groups of replies processed.\n",
      "Progress: 5755 / 5587 groups of replies processed.\n",
      "Progress: 5811 / 5587 groups of replies processed.\n",
      "Progress: 5867 / 5587 groups of replies processed.\n",
      "Progress: 5923 / 5587 groups of replies processed.\n",
      "Progress: 5979 / 5587 groups of replies processed.\n",
      "Progress: 6034 / 5587 groups of replies processed.\n",
      "Progress: 6090 / 5587 groups of replies processed.\n",
      "Progress: 6146 / 5587 groups of replies processed.\n",
      "Progress: 6202 / 5587 groups of replies processed.\n",
      "Progress: 6258 / 5587 groups of replies processed.\n",
      "Progress: 6314 / 5587 groups of replies processed.\n",
      "Progress: 6370 / 5587 groups of replies processed.\n",
      "Progress: 6426 / 5587 groups of replies processed.\n"
     ]
    }
   ],
   "source": [
    "opinion_changes = { group: opinion_change(groups_of_replies, group, rows_indices)[1].tolist() for group, rows_indices in groups_of_replies.items() \n",
    "                   if opinion_change(groups_of_replies, group, rows_indices)[0] == np.bool_('True') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_opinion_changes_to_JSON(opinion_changes, path):\n",
    "    # create a new dictionary with string keys\n",
    "    opinion_changes_for_JSON_file = {str(key): value for key, value in opinion_changes.items() }\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(opinion_changes_for_JSON_file, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opinion_changes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[211], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_opinion_changes_to_JSON(opinion_changes, path_to_replies_opinion_changes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opinion_changes' is not defined"
     ]
    }
   ],
   "source": [
    "save_opinion_changes_to_JSON(opinion_changes, path_to_replies_opinion_changes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opinion_changes(path_to_replies_opinion_changes):\n",
    "    with open(path_to_replies_opinion_changes) as f:\n",
    "        # Load the JSON data into a Python dictionary\n",
    "        opinion_changes_from_file = json.load(f)\n",
    "        # Create a new dictionary with tuple keys\n",
    "        original_opinion_changes = {}\n",
    "        for key in opinion_changes_from_file:\n",
    "            # Convert the string key to a tuple\n",
    "            new_key = eval(key)\n",
    "            # Add the key-value pair to the new dictionary\n",
    "            original_opinion_changes[new_key] = opinion_changes_from_file[key]\n",
    "            \n",
    "    return original_opinion_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_changes = load_opinion_changes(path_to_replies_opinion_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of opinion changes from the replies: 15.6%.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of opinion changes from the replies: {round(len(opinion_changes) / len(groups_of_replies) * 100, 1)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biggest_opinion_change(opinion_changes):\n",
    "    change_type = 'negative'\n",
    "    biggest_change = 0\n",
    "    target_group = tuple()\n",
    "    for group, sentiments in opinion_changes.items():\n",
    "        change = max(biggest_change, max(sentiments) - min(sentiments))\n",
    "        if change > biggest_change:\n",
    "            biggest_change = change\n",
    "            target_group = group\n",
    "    \n",
    "    min_sentiment_index = opinion_changes[target_group].index(min(opinion_changes[target_group]))\n",
    "    max_sentiment_index = opinion_changes[target_group].index(max(opinion_changes[target_group]))\n",
    "    change_type = 'positive' if min_sentiment_index < max_sentiment_index else change_type\n",
    "\n",
    "    return target_group, change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_group, change_type = biggest_opinion_change(opinion_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replies_with_biggest_opinion_change(replies, target_group):\n",
    "    condition1 = replies['author_id'] == target_group[0]\n",
    "    condition2 = replies['in_reply_to_tweet_id'] == target_group[1]\n",
    "\n",
    "    return replies[condition1 & condition2].loc[:, 'text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_biggest_change = replies_with_biggest_opinion_change(multiple_replies, target_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@KATIEDOLL1201 @daulan @SandySue1958 I've been taking vaccines since I was very young..  I got the vax -my SIL was SORRY she didn't.  My mom and her sis just got both covid shots -NP.  Mom's 90, her sis it is 94.  Grandma was born in 1899, saw the 1st pandemic.. She loved vaccines. She lived to 94, maybe that's why.\",\n",
       " \"@KATIEDOLL1201 @daulan @SandySue1958 Sorry, they're being so hard on you, but the 'majority' of people who get Shingles say it is very painful, and the 'majority' of people who have the vaccine do not have an issue.  That is by the numbers.  And right now there is soo much misinformation being spread- It's just sad.\"]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies_biggest_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_changes_types(opinion_changes):\n",
    "    changes_types = {}\n",
    "    for group in opinion_changes:\n",
    "        min_sentiment_index = opinion_changes[group].index(min(opinion_changes[group]))\n",
    "        max_sentiment_index = opinion_changes[group].index(max(opinion_changes[group]))\n",
    "        \n",
    "        change_type = 'negative'\n",
    "        change_type = 'positive' if min_sentiment_index < max_sentiment_index else change_type\n",
    "        changes_types[group] = change_type\n",
    "        \n",
    "    return changes_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_types = opinion_changes_types(opinion_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_count_in_dict(dict, value_to_count):\n",
    "    # Create a reverse dictionary that maps values to their frequencies\n",
    "    reverse_dict = defaultdict(int)\n",
    "    for value in changes_types.values():\n",
    "        reverse_dict[value] += 1\n",
    "\n",
    "    # Count the occurrences of the specific value\n",
    "    count = reverse_dict.get(value_to_count, 0)\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive opinion changes from the replies: 46.1%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of positive opinion changes from the replies: {round(value_count_in_dict(changes_types, 'positive') / len(changes_types) * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of negative opinion changes from the replies: 53.9%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of negative opinion changes from the replies: {round(value_count_in_dict(changes_types, 'negative') / len(changes_types) * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the distribution of the tweets per hour, I will parse the \"created_at\" column, extract the hour property and create a separate column in each dataframe. I will place it next to the \"created_at\" column in order to be easily verifiable. Data originates frmo the Twitter API, so it comes in a standard ISO 8601 format, which can be easily parsed using the parser module from the dateutil package.\n",
    "\n",
    "Note: the cell below runs for approximately 2m30' on my machine (~25-30 seconds for each file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, day in days.items():\n",
    "#     if 'hour' not in day.columns:\n",
    "#         day.insert(1, 'hour', day['created_at'].apply(lambda date: parser.parse(date).hour))\n",
    "#         print(f\"New 'hour' column inserted in the {key} dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, day in days.items():\n",
    "#     if 'hour' not in day.columns:\n",
    "#         hours = []\n",
    "#         for time in day.loc[:,\"created_at\"]:\n",
    "#             hour = parser.parse(time).hour\n",
    "#             hours.append(hour)\n",
    "#         day.insert(1, \"hour\", hours, True)\n",
    "#         print(key + \" - added 'hour' column\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final distribution is made up of the sum of all individual days' distributions. I save a figure in the graphs/ folder for each day, as well as an overall distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_distribution = pd.Series(0, index=days['1-3-2021'].loc[:,'hour'].sort_values(ascending=True).unique())\n",
    "# for key, day in days.items():\n",
    "#     hour_column_ascending = day.loc[:,\"hour\"].sort_values(ascending=True)\n",
    "#     distribution = hour_column_ascending.value_counts()[hour_column_ascending.unique()]\n",
    "#     final_distribution = final_distribution.add(distribution)\n",
    "#     axes = distribution.plot(kind='bar')\n",
    "#     figure_path = f\"{covaxxy_longitudinal_analysis_graphs}/{key}_distribution.png\"\n",
    "#     axes.figure.savefig(figure_path)\n",
    "#     plt.close()\n",
    "# axes = final_distribution.plot(kind='bar')\n",
    "# figure_path = f\"{covaxxy_longitudinal_analysis_graphs}/overall_distribution.png\"\n",
    "# axes.figure.savefig(figure_path)\n",
    "# plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
