{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Replace the download directory of the NLTK tokenizer files with your preferred directory (I chose the root directory of the Research Internship project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/andreistoica12/research-internship...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "from sentistrength import PySentiStr\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', download_dir='/home/andreistoica12/research-internship')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "# Load the punkt tokenizer data from the local directory\n",
    "nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = '/home/andreistoica12/research-internship'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where we store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/andreistoica12/research-internship/data/covaxxy-csv-complete'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_jar = '/home/andreistoica12/research-internship/SentiStrength/SentiStrengthCom.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_language_folder = '/home/andreistoica12/research-internship/SentiStrength/LanguageFolder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_replies_opinion_changes = rootdir_path + '/replies_opinion_changes.json'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 subfolders to store important files and graphs, respectively. If they already existed (from previous runnings of the project), delete the folders and their contents and create empty folders to store the current files and graphs, relevant to the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = os.path.join(rootdir_path, 'files')\n",
    "if os.path.exists(files_path):\n",
    "   shutil.rmtree(files_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(files_path)\n",
    "\n",
    "graphs_path = os.path.join(rootdir_path, 'graphs')\n",
    "if os.path.exists(graphs_path):\n",
    "   shutil.rmtree(graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_graphs_path = os.path.join(graphs_path, 'covaxxy')\n",
    "if os.path.exists(covaxxy_graphs_path):\n",
    "   shutil.rmtree(covaxxy_graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(covaxxy_graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_longitudinal_analysis_graphs = os.path.join(covaxxy_graphs_path, 'longitudinal-analysis')\n",
    "if os.path.exists(covaxxy_longitudinal_analysis_graphs):\n",
    "   shutil.rmtree(covaxxy_longitudinal_analysis_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(covaxxy_longitudinal_analysis_graphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function remove_emoji() that takes a text string as input and uses a regular expression to match all Unicode characters that are classified as emojis. The regular expression includes different ranges of Unicode characters that represent different types of emojis, such as emoticons, symbols, and flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    # # I can use the predefined list of stopwords provided by NLTK, but it's for general purpose\n",
    "    # # and the results when computing the sentiment are worse than expected, e.g. it considers\n",
    "    # # words, such as \"not\" and \"all\" to be stopwords in contexts where they are actually important.\n",
    "    \n",
    "    # nltk.download('stopwords')\n",
    "    # from nltk.corpus import stopwords\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "    # To avoid this problem, I defined a custom list of stopwords, which I made sure doesn't contain wrong stopwords.\n",
    "    stop_words = {\"the\", \"and\", \"or\", \"a\", \"an\", \"in\", \"of\", \"on\", \"to\", \"that\", \"this\", \"is\", \"are\", \n",
    "                  \"was\", \"were\", \"am\", \"be\", \"been\", \"has\", \"have\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \n",
    "                  \"should\", \"can\", \"could\", \"may\", \"might\", \"must\", \"shall\", \"shouldn't\", \"wouldn't\", \"couldn't\", \n",
    "                  \"can't\", \"mustn't\", \"haven't\", \"hasn't\", \"hadn't\", \"didn't\", \"doesn't\", \"don't\"}\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove the stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):    \n",
    "    # 1. Lowercase all words in the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace the new line character with empty string\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # 2. Remove words starting with '@' - tags (most common noise in replies)\n",
    "    text = re.sub(r'@\\w+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 3. Remove words starting with 'http' - hyperlinks\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 4. Remove punctuation from the text using regular expressions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    import contractions\n",
    "    # 5. Remove contractions, such as you're => you are\n",
    "    contractions.fix(text)\n",
    "\n",
    "    # 6. Remove emojis\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    # 7. Remove stopwords in English\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_days(data_path):\n",
    "    # A list of the current data files need for my analysis.\n",
    "    file_list = os.listdir(data_path)\n",
    "\n",
    "    # For simplicity's and consistency's sake, I will store all data in chronological order, \n",
    "    # so we sort the list of file names from the start.\n",
    "    file_list.sort(key=lambda date: datetime.strptime(date, \"tweet_ids--%Y-%m-%d.csv\"))\n",
    "\n",
    "    # I parse the date of the tweets from the file names and transform them into datetime objects. \n",
    "    # This makes it easier to get the day/month/year, as they are already properties of such type of objects.\n",
    "    keys_datetime = [ datetime.strptime(key, \"tweet_ids--%Y-%m-%d.csv\") for key in file_list ]\n",
    "\n",
    "    # Ultimately, I will store each .csv file as a pandas DataFrame in a dictionary, where the keys represent a \n",
    "    # simplified form of the date. So, here, I will format the dates from the datetime objects into simple strings.\n",
    "    keys = [ f\"{key.day}-{key.month}-{key.year}\" for key in keys_datetime ]\n",
    "\n",
    "    # In order to read the data from the files, I need the paths of the files to be passed on to the read_csv() function. \n",
    "    # The order of the days in the file paths needs to be consistent with the order of the dates in the keys\n",
    "    paths = [ os.path.join(data_path, file) for file in file_list ]\n",
    "\n",
    "    # Here, I will build the dictionary where the keys represent the formatted simple date and \n",
    "    # the values are dataframes corresponding to each file.\n",
    "    days = dict()\n",
    "    for i in range(len(file_list)):\n",
    "        days[keys[i]] = pd.read_csv(paths[i], index_col=False)\n",
    "        days[keys[i]].drop('id', axis=1, inplace=True)\n",
    "\n",
    "    return days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = create_days(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_merged_days(days):\n",
    "    # Here, I merged all data (from all available days) into a single dataframe (they have the same structure).\n",
    "    # I did that because some replies to a tweet posted today can come some days after, so we need to take care\n",
    "    # of the dataset as a whole.\n",
    "\n",
    "    # concatenate the dataframes and reset the index\n",
    "    merged_days = pd.concat([df for key, df in days.items()], ignore_index=True)\n",
    "\n",
    "    # Convert string column to datetime\n",
    "    merged_days['created_at'] = pd.to_datetime(merged_days['created_at'])\n",
    "\n",
    "    # Sort dataframe based on datetime column\n",
    "    # NOTE: I also reset the index so that I know later on which tweet was posted first based on the index (useful for opinion change)\n",
    "    merged_days = merged_days.sort_values('created_at').reset_index(drop=True)\n",
    "\n",
    "    return merged_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_days = create_merged_days(days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>credible</th>\n",
       "      <th>author_id</th>\n",
       "      <th>text</th>\n",
       "      <th>urls</th>\n",
       "      <th>name</th>\n",
       "      <th>username</th>\n",
       "      <th>verified</th>\n",
       "      <th>location</th>\n",
       "      <th>...</th>\n",
       "      <th>retweet_author_id</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>retweeted_screen_name</th>\n",
       "      <th>user_mentions_id</th>\n",
       "      <th>user_mentions_screen_name</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_tweet_id</th>\n",
       "      <th>in_reply_to_username</th>\n",
       "      <th>reference_type</th>\n",
       "      <th>reference_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-24 18:00:10+00:00</td>\n",
       "      <td>1364636249852502018</td>\n",
       "      <td>1</td>\n",
       "      <td>107501328</td>\n",
       "      <td>RT @Maricopahealth: At one of our community po...</td>\n",
       "      <td>#</td>\n",
       "      <td>2-1-1 Arizona</td>\n",
       "      <td>211arizona</td>\n",
       "      <td>False</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>...</td>\n",
       "      <td>29816986</td>\n",
       "      <td>1364632754042802176</td>\n",
       "      <td>Maricopahealth</td>\n",
       "      <td>29816986</td>\n",
       "      <td>Maricopahealth</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1364632754042802176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-24 18:00:18+00:00</td>\n",
       "      <td>1364636282664574978</td>\n",
       "      <td>1</td>\n",
       "      <td>26761523</td>\n",
       "      <td>Ready for DAY 2 of State of the Valley? Join u...</td>\n",
       "      <td>jointventure.org,twitter.com,</td>\n",
       "      <td>Joint Venture SV</td>\n",
       "      <td>JointVentureSVN</td>\n",
       "      <td>False</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-24 18:00:30+00:00</td>\n",
       "      <td>1364636333596008449</td>\n",
       "      <td>1</td>\n",
       "      <td>1234926105234034689</td>\n",
       "      <td>RT @SteveStaeger: When #COVID19Colorado is ove...</td>\n",
       "      <td>#</td>\n",
       "      <td>Colorado Coronavirus Updates</td>\n",
       "      <td>COVIDinColorado</td>\n",
       "      <td>False</td>\n",
       "      <td>Denver, Colorado</td>\n",
       "      <td>...</td>\n",
       "      <td>182037688</td>\n",
       "      <td>1364293582157307906</td>\n",
       "      <td>SteveStaeger</td>\n",
       "      <td>182037688</td>\n",
       "      <td>SteveStaeger</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1364293582157307906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-24 18:03:16+00:00</td>\n",
       "      <td>1364637028948709377</td>\n",
       "      <td>1</td>\n",
       "      <td>1329106574082641920</td>\n",
       "      <td>#SD37: Starting next week, @OCHealth will star...</td>\n",
       "      <td>bit.ly,www.ocregister.com,</td>\n",
       "      <td>Senator Dave Min</td>\n",
       "      <td>SenDaveMin</td>\n",
       "      <td>True</td>\n",
       "      <td>Irvine, CA</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>36069538</td>\n",
       "      <td>ochealth</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-24 18:03:35+00:00</td>\n",
       "      <td>1364637110951583746</td>\n",
       "      <td>1</td>\n",
       "      <td>1363750425459970048</td>\n",
       "      <td>RT @jatinde45666597: Vaccination has been star...</td>\n",
       "      <td>#</td>\n",
       "      <td>Reena Sharma</td>\n",
       "      <td>write2reena</td>\n",
       "      <td>False</td>\n",
       "      <td>Auckland, New Zealand</td>\n",
       "      <td>...</td>\n",
       "      <td>1295748297529884673</td>\n",
       "      <td>1364087633538859008</td>\n",
       "      <td>jatinde45666597</td>\n",
       "      <td>1295748297529884673</td>\n",
       "      <td>jatinde45666597</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1364087633538859008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123691</th>\n",
       "      <td>2021-03-10 23:59:52+00:00</td>\n",
       "      <td>1369800203939745796</td>\n",
       "      <td>1</td>\n",
       "      <td>434360613</td>\n",
       "      <td>RT @Philo: The boys of #SouthPark are at it ag...</td>\n",
       "      <td>#</td>\n",
       "      <td>ami_</td>\n",
       "      <td>ami_tvdfan</td>\n",
       "      <td>False</td>\n",
       "      <td>#</td>\n",
       "      <td>...</td>\n",
       "      <td>81766872</td>\n",
       "      <td>1369799981763162113</td>\n",
       "      <td>philoTV</td>\n",
       "      <td>23827692</td>\n",
       "      <td>ComedyCentral</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369799981763162113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123692</th>\n",
       "      <td>2021-03-10 23:59:52+00:00</td>\n",
       "      <td>1369800204094963712</td>\n",
       "      <td>1</td>\n",
       "      <td>3083078947</td>\n",
       "      <td>RT @ericswalwell: The #AmericanRescuePlan puts...</td>\n",
       "      <td>#</td>\n",
       "      <td>Thomas Albrecht 🇺🇸☮️</td>\n",
       "      <td>TomAlb88</td>\n",
       "      <td>False</td>\n",
       "      <td>#</td>\n",
       "      <td>...</td>\n",
       "      <td>377609596</td>\n",
       "      <td>1369727803768201218</td>\n",
       "      <td>ericswalwell</td>\n",
       "      <td>377609596</td>\n",
       "      <td>ericswalwell</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369727803768201218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123693</th>\n",
       "      <td>2021-03-10 23:59:53+00:00</td>\n",
       "      <td>1369800205592363018</td>\n",
       "      <td>1</td>\n",
       "      <td>3538956135</td>\n",
       "      <td>RT @Doc_Wolverine: \"Gee Doc, why are you pisse...</td>\n",
       "      <td>#</td>\n",
       "      <td>Jackaxed</td>\n",
       "      <td>Jackaxed</td>\n",
       "      <td>False</td>\n",
       "      <td>United States</td>\n",
       "      <td>...</td>\n",
       "      <td>898321581444911108</td>\n",
       "      <td>1369771222481985543</td>\n",
       "      <td>Doc_Wolverine</td>\n",
       "      <td>898321581444911108</td>\n",
       "      <td>Doc_Wolverine</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369771222481985543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123694</th>\n",
       "      <td>2021-03-10 23:59:53+00:00</td>\n",
       "      <td>1369800204761899011</td>\n",
       "      <td>1</td>\n",
       "      <td>29801287</td>\n",
       "      <td>RT @TheDweck: Wow, vision boards work</td>\n",
       "      <td>#</td>\n",
       "      <td>Fauxnly Fans</td>\n",
       "      <td>thenickkontz</td>\n",
       "      <td>False</td>\n",
       "      <td>ÜT: 43.508306,-96.779489</td>\n",
       "      <td>...</td>\n",
       "      <td>98247788</td>\n",
       "      <td>1369742802590990336</td>\n",
       "      <td>TheDweck</td>\n",
       "      <td>98247788</td>\n",
       "      <td>TheDweck</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>1369742802590990336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123695</th>\n",
       "      <td>2021-03-10 23:59:53+00:00</td>\n",
       "      <td>1369800205445521409</td>\n",
       "      <td>1</td>\n",
       "      <td>1095155084</td>\n",
       "      <td>just saw some lady on the news say she’s not g...</td>\n",
       "      <td>#</td>\n",
       "      <td>cristal✨</td>\n",
       "      <td>cristal_guz</td>\n",
       "      <td>False</td>\n",
       "      <td>Raleigh |22|</td>\n",
       "      <td>...</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5123696 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at             tweet_id  credible  \\\n",
       "0       2021-02-24 18:00:10+00:00  1364636249852502018         1   \n",
       "1       2021-02-24 18:00:18+00:00  1364636282664574978         1   \n",
       "2       2021-02-24 18:00:30+00:00  1364636333596008449         1   \n",
       "3       2021-02-24 18:03:16+00:00  1364637028948709377         1   \n",
       "4       2021-02-24 18:03:35+00:00  1364637110951583746         1   \n",
       "...                           ...                  ...       ...   \n",
       "5123691 2021-03-10 23:59:52+00:00  1369800203939745796         1   \n",
       "5123692 2021-03-10 23:59:52+00:00  1369800204094963712         1   \n",
       "5123693 2021-03-10 23:59:53+00:00  1369800205592363018         1   \n",
       "5123694 2021-03-10 23:59:53+00:00  1369800204761899011         1   \n",
       "5123695 2021-03-10 23:59:53+00:00  1369800205445521409         1   \n",
       "\n",
       "                   author_id  \\\n",
       "0                  107501328   \n",
       "1                   26761523   \n",
       "2        1234926105234034689   \n",
       "3        1329106574082641920   \n",
       "4        1363750425459970048   \n",
       "...                      ...   \n",
       "5123691            434360613   \n",
       "5123692           3083078947   \n",
       "5123693           3538956135   \n",
       "5123694             29801287   \n",
       "5123695           1095155084   \n",
       "\n",
       "                                                      text  \\\n",
       "0        RT @Maricopahealth: At one of our community po...   \n",
       "1        Ready for DAY 2 of State of the Valley? Join u...   \n",
       "2        RT @SteveStaeger: When #COVID19Colorado is ove...   \n",
       "3        #SD37: Starting next week, @OCHealth will star...   \n",
       "4        RT @jatinde45666597: Vaccination has been star...   \n",
       "...                                                    ...   \n",
       "5123691  RT @Philo: The boys of #SouthPark are at it ag...   \n",
       "5123692  RT @ericswalwell: The #AmericanRescuePlan puts...   \n",
       "5123693  RT @Doc_Wolverine: \"Gee Doc, why are you pisse...   \n",
       "5123694              RT @TheDweck: Wow, vision boards work   \n",
       "5123695  just saw some lady on the news say she’s not g...   \n",
       "\n",
       "                                  urls                          name  \\\n",
       "0                                    #                 2-1-1 Arizona   \n",
       "1        jointventure.org,twitter.com,              Joint Venture SV   \n",
       "2                                    #  Colorado Coronavirus Updates   \n",
       "3           bit.ly,www.ocregister.com,              Senator Dave Min   \n",
       "4                                    #                  Reena Sharma   \n",
       "...                                ...                           ...   \n",
       "5123691                              #                          ami_   \n",
       "5123692                              #          Thomas Albrecht 🇺🇸☮️   \n",
       "5123693                              #                      Jackaxed   \n",
       "5123694                              #                  Fauxnly Fans   \n",
       "5123695                              #                      cristal✨   \n",
       "\n",
       "                username  verified                  location  ...  \\\n",
       "0             211arizona     False                   Arizona  ...   \n",
       "1        JointVentureSVN     False              San Jose, CA  ...   \n",
       "2        COVIDinColorado     False          Denver, Colorado  ...   \n",
       "3             SenDaveMin      True                Irvine, CA  ...   \n",
       "4            write2reena     False     Auckland, New Zealand  ...   \n",
       "...                  ...       ...                       ...  ...   \n",
       "5123691       ami_tvdfan     False                         #  ...   \n",
       "5123692         TomAlb88     False                         #  ...   \n",
       "5123693         Jackaxed     False             United States  ...   \n",
       "5123694     thenickkontz     False  ÜT: 43.508306,-96.779489  ...   \n",
       "5123695      cristal_guz     False              Raleigh |22|  ...   \n",
       "\n",
       "           retweet_author_id           retweet_id  retweeted_screen_name  \\\n",
       "0                   29816986  1364632754042802176         Maricopahealth   \n",
       "1                          #                    #                      #   \n",
       "2                  182037688  1364293582157307906           SteveStaeger   \n",
       "3                          #                    #                      #   \n",
       "4        1295748297529884673  1364087633538859008        jatinde45666597   \n",
       "...                      ...                  ...                    ...   \n",
       "5123691             81766872  1369799981763162113                philoTV   \n",
       "5123692            377609596  1369727803768201218           ericswalwell   \n",
       "5123693   898321581444911108  1369771222481985543          Doc_Wolverine   \n",
       "5123694             98247788  1369742802590990336               TheDweck   \n",
       "5123695                    #                    #                      #   \n",
       "\n",
       "            user_mentions_id  user_mentions_screen_name  in_reply_to_user_id  \\\n",
       "0                   29816986             Maricopahealth                    #   \n",
       "1                          #                          #                    #   \n",
       "2                  182037688               SteveStaeger                    #   \n",
       "3                   36069538                   ochealth                    #   \n",
       "4        1295748297529884673            jatinde45666597                    #   \n",
       "...                      ...                        ...                  ...   \n",
       "5123691             23827692              ComedyCentral                    #   \n",
       "5123692            377609596               ericswalwell                    #   \n",
       "5123693   898321581444911108              Doc_Wolverine                    #   \n",
       "5123694             98247788                   TheDweck                    #   \n",
       "5123695                    #                          #                    #   \n",
       "\n",
       "         in_reply_to_tweet_id in_reply_to_username reference_type  \\\n",
       "0                           #                    #      retweeted   \n",
       "1                           #                    #              #   \n",
       "2                           #                    #      retweeted   \n",
       "3                           #                    #              #   \n",
       "4                           #                    #      retweeted   \n",
       "...                       ...                  ...            ...   \n",
       "5123691                     #                    #      retweeted   \n",
       "5123692                     #                    #      retweeted   \n",
       "5123693                     #                    #      retweeted   \n",
       "5123694                     #                    #      retweeted   \n",
       "5123695                     #                    #              #   \n",
       "\n",
       "                reference_id  \n",
       "0        1364632754042802176  \n",
       "1                          #  \n",
       "2        1364293582157307906  \n",
       "3                          #  \n",
       "4        1364087633538859008  \n",
       "...                      ...  \n",
       "5123691  1369799981763162113  \n",
       "5123692  1369727803768201218  \n",
       "5123693  1369771222481985543  \n",
       "5123694  1369742802590990336  \n",
       "5123695                    #  \n",
       "\n",
       "[5123696 rows x 27 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created_at', 'tweet_id', 'credible', 'author_id', 'text', 'urls',\n",
       "       'name', 'username', 'verified', 'location', 'followers_count',\n",
       "       'following_count', 'tweet_count', 'like_count', 'quote_count',\n",
       "       'reply_count', 'retweet_count', 'retweet_author_id', 'retweet_id',\n",
       "       'retweeted_screen_name', 'user_mentions_id',\n",
       "       'user_mentions_screen_name', 'in_reply_to_user_id',\n",
       "       'in_reply_to_tweet_id', 'in_reply_to_username', 'reference_type',\n",
       "       'reference_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_days.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUOTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = merged_days[merged_days['reference_type'] == 'quoted'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_quote_texts = quotes.head(200).loc[:, 'text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for writing (use 'a' instead of 'w' to append)\n",
    "with open('/home/andreistoica12/research-internship/first_200_quotes.txt', 'w') as file:\n",
    "    # Loop through the list_of_quote_texts and write each text to a new line in the file\n",
    "    for text in list_of_quote_texts:\n",
    "        file.write(text + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REPLIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = merged_days[merged_days['reference_type'] == 'replied_to'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_replies = replies[replies.duplicated(subset=['author_id', 'in_reply_to_tweet_id'], keep=False)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_replies['in_reply_to_tweet_id'] = multiple_replies['in_reply_to_tweet_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_multiple_replies = multiple_replies.head(300).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups_of_replies_for_opinion_change(multiple_replies):\n",
    "    # group the rows by the two columns\n",
    "    grouped_df = multiple_replies.groupby(['author_id', 'in_reply_to_tweet_id'])\n",
    "    # grouped_df = small_multiple_replies.groupby(['author_id', 'in_reply_to_tweet_id'])\n",
    "\n",
    "    groups_of_replies = grouped_df.groups\n",
    "\n",
    "    return groups_of_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_of_replies = create_groups_of_replies_for_opinion_change(multiple_replies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath(path_to_sentistrength_jar)\n",
    "senti.setSentiStrengthLanguageFolderPath(path_to_sentistrength_language_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_change(rows_indices, replies):\n",
    "    \"\"\"Function to detect whether an opinion change occured within a group of replies.\n",
    "\n",
    "    Args:\n",
    "        rows_indices (pandas.core.indexes.numeric.Int64Index): list of indices in the original dataframe\n",
    "                                                               where an opinion change has been detected\n",
    "                                                               (e.g. Int64Index([1848965, 1850146, 1850687], dtype='int64'))\n",
    "\n",
    "    Returns:\n",
    "        bool: boolean value which confirms or denies the existence of an opinion change within a group\n",
    "    \"\"\"    \n",
    "    texts = [ clean_text(replies.loc[index, 'text']) for index in rows_indices ]\n",
    "\n",
    "    sentiments = senti.getSentiment(texts, score='scale')\n",
    "    sentiments = np.array(sentiments)\n",
    "\n",
    "    positive = np.any(sentiments > 0)\n",
    "    negative = np.any(sentiments < 0)\n",
    "\n",
    "    return positive and negative, sentiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE:\n",
    "\n",
    "THERE IS NO NEED TO RUN THE COMMENTED CELLS BETWEEN THE LINES BELOW!\n",
    "It took more than 15 minutes when I ran the creation of the opinion_changes dictionary on the whole replies dataset...\n",
    "\n",
    "I saved the resulting dictionary into a JSON file, which can be found in the root directory of the project. This can be imported into a dictionary with ease (code can be found in the next parts of the notebook)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_counter = 0\n",
    "progress = 0.01\n",
    "\n",
    "def print_progress(groups_of_replies):\n",
    "    \"\"\"Function that prints the progress of the computation of the opinion_changes dictionary,\n",
    "    as it takes a lot of time for large datasets.\n",
    "\n",
    "    Args:\n",
    "        groups_of_replies (dict): dictionary of replies grouped by some columns\n",
    "    \"\"\"    \n",
    "    global group_counter\n",
    "    global progress\n",
    "    group_counter += 1\n",
    "\n",
    "    if ((group_counter / len(groups_of_replies)) >= progress):\n",
    "        print(f\"Progress: {group_counter} / {len(groups_of_replies)} groups of replies processed.\")\n",
    "        progress += 0.01\n",
    "    if group_counter == len(groups_of_replies):\n",
    "        print(\"All groups have been processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opinion_changes(groups_of_replies, progress_printing):\n",
    "    \"\"\"Function to create the data structure associated with the groups (pairs of user id-s who interacted through replies)\n",
    "    where an opinio change occured, i.e. when, between their interactions (e.g. one's replies to the other's original post),\n",
    "    there have been both positive and negative opinions.\n",
    "\n",
    "    Args:\n",
    "        groups_of_replies (dict): dictionary of replies grouped by some columns\n",
    "        progress_printing (bool): boolean value indicating whether the user wishes to print the progress of the groups processed or not\n",
    "                                  (this can be useful to track when processing large datasets - they usually take a lot of time)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary where the keys represent the groups where opinion changes occured (as tuples) and the values are\n",
    "              lists of the sentiments associated to the interactions within each group\n",
    "    \"\"\"    \n",
    "    if progress_printing == True:\n",
    "        opinion_changes = {}\n",
    "        for group, rows_indices in groups_of_replies.items():\n",
    "            print_progress(groups_of_replies)\n",
    "            if opinion_change(rows_indices, replies)[0] == True:\n",
    "                opinion_changes[group] = opinion_change(rows_indices, replies)[1].tolist()\n",
    "    else:\n",
    "        print(\"Gradual progress will not be printed.\")\n",
    "        print(\"If you wish to see it, change the value of the progress_printing input parameter to True.\")\n",
    "        opinion_changes = { group: opinion_change(rows_indices, replies)[1].tolist() for group, rows_indices in groups_of_replies.items() \n",
    "                        if opinion_change(rows_indices, replies)[0] == True }\n",
    "    \n",
    "    return opinion_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_printing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 56 / 5587 groups of replies processed.\n",
      "Progress: 112 / 5587 groups of replies processed.\n",
      "Progress: 168 / 5587 groups of replies processed.\n",
      "Progress: 224 / 5587 groups of replies processed.\n",
      "Progress: 280 / 5587 groups of replies processed.\n",
      "Progress: 336 / 5587 groups of replies processed.\n",
      "Progress: 392 / 5587 groups of replies processed.\n",
      "Progress: 447 / 5587 groups of replies processed.\n",
      "Progress: 503 / 5587 groups of replies processed.\n",
      "Progress: 559 / 5587 groups of replies processed.\n",
      "Progress: 615 / 5587 groups of replies processed.\n",
      "Progress: 671 / 5587 groups of replies processed.\n",
      "Progress: 727 / 5587 groups of replies processed.\n",
      "Progress: 783 / 5587 groups of replies processed.\n",
      "Progress: 839 / 5587 groups of replies processed.\n",
      "Progress: 894 / 5587 groups of replies processed.\n",
      "Progress: 950 / 5587 groups of replies processed.\n",
      "Progress: 1006 / 5587 groups of replies processed.\n",
      "Progress: 1062 / 5587 groups of replies processed.\n",
      "Progress: 1118 / 5587 groups of replies processed.\n",
      "Progress: 1174 / 5587 groups of replies processed.\n",
      "Progress: 1230 / 5587 groups of replies processed.\n",
      "Progress: 1286 / 5587 groups of replies processed.\n",
      "Progress: 1341 / 5587 groups of replies processed.\n",
      "Progress: 1397 / 5587 groups of replies processed.\n",
      "Progress: 1453 / 5587 groups of replies processed.\n",
      "Progress: 1509 / 5587 groups of replies processed.\n",
      "Progress: 1565 / 5587 groups of replies processed.\n",
      "Progress: 1621 / 5587 groups of replies processed.\n",
      "Progress: 1677 / 5587 groups of replies processed.\n",
      "Progress: 1732 / 5587 groups of replies processed.\n",
      "Progress: 1788 / 5587 groups of replies processed.\n",
      "Progress: 1844 / 5587 groups of replies processed.\n",
      "Progress: 1900 / 5587 groups of replies processed.\n",
      "Progress: 1956 / 5587 groups of replies processed.\n",
      "Progress: 2012 / 5587 groups of replies processed.\n",
      "Progress: 2068 / 5587 groups of replies processed.\n",
      "Progress: 2124 / 5587 groups of replies processed.\n",
      "Progress: 2179 / 5587 groups of replies processed.\n",
      "Progress: 2235 / 5587 groups of replies processed.\n",
      "Progress: 2291 / 5587 groups of replies processed.\n",
      "Progress: 2347 / 5587 groups of replies processed.\n",
      "Progress: 2403 / 5587 groups of replies processed.\n",
      "Progress: 2459 / 5587 groups of replies processed.\n",
      "Progress: 2515 / 5587 groups of replies processed.\n",
      "Progress: 2571 / 5587 groups of replies processed.\n",
      "Progress: 2626 / 5587 groups of replies processed.\n",
      "Progress: 2682 / 5587 groups of replies processed.\n",
      "Progress: 2738 / 5587 groups of replies processed.\n",
      "Progress: 2794 / 5587 groups of replies processed.\n",
      "Progress: 2850 / 5587 groups of replies processed.\n",
      "Progress: 2906 / 5587 groups of replies processed.\n",
      "Progress: 2962 / 5587 groups of replies processed.\n",
      "Progress: 3017 / 5587 groups of replies processed.\n",
      "Progress: 3073 / 5587 groups of replies processed.\n",
      "Progress: 3129 / 5587 groups of replies processed.\n",
      "Progress: 3185 / 5587 groups of replies processed.\n",
      "Progress: 3241 / 5587 groups of replies processed.\n",
      "Progress: 3297 / 5587 groups of replies processed.\n",
      "Progress: 3353 / 5587 groups of replies processed.\n",
      "Progress: 3409 / 5587 groups of replies processed.\n",
      "Progress: 3464 / 5587 groups of replies processed.\n",
      "Progress: 3520 / 5587 groups of replies processed.\n",
      "Progress: 3576 / 5587 groups of replies processed.\n",
      "Progress: 3632 / 5587 groups of replies processed.\n",
      "Progress: 3688 / 5587 groups of replies processed.\n",
      "Progress: 3744 / 5587 groups of replies processed.\n",
      "Progress: 3800 / 5587 groups of replies processed.\n",
      "Progress: 3856 / 5587 groups of replies processed.\n",
      "Progress: 3911 / 5587 groups of replies processed.\n",
      "Progress: 3967 / 5587 groups of replies processed.\n",
      "Progress: 4023 / 5587 groups of replies processed.\n",
      "Progress: 4079 / 5587 groups of replies processed.\n",
      "Progress: 4135 / 5587 groups of replies processed.\n",
      "Progress: 4191 / 5587 groups of replies processed.\n",
      "Progress: 4247 / 5587 groups of replies processed.\n",
      "Progress: 4302 / 5587 groups of replies processed.\n",
      "Progress: 4358 / 5587 groups of replies processed.\n",
      "Progress: 4414 / 5587 groups of replies processed.\n",
      "Progress: 4470 / 5587 groups of replies processed.\n",
      "Progress: 4526 / 5587 groups of replies processed.\n",
      "Progress: 4582 / 5587 groups of replies processed.\n",
      "Progress: 4638 / 5587 groups of replies processed.\n",
      "Progress: 4694 / 5587 groups of replies processed.\n",
      "Progress: 4749 / 5587 groups of replies processed.\n",
      "Progress: 4805 / 5587 groups of replies processed.\n",
      "Progress: 4861 / 5587 groups of replies processed.\n",
      "Progress: 4917 / 5587 groups of replies processed.\n",
      "Progress: 4973 / 5587 groups of replies processed.\n",
      "Progress: 5029 / 5587 groups of replies processed.\n",
      "Progress: 5085 / 5587 groups of replies processed.\n",
      "Progress: 5141 / 5587 groups of replies processed.\n",
      "Progress: 5196 / 5587 groups of replies processed.\n",
      "Progress: 5252 / 5587 groups of replies processed.\n",
      "Progress: 5308 / 5587 groups of replies processed.\n",
      "Progress: 5364 / 5587 groups of replies processed.\n",
      "Progress: 5420 / 5587 groups of replies processed.\n",
      "Progress: 5476 / 5587 groups of replies processed.\n",
      "Progress: 5532 / 5587 groups of replies processed.\n",
      "All groups have been processed.\n"
     ]
    }
   ],
   "source": [
    "opinion_changes = create_opinion_changes(groups_of_replies, progress_printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_opinion_changes_to_JSON(opinion_changes, path):\n",
    "    \"\"\"Function to save the dictionary pf opinion changes to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "        path (str): path where you wish to save the JSON file\n",
    "    \"\"\"    \n",
    "    # create a new dictionary with string keys\n",
    "    opinion_changes_for_JSON_file = {str(key): value for key, value in opinion_changes.items() }\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(opinion_changes_for_JSON_file, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_opinion_changes_to_JSON(opinion_changes, path_to_replies_opinion_changes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opinion_changes(path_to_replies_opinion_changes):\n",
    "    \"\"\"Function that generates a dictionary based on a JSON file which contains the opinion changes within the replies of the dataset.\n",
    "\n",
    "    Args:\n",
    "        path_to_replies_opinion_changes (str): path to the JSON file associated with the opinion changes within the replies\n",
    "                                               (e.g. /your/path/to/research-internship)\n",
    "\n",
    "    Returns:\n",
    "        dict: the original dictionary containing opinion changes from replies\n",
    "    \"\"\"    \n",
    "    with open(path_to_replies_opinion_changes) as f:\n",
    "        # Load the JSON data into a Python dictionary\n",
    "        opinion_changes_from_file = json.load(f)\n",
    "        # Create a new dictionary with tuple keys\n",
    "        original_opinion_changes = {}\n",
    "        for key in opinion_changes_from_file:\n",
    "            # Convert the string key to a tuple\n",
    "            new_key = eval(key)\n",
    "            # Add the key-value pair to the new dictionary\n",
    "            original_opinion_changes[new_key] = opinion_changes_from_file[key]\n",
    "            \n",
    "    return original_opinion_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_changes = load_opinion_changes(path_to_replies_opinion_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of opinion changes out of the interactions where one user replied multiple times to a source tweet:\n",
      "15.6%.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of opinion changes out of the interactions where one user replied multiple times to a source tweet:\")\n",
    "print(f\"{round(len(opinion_changes) / len(groups_of_replies) * 100, 1)}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biggest_opinion_change(opinion_changes):\n",
    "    \"\"\"Function that returns the group (pair of user id-s) which interacted more than once in the context of a single source tweet,\n",
    "    i.e. one user posted more than one reply to the same source tweet, where the user who reacted had the most drastic opinion change,\n",
    "    based on the previously computed sentiments of the text.\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "\n",
    "    Returns:\n",
    "        tuple: pair of user id-s where the biggest opinion change occured\n",
    "        str: type of change that occured, e.g. one user tends to agree with the source tweet after some time, \n",
    "             when initially he disagreed or vice-versa\n",
    "    \"\"\"    \n",
    "    change_type = 'negative'\n",
    "    biggest_change = 0\n",
    "    target_group = tuple()\n",
    "    for group, sentiments in opinion_changes.items():\n",
    "        change = max(biggest_change, max(sentiments) - min(sentiments))\n",
    "        if change > biggest_change:\n",
    "            biggest_change = change\n",
    "            target_group = group\n",
    "    \n",
    "    min_sentiment_index = opinion_changes[target_group].index(min(opinion_changes[target_group]))\n",
    "    max_sentiment_index = opinion_changes[target_group].index(max(opinion_changes[target_group]))\n",
    "    change_type = 'positive' if min_sentiment_index < max_sentiment_index else change_type\n",
    "\n",
    "    return target_group, change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_group, change_type = biggest_opinion_change(opinion_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(932012546, 1366061818699989002)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replies_with_biggest_opinion_change(multiple_replies, target_group):\n",
    "    \"\"\"Function that queries the multiple_replies dataset and returns a list of the actual texts that the pair of users posted.\n",
    "     The user id-s of these users are passed on as input parameters (the target group).\n",
    "\n",
    "    Args:\n",
    "        replies (pandas Dataframe): the dataframe with the replies\n",
    "        target_group (tuple): pair of user id-s whose posts had the biggest opinion change\n",
    "\n",
    "    Returns:\n",
    "        list: list of texts posted by the 2 users\n",
    "    \"\"\"    \n",
    "    condition1 = multiple_replies['author_id'] == target_group[0]\n",
    "    condition2 = multiple_replies['in_reply_to_tweet_id'] == target_group[1]\n",
    "\n",
    "    return multiple_replies[condition1 & condition2].loc[:, 'text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_biggest_change = replies_with_biggest_opinion_change(multiple_replies, target_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"@KATIEDOLL1201 @daulan @SandySue1958 I've been taking vaccines since I was very young..  I got the vax -my SIL was SORRY she didn't.  My mom and her sis just got both covid shots -NP.  Mom's 90, her sis it is 94.  Grandma was born in 1899, saw the 1st pandemic.. She loved vaccines. She lived to 94, maybe that's why.\",\n",
       " \"@KATIEDOLL1201 @daulan @SandySue1958 Sorry, they're being so hard on you, but the 'majority' of people who get Shingles say it is very painful, and the 'majority' of people who have the vaccine do not have an issue.  That is by the numbers.  And right now there is soo much misinformation being spread- It's just sad.\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replies_biggest_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_change_type(opinion_changes, group):\n",
    "    \"\"\"Function to detect what type of opinion change occured in the case of a group (pair of user ids-s) which interacted\n",
    "    through replies\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "        group (tuple): pair of user id-s that interacted through replies and the respondent changed his viewpoint w.r.t. a source tweet\n",
    "\n",
    "    Returns:\n",
    "        str: either 'positive' (if the respondent now agrees after initially disagreeing) or 'negative'\n",
    "    \"\"\"    \n",
    "    min_sentiment_index = opinion_changes[group].index(min(opinion_changes[group]))\n",
    "    max_sentiment_index = opinion_changes[group].index(max(opinion_changes[group]))\n",
    "    \n",
    "    change_type = 'negative'\n",
    "    change_type = 'positive' if min_sentiment_index < max_sentiment_index else change_type\n",
    "\n",
    "    return change_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask indicating what type of opinion change each group has\n",
    "mask = {group: opinion_change_type(opinion_changes, group) for group in opinion_changes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_count_in_dict(dict, value_to_count):\n",
    "    \"\"\"Function to count the occurences of a certain value in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        dict (dict): dictionary where we need to count the occurences of a value\n",
    "        value_to_count (any): value to be counted\n",
    "\n",
    "    Returns:\n",
    "        int: number of occurences of value_to_count\n",
    "    \"\"\"    \n",
    "    # Create a reverse dictionary that maps values to their frequencies\n",
    "    reverse_dict = defaultdict(int)\n",
    "    for value in dict.values():\n",
    "        reverse_dict[value] += 1\n",
    "\n",
    "    # Count the occurrences of the specific value\n",
    "    count = reverse_dict.get(value_to_count, 0)\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive opinion changes out of:\n",
      "- the interactions where one user replied multiple times to a source tweet and an opinion change was detected => 46.1%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of positive opinion changes out of:\")\n",
    "print(f\"- the interactions where one user replied multiple times to a source tweet and an opinion change was detected => {round(value_count_in_dict(mask, 'positive') / len(mask) * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of negative opinion changes out of:\n",
      "- the interactions where one user replied multiple times to a source tweet and an opinion change was detected => 53.9%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Percentage of negative opinion changes out of:\")\n",
    "print(f\"- the interactions where one user replied multiple times to a source tweet and an opinion change was detected => {round(value_count_in_dict(mask, 'negative') / len(mask) * 100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the distribution of the tweets per hour, I will parse the \"created_at\" column, extract the hour property and create a separate column in each dataframe. I will place it next to the \"created_at\" column in order to be easily verifiable. Data originates frmo the Twitter API, so it comes in a standard ISO 8601 format, which can be easily parsed using the parser module from the dateutil package.\n",
    "\n",
    "Note: the cell below runs for approximately 2m30' on my machine (~25-30 seconds for each file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, day in days.items():\n",
    "#     if 'hour' not in day.columns:\n",
    "#         day.insert(1, 'hour', day['created_at'].apply(lambda date: parser.parse(date).hour))\n",
    "#         print(f\"New 'hour' column inserted in the {key} dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key, day in days.items():\n",
    "#     if 'hour' not in day.columns:\n",
    "#         hours = []\n",
    "#         for time in day.loc[:,\"created_at\"]:\n",
    "#             hour = parser.parse(time).hour\n",
    "#             hours.append(hour)\n",
    "#         day.insert(1, \"hour\", hours, True)\n",
    "#         print(key + \" - added 'hour' column\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final distribution is made up of the sum of all individual days' distributions. I save a figure in the graphs/ folder for each day, as well as an overall distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_distribution = pd.Series(0, index=days['1-3-2021'].loc[:,'hour'].sort_values(ascending=True).unique())\n",
    "# for key, day in days.items():\n",
    "#     hour_column_ascending = day.loc[:,\"hour\"].sort_values(ascending=True)\n",
    "#     distribution = hour_column_ascending.value_counts()[hour_column_ascending.unique()]\n",
    "#     final_distribution = final_distribution.add(distribution)\n",
    "#     axes = distribution.plot(kind='bar')\n",
    "#     figure_path = f\"{covaxxy_longitudinal_analysis_graphs}/{key}_distribution.png\"\n",
    "#     axes.figure.savefig(figure_path)\n",
    "#     plt.close()\n",
    "# axes = final_distribution.plot(kind='bar')\n",
    "# figure_path = f\"{covaxxy_longitudinal_analysis_graphs}/overall_distribution.png\"\n",
    "# axes.figure.savefig(figure_path)\n",
    "# plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
