{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the operating system of the host machine so that we can treat the path separator correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "path_separator_Windows = \"\\\\\"\n",
    "path_separator = '/' if platform.system() == 'Linux' else path_separator_Windows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\andrei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\andrei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sentistrength import PySentiStr\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "# Load the punkt tokenizer data from the local directory\n",
    "nltk.data.load(f'tokenizers{path_separator}punkt{path_separator}english.pickle')\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.getipython import get_ipython\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where the raw dataset (the initial .csv files) is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = rootdir_path + f'{path_separator}data{path_separator}covaxxy-csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_possibilities = ['15_days', '25_days']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_days = dataset_possibilities[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = rootdir_path + f'{path_separator}data{path_separator}covaxxy_merged_{number_of_days}.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you do not have access to the .csv file associated with the merged data, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chmod(data_path, 0o777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = rootdir_path + f'{path_separator}files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_unique_dates = files_path + f'{path_separator}unique_dates_{number_of_days}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_changes_path = files_path + f'{path_separator}opinion-changes-{number_of_days}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_path = rootdir_path + f'{path_separator}graphs'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 1 subfolder within the graphs/ folder to store important graphs for the covaxxy dataset. If it already existed (from previous runnings of the project), delete the folder and its contents and create an empty folder to store the current graphs, relevant to the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_graphs_path = os.path.join(graphs_path, 'covaxxy')\n",
    "# if os.path.exists(covaxxy_graphs_path):\n",
    "#    shutil.rmtree(covaxxy_graphs_path, ignore_errors=False, onerror=None)\n",
    "# os.makedirs(covaxxy_graphs_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subfolders specific to the different types of analyses performed in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "covaxxy_deltas_OC_graphs_path = os.path.join(covaxxy_graphs_path, f'deltas-OC-{number_of_days}')\n",
    "if os.path.exists(covaxxy_deltas_OC_graphs_path):\n",
    "   shutil.rmtree(covaxxy_deltas_OC_graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(covaxxy_deltas_OC_graphs_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where the SentiStrength library is stored.\n",
    "\n",
    "NOTE: Due to their policy, the Java version of the library (the one I am using) is only free for academic use. Therefore, I could not make it publicly available. If you wish to use the free library (for academic purposes), I will gladly redirect you to the author at M.Thelwall@wlv.ac.uk . \n",
    "\n",
    "More information is available at: http://sentistrength.wlv.ac.uk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength = rootdir_path + f'{path_separator}SentiStrength'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the Java executable file of SentiStrength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_jar = path_to_sentistrength + f'{path_separator}SentiStrengthCom.jar'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the language folder, which is used along with the .jar file to compute sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_sentistrength_language_folder = path_to_sentistrength + f'{path_separator}LanguageFolder'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the general nature of the NLTK built-in stop words list, most words could actually have an impact in the computation of sentiment scores if removed from the texts (e.g. \"all\" or \"not\"), thus I decided against using this pre-defined list. Instead I created a custom list of stop words, which can be found at the following relative location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_stopwords = files_path + f'{path_separator}stopwords.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stop_words(path_to_stopwords):\n",
    "    \"\"\"Function to read a .txt file containing (custom) stop words and return a set of these stop words.\n",
    "\n",
    "    Args:\n",
    "        path_to_stopwords (str): path to the.txt file containing stop words (e.g. /your/path/to/files/stop_words.txt)\n",
    "\n",
    "    Returns:\n",
    "        set: set of stop words\n",
    "    \"\"\"    \n",
    "    stop_words = set()\n",
    "    with open(path_to_stopwords, 'r') as f:\n",
    "        for line in f:\n",
    "            word = line.strip()  # remove whitespace and newline characters\n",
    "            stop_words.add(word)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = custom_stop_words(path_to_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    \"\"\"Function that takes a text string as input and uses a regular expression pattern to match all Unicode characters\n",
    "    that are classified as emojis. The regular expression includes different ranges of Unicode characters \n",
    "    that represent different types of emojis, such as emoticons, symbols, and flags.\n",
    "\n",
    "    Args:\n",
    "        text (str): text string to remove emokis from\n",
    "\n",
    "    Returns:\n",
    "        str: text string with all emojis removed\n",
    "    \"\"\"    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stop_words):\n",
    "    \"\"\"Function that removes stop words from a given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): text string\n",
    "        stop_words (set): set of stop words\n",
    "\n",
    "    Returns:\n",
    "        str: text string without stop words\n",
    "    \"\"\"    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove the stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Join the filtered tokens back into a string\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stop_words):\n",
    "    \"\"\"Function to clean the raw text, e.g. from a tweet. Performs the following steps:\n",
    "    1. Lowercase all the words in the text\n",
    "    2. Replace all new line characters with a white space\n",
    "    3. Remove tags\n",
    "    4. Remove URLs\n",
    "    5. Remove punctuations\n",
    "    6. Convert contractions to their full forms\n",
    "    7. Remove emojis (emoticons, symbols, flags, etc.)\n",
    "    8. Remove stopwords\n",
    "\n",
    "\n",
    "    Args:\n",
    "        text (str): text string to be cleaned before passing it to the sentiment analysis model\n",
    "        stop_words (set): set of stop words to be removed from the text\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned text string\n",
    "    \"\"\"        \n",
    "    # 1. Lowercase all words in the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Replace the new line character with empty string\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # 3. Remove words starting with '@' - tags (most common noise in replies)\n",
    "    text = re.sub(r'@\\w+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 4. Remove words starting with 'http' - hyperlinks\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 5. Remove punctuation from the text using regular expressions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # 6. Remove contractions, such as you're => you are\n",
    "    contractions.fix(text)\n",
    "\n",
    "    # 7. Remove emojis\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    # 8. Remove stopwords in English\n",
    "    text = remove_stopwords(text, stop_words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_days = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_days = merged_days.iloc[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_int(reference_id):\n",
    "    try:\n",
    "        return int(reference_id)\n",
    "    except ValueError:\n",
    "        return reference_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_days['reference_id'] = merged_days['reference_id'].apply(string_to_int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REACTIONS\n",
    "\n",
    "There are 3 types of reactions:\n",
    "- replies ('replied_to')\n",
    "- quotes ('quoted')\n",
    "- retweets ('retweeted')\n",
    "\n",
    "All possible combinations of reactions types you may wish to take into account further down the line are specified in the full list below. \n",
    "\n",
    "The reaction_types list should be equal to one of the elements of the full list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reaction_types_full_list = [['quoted'], \n",
    "                            ['quoted', 'retweeted'], \n",
    "                            ['replied_to'], \n",
    "                            ['replied_to', 'quoted'], \n",
    "                            ['replied_to', 'quoted', 'retweeted'],\n",
    "                            ['replied_to', 'retweeted']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can choose what (combination of) reaction types you wish to be included in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_to_opinion_changes(reaction_types):\n",
    "    \"\"\"Function to create the path to the opinion changes JSON file, based on the reaction types we took into consideration.\n",
    "\n",
    "    Args:\n",
    "        reaction_types (list): list of reaction types\n",
    "\n",
    "    Returns:\n",
    "        str: path to the opinion changes file\n",
    "    \"\"\"    \n",
    "    type = \"_\".join(reaction_types)\n",
    "    path = opinion_changes_path + f\"{path_separator}{type}_OC.json\"\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_reactions(merged_days, reaction_types):\n",
    "    \"\"\"Function to group reactions based on the reaction types list given as an input parameter, by the\n",
    "    'author_id' and 'reference_id' columns. This means that each group of reactions contains a (set of) reaction(s)\n",
    "    posted by the user identified by the 'author_id' and the source tweet identified by the 'reference_id'.\n",
    "\n",
    "    Args:\n",
    "        merged_days (pandas.core.frame.DataFrame): dataframe with all the data\n",
    "        reaction_types (list): list of reaction types we want to consider\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary where the key is a tuple of the form (author_id, reference_id)\n",
    "              and the value is a dataframe with all reactions corresponding to that combination\n",
    "    \"\"\"    \n",
    "    reactions = merged_days[merged_days['reference_type'].isin(reaction_types)]\n",
    "    multiple_reactions = reactions[reactions.duplicated(subset=['author_id', 'reference_id'], keep=False)]\n",
    "\n",
    "    # group the rows by the two columns\n",
    "    grouped_df = multiple_reactions.groupby(['author_id', 'reference_id'])\n",
    "    groups_of_reactions = grouped_df.groups\n",
    "\n",
    "    return groups_of_reactions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an instance of the PySentiStr class to which we set the path to the Java executable and the path to the language folder. After this, we are all set to use the SentiStrength library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath(path_to_sentistrength_jar)\n",
    "senti.setSentiStrengthLanguageFolderPath(path_to_sentistrength_language_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiments(rows_indexes, dataset, stop_words):\n",
    "    \"\"\"Function to compute the sentiment list for a set of rows in the dataset (given by dataset), taking\n",
    "    into account the given stop words.\n",
    "\n",
    "    Args:\n",
    "        rows_indexes (pandas.core.indexes.numeric.Int64Index): indexes of rows in the dataset that we want to compute the sentiment for\n",
    "        dataset (pandas.core.frame.DataFrame): dataframe containing the dataset\n",
    "        stop_words (set): set of stop words\n",
    "\n",
    "    Returns:\n",
    "        list: list of sentiment scores for each row identified by rows_indexes\n",
    "    \"\"\"    \n",
    "    texts = [ clean_text(dataset.loc[index, 'text'], stop_words) \n",
    "             if dataset.loc[index, 'reference_type'] != 'retweeted' else 'extremely fabulous'\n",
    "             for index in rows_indexes ]\n",
    "    \n",
    "    sentiments = senti.getSentiment(texts, score='scale')\n",
    "\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_change(rows_indexes, dataset, stop_words):\n",
    "    \"\"\"Function to detect whether an opinion change occured within a group of reactions (replies/quotes/retweets).\n",
    "\n",
    "    Args:\n",
    "        rows_indexes (pandas.core.indexes.numeric.Int64Index): list of indexes in the original dataframe (dataset)\n",
    "                                                               where we aim to detect an opinion change\n",
    "                                                               (e.g. Int64Index([1848965, 1850146, 1850687], dtype='int64'))\n",
    "        dataset (pandas.core.frame.DataFrame): dataframe containing the opinion changes\n",
    "        stop_words (list): list of stopwords\n",
    "\n",
    "    Returns:\n",
    "        bool: boolean value which confirms or denies the existence of an opinion change between the rows analyzed\n",
    "    \"\"\" \n",
    "    sentiments = compute_sentiments(rows_indexes, dataset, stop_words)\n",
    "    sentiments = np.array(sentiments)\n",
    "\n",
    "    positive = np.any(sentiments > 0)\n",
    "    negative = np.any(sentiments < 0)\n",
    "\n",
    "    return positive and negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARALLEL COMPUTATION - OPINION CHANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentiments_for_group(rows_indexes):\n",
    "    \"\"\"Function to compute the sentiments of the tweets provided by the row indexes within the merged_days dataframe.\n",
    "    Returns a list of sentiments corresponding to each of the tweets or an empty list if there was no opinion change \n",
    "    within that group.\n",
    "\n",
    "    Args:\n",
    "        rows_indexes (pandas.core.indexes.numeric.Int64Index): list of indexes in the original dataframe (dataset)\n",
    "                                                               where we aim to detect an opinion change\n",
    "                                                               (e.g. Int64Index([1848965, 1850146, 1850687], dtype='int64'))\n",
    "    Returns:\n",
    "        list: list of sentiments for the rows or empty list if there was no opinion change within that group.\n",
    "    \"\"\"    \n",
    "    global merged_days\n",
    "    global stop_words\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    if ipython is not None:\n",
    "        merged_days = ipython.user_ns['merged_days']\n",
    "        stop_words = ipython.user_ns['stop_words']\n",
    "\n",
    "    processed_values = []\n",
    "    if opinion_change(rows_indexes, merged_days, stop_words):\n",
    "        processed_values = compute_sentiments(rows_indexes, merged_days, stop_words)\n",
    "\n",
    "    return processed_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dict_chunk(input_dict):\n",
    "    \"\"\"Function to create an opinion_changes dictionary only for a chunk of data.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): chunk of data\n",
    "\n",
    "    Returns:\n",
    "        dict: opinion_changes dictionary for a chunk of data\n",
    "    \"\"\"    \n",
    "    # Process a chunk of the input dictionary\n",
    "    processed_dict = {}\n",
    "    counter = 0\n",
    "    progress = 0.0001\n",
    "    \n",
    "    for group, rows_indexes in input_dict.items():\n",
    "        processed_values = process_sentiments_for_group(rows_indexes)\n",
    "        if processed_values:  # only add non-empty lists to the dictionary\n",
    "            processed_dict[group] = processed_values\n",
    "\n",
    "        counter += 1\n",
    "        if ((counter / len(input_dict)) >= progress):\n",
    "            print(f\"{counter} / {len(input_dict)} entries processed...\\n\")\n",
    "            progress += 0.0001\n",
    "        if counter == len(input_dict):\n",
    "            print(f\"Thread has finished processing all {len(input_dict)} entries.\")\n",
    "\n",
    "\n",
    "    return processed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dict_in_parallel(input_dict, num_processes=None):\n",
    "    \"\"\"Function to process the input dictionary of reactions in parallel and merge the atomic results together into a single dictionary,\n",
    "    which will be the final opinion_changes dictionary.\n",
    "\n",
    "    Args:\n",
    "        input_dict (dict): dictionary of reactions grouped by some columns\n",
    "                           (we expect the columns to be 'author_id' and 'reference_id')\n",
    "        num_processes (int): number of parallel(worker) threads/processes. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: the final opinion_changes dictionary, which contains all the pairs of 'author_id' and 'reference_id'\n",
    "              (and their respective rows in the original dataframe) in the whole dataset, where an opinion change occured\n",
    "    \"\"\"    \n",
    "    # Default to using all available CPU cores\n",
    "    if num_processes is None:\n",
    "        num_processes = multiprocessing.cpu_count()\n",
    "    print(f'Chose number of processes: {num_processes}')\n",
    "\n",
    "    # Split the input dictionary into smaller chunks for parallel processing\n",
    "    chunk_size = len(input_dict) // num_processes if len(input_dict) // num_processes != 0 else 1\n",
    "    input_chunks = [dict(list(input_dict.items())[i:i + chunk_size]) for i in range(0, len(input_dict), chunk_size)]\n",
    "    print(f'Splitted input dictionary into {len(input_chunks)} chunks of size {chunk_size}')\n",
    "\n",
    "    # Process the input chunks in parallel using a pool of worker processes\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        processed_dicts = pool.map(process_dict_chunk, input_chunks)\n",
    "\n",
    "    # Merge the processed dictionaries from each input chunk\n",
    "    processed_dict = {}\n",
    "    for d in processed_dicts:\n",
    "        processed_dict.update(d)\n",
    "\n",
    "    return processed_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAVE DICTIONARY TO JSON FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_opinion_changes_to_JSON(opinion_changes, reaction_types):\n",
    "    \"\"\"Function to save the dictionary of opinion changes to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        opinion_changes (dict): dictionary with opinion changes\n",
    "        reaction_types (list): list of reaction types\n",
    "    \"\"\"    \n",
    "    path = create_path_to_opinion_changes(reaction_types)\n",
    "\n",
    "    # create a new dictionary with string keys\n",
    "    opinion_changes_for_JSON_file = {str(key): value for key, value in opinion_changes.items() }\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(opinion_changes_for_JSON_file, file, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_OC(merged_days, reaction_types_full_list):\n",
    "    for reaction_types in reaction_types_full_list:\n",
    "        groups_of_reactions = group_reactions(merged_days, reaction_types)\n",
    "        opinion_changes_parallel = process_dict_in_parallel(groups_of_reactions)\n",
    "        save_opinion_changes_to_JSON(opinion_changes_parallel, reaction_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose number of processes: 8\n",
      "Splitted input dictionary into 0 chunks of size 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module '__main__' has no attribute '__spec__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m create_all_OC(merged_days, reaction_types_full_list)\n",
      "Cell \u001b[1;32mIn[45], line 4\u001b[0m, in \u001b[0;36mcreate_all_OC\u001b[1;34m(merged_days, reaction_types_full_list)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m reaction_types \u001b[39min\u001b[39;00m reaction_types_full_list:\n\u001b[0;32m      3\u001b[0m     groups_of_reactions \u001b[39m=\u001b[39m group_reactions(merged_days, reaction_types)\n\u001b[1;32m----> 4\u001b[0m     opinion_changes_parallel \u001b[39m=\u001b[39m process_dict_in_parallel(groups_of_reactions)\n\u001b[0;32m      5\u001b[0m     save_opinion_changes_to_JSON(opinion_changes_parallel, reaction_types)\n",
      "Cell \u001b[1;32mIn[43], line 25\u001b[0m, in \u001b[0;36mprocess_dict_in_parallel\u001b[1;34m(input_dict, num_processes)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSplitted input dictionary into \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(input_chunks)\u001b[39m}\u001b[39;00m\u001b[39m chunks of size \u001b[39m\u001b[39m{\u001b[39;00mchunk_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39m# Process the input chunks in parallel using a pool of worker processes\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mwith\u001b[39;00m multiprocessing\u001b[39m.\u001b[39;49mPool(processes\u001b[39m=\u001b[39;49mnum_processes) \u001b[39mas\u001b[39;00m pool:\n\u001b[0;32m     26\u001b[0m     processed_dicts \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39mmap(process_dict_chunk, input_chunks)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Merge the processed dictionaries from each input chunk\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:119\u001b[0m, in \u001b[0;36mBaseContext.Pool\u001b[1;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Returns a process pool object'''\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpool\u001b[39;00m \u001b[39mimport\u001b[39;00m Pool\n\u001b[1;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m Pool(processes, initializer, initargs, maxtasksperchild,\n\u001b[0;32m    120\u001b[0m             context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_context())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\pool.py:215\u001b[0m, in \u001b[0;36mPool.__init__\u001b[1;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_processes \u001b[39m=\u001b[39m processes\n\u001b[0;32m    214\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 215\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_repopulate_pool()\n\u001b[0;32m    216\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\pool.py:306\u001b[0m, in \u001b[0;36mPool._repopulate_pool\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_repopulate_pool\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_repopulate_pool_static(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ctx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mProcess,\n\u001b[0;32m    307\u001b[0m                                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_processes,\n\u001b[0;32m    308\u001b[0m                                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inqueue,\n\u001b[0;32m    309\u001b[0m                                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_outqueue, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initializer,\n\u001b[0;32m    310\u001b[0m                                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initargs,\n\u001b[0;32m    311\u001b[0m                                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maxtasksperchild,\n\u001b[0;32m    312\u001b[0m                                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\pool.py:329\u001b[0m, in \u001b[0;36mPool._repopulate_pool_static\u001b[1;34m(ctx, Process, processes, pool, inqueue, outqueue, initializer, initargs, maxtasksperchild, wrap_exception)\u001b[0m\n\u001b[0;32m    327\u001b[0m w\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m w\u001b[39m.\u001b[39mname\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mProcess\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPoolWorker\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    328\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m    330\u001b[0m pool\u001b[39m.\u001b[39mappend(w)\n\u001b[0;32m    331\u001b[0m util\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39madded worker\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py:45\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m---> 45\u001b[0m     prep_data \u001b[39m=\u001b[39m spawn\u001b[39m.\u001b[39;49mget_preparation_data(process_obj\u001b[39m.\u001b[39;49m_name)\n\u001b[0;32m     47\u001b[0m     \u001b[39m# read end of pipe will be duplicated by the child process\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[39m# -- see spawn_main() in spawn.py.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[39m# bpo-33929: Previously, the read end of pipe was \"stolen\" by the child\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[39m# process, but it leaked a handle if the child process had been\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[39m# terminated before it could steal the handle from the parent process.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     rhandle, whandle \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39mCreatePipe(\u001b[39mNone\u001b[39;00m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\spawn.py:183\u001b[0m, in \u001b[0;36mget_preparation_data\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39m# Figure out whether to initialise main in the subprocess as a module\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[39m# or through direct execution (or to leave it alone entirely)\u001b[39;00m\n\u001b[0;32m    182\u001b[0m main_module \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[\u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 183\u001b[0m main_mod_name \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(main_module\u001b[39m.\u001b[39;49m__spec__, \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m main_mod_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     d[\u001b[39m'\u001b[39m\u001b[39minit_main_from_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m main_mod_name\n",
      "\u001b[1;31mAttributeError\u001b[0m: module '__main__' has no attribute '__spec__'"
     ]
    }
   ],
   "source": [
    "create_all_OC(merged_days, reaction_types_full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
