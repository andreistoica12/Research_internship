{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from kneed import KneeLocator\n",
    "import networkx as nx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = '/home/andreistoica12/research-internship'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where we store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/andreistoica12/research-internship/data/PhemeDataset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: After running the code, some files from the dataset will be different from the original versions, i.e. the \"retweets.json\" files inside each story folder were initially invalid. In order to consider and process the retweets in the longitudinal analysis, I formatted these files so that they are valid, parsable JSON files. If the file contained only one retweet object, it has not been modified. If the file contained multiple retweets, the file now contains a list of retweet objects, separated by a comma, as per the JSON syntax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paths to the folder containing all subfolders corresponding to each event of major interest (the Charlie Hebdo shooting, footballer Essien having Ebola, etc.). Tweets here are all written in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = data_path + \"/threads/en\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 subfolders to store important files and graphs, respectively. If they already existed (from previous runnings of the project), delete the folders and their contents and create empty folders to store the current files and graphs, relevant to the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = os.path.join(rootdir_path, 'files')\n",
    "if os.path.exists(files_path):\n",
    "   shutil.rmtree(files_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(files_path)\n",
    "\n",
    "graphs_path = os.path.join(rootdir_path, 'graphs')\n",
    "if os.path.exists(graphs_path):\n",
    "   shutil.rmtree(graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_graphs_path = os.path.join(graphs_path, 'pheme')\n",
    "if os.path.exists(pheme_graphs_path):\n",
    "   shutil.rmtree(pheme_graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_longitudinal_analysis_graphs = os.path.join(pheme_graphs_path, 'longitudinal-analysis')\n",
    "if os.path.exists(pheme_longitudinal_analysis_graphs):\n",
    "   shutil.rmtree(pheme_longitudinal_analysis_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_longitudinal_analysis_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_reaction_times_graphs = os.path.join(pheme_graphs_path, 'reaction-times')\n",
    "if os.path.exists(pheme_reaction_times_graphs):\n",
    "   shutil.rmtree(pheme_reaction_times_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_reaction_times_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function that first reads the JSON file and stores it into a dictionary, then parses the date contained at the \"created_at\" key. The number returned is an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_hour(tweet_path):\n",
    "    with open(tweet_path) as f:\n",
    "        tweet = json.load(f)\n",
    "    date = parser.parse(tweet['created_at'])\n",
    "    \n",
    "    return date.hour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return the source path, given the story path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_tweet_path(story_path):\n",
    "    source_dir_path = story_path + \"/source-tweets\"\n",
    "    source_path = source_dir_path + \"/\" + os.listdir(source_dir_path)[0]\n",
    "    \n",
    "    return source_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of all reactions' paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaction_tweets_paths(story_path):\n",
    "    reactions_paths_list = []\n",
    "    reactions_dir_path = story_path + \"/reactions\"\n",
    "    for reaction_name in os.listdir(reactions_dir_path):\n",
    "        reaction_path = reactions_dir_path + \"/\" + reaction_name\n",
    "        reactions_paths_list.append(reaction_path)\n",
    "        \n",
    "    return reactions_paths_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to validate whether a given JSON file is valid or not. Unmodified retweets files (the ones from teh original dataset) are not valid. We only want ot modify them when they are invalid, otherwise they will become invalid again, as we will have duplicate characters (\"[[\" / \"]]\" / \",,\") ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateJSON(JSON_path):\n",
    "    try:\n",
    "        with open(JSON_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    except ValueError as err:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to modify/format invalid JSON files for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retweets_json(retweets_path):\n",
    "    if not validateJSON(retweets_path):\n",
    "        with open(retweets_path, 'r') as invalid_json:\n",
    "            data = invalid_json.read()\n",
    "        data = \"[\\n\" + data.replace(\"}\\n{\", \"},\\n{\") + \"]\"\n",
    "        with open(retweets_path,'w') as valid_json:\n",
    "            valid_json.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of the hours when all retweets of a specific story have been posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_retweets(story_path):\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    hours = []\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            hours = [ parser.parse(retweet['created_at']).hour for retweet in retweets_list ]\n",
    "        else:   # we have this case when the JSON file contains one object, but we need to pass a list forward, so we'll have a 1-length list\n",
    "            hours = [parser.parse(retweets_list['created_at']).hour]\n",
    "\n",
    "    return hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define a function to store all occurences of dates (only the hours) in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_story(story_path):\n",
    "    # I create a list with all occurences of dates corresponding to the source tweet, reactions (replies) and retweets.\n",
    "    hours = []\n",
    "\n",
    "    # source hour\n",
    "    source_path = source_tweet_path(story_path)\n",
    "    hour = tweet_hour(source_path)\n",
    "    hours.append(hour)\n",
    "\n",
    "    # reactions hours\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        hour = tweet_hour(reaction_path)\n",
    "        hours.append(hour)\n",
    "    \n",
    "    # retweets hours\n",
    "    hours.extend(hours_list_retweets(story_path))\n",
    "    \n",
    "    return hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function to return a pandas Series, representing the distribution of the hours of tweets (source tweets, reactions and retweets) posted regarding a specific event given as an input parameter. I chose to convert the list to a pandas Series due to the ease in creating a distribution and corresponding box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distribution_event(event_path):\n",
    "    hours = []\n",
    "    for story_id in os.listdir(event_path):\n",
    "        story_path = event_path + \"/\" + story_id\n",
    "        hours.extend(hours_list_story(story_path))\n",
    "    hours.sort()\n",
    "    hours_series = pd.Series(hours)\n",
    "    distribution = hours_series.value_counts()[hours_series.unique()]\n",
    "    \n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is delegated to plot the distribution per hour of the tweets sent about a specific topic/event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_distribution(event_name, distribution):\n",
    "    axes = distribution.plot(kind='bar')\n",
    "    figure_path = f\"{pheme_longitudinal_analysis_graphs}/{event_name}_distribution.png\"\n",
    "    axes.figure.savefig(figure_path)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot and save in the local graphs/ folder the distributions corresponding to all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_event_distributions(events_path):\n",
    "    for event in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event\n",
    "        distribution = time_distribution_event(event_path)\n",
    "        plot_event_distribution(event, distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_event_distributions(events_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the reaction times to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_types = ['hours', 'minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between 'hours' and 'minutes' if you want to plot the final graph in hours or minutes\n",
    "NOTE: This variable needs to have either one of the values in the deltas_types list, otherwise the deltas will be an empty list,\n",
    "so the plots will make no sense in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_type = deltas_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_in_minutes_story(story_path):\n",
    "    deltas = []\n",
    "    # Step 1: get t0 datetime object from the source timestamp\n",
    "    source_path = source_tweet_path(story_path)\n",
    "    with open(source_path) as file:\n",
    "        source = json.load(file)\n",
    "    t0 = parser.parse(source['created_at'])\n",
    "\n",
    "    # Step 2: for all reactions, get the difference in minutes from the time the source was posted and the time each reaction was posted\n",
    "    # reactions_count = 0\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        # reactions_count += 1\n",
    "        with open(reaction_path) as file:\n",
    "            reaction = json.load(file)\n",
    "        deltas.append((parser.parse(reaction['created_at']) - t0).total_seconds() / 60)\n",
    "    # print(f\"nr of reactions = {reactions_count}\")\n",
    "\n",
    "    # Step 3: for all retweets, get the same time difference in miuntes as above\n",
    "    # retweets_count = 0\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            deltas.extend([ (parser.parse(retweet['created_at']) - t0).total_seconds() / 60 for retweet in retweets_list ])\n",
    "        else:   # here, the JSON file contains one object, but we need to pass a list forward, so we'll have a 1-length list\n",
    "            deltas.extend([ (parser.parse(retweets_list['created_at']) - t0).total_seconds() / 60 ])\n",
    "        # retweets_count = len(retweets_list)\n",
    "        # print(f\"nr of retweets = {retweets_count}\")\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_event(event_path, deltas_type):\n",
    "    deltas = []\n",
    "    if deltas_type == 'hours':\n",
    "        for story_name in os.listdir(event_path):\n",
    "            story_path = event_path + \"/\" + story_name\n",
    "            deltas.extend([min / 60 for min in deltas_in_minutes_story(story_path)])\n",
    "            lista = [min / 60 for min in deltas_in_minutes_story(story_path)]\n",
    "            # print(f\"story {story_name}:\")\n",
    "            # print(f\"len(lista) = {len(lista)}\")\n",
    "            # print(f\"len(set(lista)) = {len(set(lista))}\")\n",
    "            # print(lista)\n",
    "    elif deltas_type == 'minutes':\n",
    "        for story_name in os.listdir(event_path):\n",
    "            story_path = event_path + \"/\" + story_name\n",
    "            deltas.extend(deltas_in_minutes_story(story_path))\n",
    "    else:\n",
    "        pass    # TODO: evaluate this case maybe differently\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_all_events(events_path, deltas_type):\n",
    "    deltas_all_events = {}\n",
    "    for event_name in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event_name\n",
    "        deltas_all_events[event_name] = deltas_event(event_path, deltas_type)\n",
    "    \n",
    "    return deltas_all_events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the number of clusters we want to have, as it needs to be passed onto the K-Means algorithm. Therefore, one of the easiest methods is to look for the \"elbow\" point in the plot above, i.e. the point where the rate of decrease in WCSS begins to level off. The optimal number of clusters can be found through teh KneeLocator function within the kneed Python module. Curves with positive concavity are called \"elbows\", the ones with negative concavity are called \"knees\". Our K-Means inertia values are decreasing when we increase the number of clusters, because the inertia is calculated by measuring the distance between each data point and its centroid, so having more clusters means points will be closer to their clusters' centroids. So we will have a decreasing convex curve and thus we will pass on this information as arguments in the KneeLocator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method_k_means_clusters(deltas, max_nr_clusters):\n",
    "    wcss = []   # within-cluster sum of squares\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "    for k in range(1, max_nr_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=0)\n",
    "        kmeans.fit(deltas)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    kn = KneeLocator(range(1, 11), wcss, curve='convex', direction='decreasing')\n",
    "\n",
    "    # # If you want to visualize the knee point of the graph, you can plot the graph using the following function, \n",
    "    # # specifically designed to highlight the knee point\n",
    "    # kn.plot_knee()\n",
    "\n",
    "    # # Otherwise, you can plot a graph to your own liking, e.g. the one below\n",
    "    # # Plot the within-cluster sum of squares against the number of clusters\n",
    "    # plt.plot(range(1, 11), wcss)\n",
    "    # plt.title('Elbow Method')\n",
    "    # plt.xlabel('Number of clusters')\n",
    "    # plt.ylabel('WCSS')\n",
    "    # plt.show()\n",
    "\n",
    "    return kn.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(k, deltas):\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    # Create a KMeans object with the specified number of clusters\n",
    "    kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=0)\n",
    "    kmeans.fit(deltas)\n",
    "\n",
    "    # Get the labels assigned to each data point\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Get the centroids of each cluster\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Print an overview of the resulting clusters\n",
    "    for i in range(k):\n",
    "        cluster_data = deltas[labels == i]\n",
    "        print(f\"Cluster {i+1} has {len(cluster_data)} data points and a centroid of {centroids[i][0]}\")\n",
    "\n",
    "    # Create a dictionary to store the clustered data\n",
    "    clusters_dict = {}\n",
    "    labels_list = []\n",
    "    for label in labels:\n",
    "        labels_list.append(label)\n",
    "    \n",
    "    for index, label in enumerate(labels_list):\n",
    "        if label not in clusters_dict:\n",
    "            clusters_dict[label] = [deltas[index][0]]\n",
    "        else:\n",
    "            clusters_dict[label].append(deltas[index][0])\n",
    "    \n",
    "    return clusters_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below makes use of the Nearest Neighbors algorithm in order to find the optimal value for epsilon, as per:\n",
    "https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc\n",
    "However, the distances plot seems odd and the knee obtained through the KneeLocator function doesn't yield the best results. For now, I set the eps parameter to be 0.3 so that we have good clustering results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, I tried to cluster the deltas from all the stories within the Germanwings crash event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = deltas_event('/home/andreistoica12/research-internship/data/PhemeDataset/threads/en/germanwings-crash', deltas_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_DBSCAN(deltas):\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=5) # usually, the kth nearest neighbor is chosen somewhere between 3 and 10\n",
    "    nbrs = neigh.fit(deltas)\n",
    "    distances, indices = nbrs.kneighbors(deltas)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    plt.plot(distances)\n",
    "\n",
    "    kn = KneeLocator(range(1, len(distances)+1), distances, curve='convex', direction='increasing')\n",
    "    # epsilon = distances[kn.knee]\n",
    "\n",
    "    return float(distances[kn.knee])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the graph seems odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6830555555555549"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_DBSCAN(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCAN_clustering(deltas):\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    # # Compute the value for epsilon using the function defined previously\n",
    "    # epsilon = epsilon_DBSCAN(deltas)\n",
    "    epsilon = 0.3\n",
    "\n",
    "    # Create a DBSCAN object with epsilon as the computed value and minimum samples=5\n",
    "    dbscan = DBSCAN(eps=epsilon, min_samples=5)\n",
    "\n",
    "    # Fit the DBSCAN object to the data\n",
    "    dbscan.fit(deltas)\n",
    "\n",
    "    # Get the labels assigned to each data point\n",
    "    labels = dbscan.labels_\n",
    "\n",
    "    # Print the number of clusters and the labels assigned to each data point\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(\"Number of clusters:\", n_clusters)\n",
    "    print(\"Labels:\", labels)\n",
    "\n",
    "    # Create a dictionary to store the clusters\n",
    "    clusters_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label in clusters_dict:\n",
    "            clusters_dict[label].append(deltas[i][0])\n",
    "        else:\n",
    "            clusters_dict[label] = [deltas[i][0]]\n",
    "    \n",
    "    return clusters_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, I have implemented 2 ways to cluster the deltas. By default, I use the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), because it focuses on the patterns that we observe from the majority of data points, and the outliers don't affect the way it clusters the other points, whereas the K-Means algorithm builds its centroids based on all the data points and it may be deceiving in a plot. Moreover, plots obtained with the DBSCAN clustering algorithm tend to yield more clusters than the number of clusters obtained with the help of the Elbow Method in the case of the K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algorithms = ['k-means', 'DBSCAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algorithm = clustering_algorithms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_all_events(events_path, deltas_type, clustering_algorithm):\n",
    "    all_events_deltas = deltas_all_events(events_path, deltas_type)\n",
    "    all_events_clusters = {}\n",
    "    for event_name, event_deltas in all_events_deltas.items():\n",
    "        if clustering_algorithm == 'k-means':\n",
    "            # loop over maximum 10 clusters, as this is a range where you usually find the optimum number of clusters\n",
    "            k = elbow_method_k_means_clusters(event_deltas, 10)\n",
    "            print(f\"\\nEvent {event_name}:\")\n",
    "            print(f\"NOTE: Numbers represent {deltas_type}\\n\")\n",
    "            all_events_clusters[event_name] = k_means_clustering(k, event_deltas)\n",
    "        elif clustering_algorithm == 'DBSCAN':\n",
    "            all_events_clusters[event_name] = DBSCAN_clustering(event_deltas)\n",
    "        \n",
    "    return all_events_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reaction_times(events_path, deltas_type):\n",
    "    all_clusters = clusters_all_events(events_path, deltas_type, clustering_algorithm)\n",
    "\n",
    "    for event_name, clusters in all_clusters.items():\n",
    "        clusters_for_plot = {}\n",
    "        for key, value in clusters.items():\n",
    "            clusters_for_plot[f\"{math.floor(min(value))} - {math.ceil(max(value))}\"] = len(value)\n",
    "\n",
    "        intervals_unsorted = list(clusters_for_plot.keys())\n",
    "        values_unsorted = list(clusters_for_plot.values())\n",
    "        df_plot = pd.DataFrame(\n",
    "            dict(\n",
    "                Interval=intervals_unsorted,\n",
    "                Value=values_unsorted\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        df_plot['Start of interval'] = df_plot['Interval'].str.split(' - ').str[0]\n",
    "        df_plot['Start of interval'] = df_plot['Start of interval'].apply(pd.to_numeric) \n",
    "        df_plot_sorted = df_plot.sort_values('Start of interval')\n",
    "        intervals = list(df_plot_sorted['Interval'])\n",
    "        values = list(df_plot_sorted['Value'])\n",
    "\n",
    "\n",
    "        # # calling the function to add value labels\n",
    "        # addlabels(intervals, values)\n",
    "        plt.bar(range(len(clusters_for_plot)), values, tick_label=intervals)\n",
    "        # Rotate the x-axis labels by 45 degrees\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f'Distribution of reaction times in {deltas_type}')\n",
    "        plt.xlabel(f'Reaction times (between x and y {deltas_type})')\n",
    "        plt.ylabel('Number of reactions')\n",
    "        plt.savefig(pheme_reaction_times_graphs + f\"/{event_name}_reaction_times.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the cell below runs for ~30 sec on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 12\n",
      "Labels: [-1  0  1 ...  1  1  3]\n",
      "Number of clusters: 6\n",
      "Labels: [0 0 0 ... 0 0 0]\n",
      "Number of clusters: 11\n",
      "Labels: [ 0  0  0 ... -1 -1  3]\n",
      "Number of clusters: 5\n",
      "Labels: [0 0 0 ... 0 0 0]\n",
      "Number of clusters: 18\n",
      "Labels: [ 0  0  0 ...  0  0 12]\n",
      "Number of clusters: 2\n",
      "Labels: [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 -1  1  1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0 -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  1  1  0  1  1  0  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "Number of clusters: 4\n",
      "Labels: [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1 -1 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  3  3 -1  1  0  1  0  0  0  0  0 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  2  2\n",
      "  2  2  2 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  3  3 -1]\n",
      "Number of clusters: 4\n",
      "Labels: [ 0  0  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1 -1 -1  2  2\n",
      " -1 -1 -1 -1 -1 -1 -1  0  0  0  0  0  0  0  1  0  0  0  1  0  0  0  0  0\n",
      "  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  3  3  2  2 -1 -1 -1\n",
      "  0  0  0  0 -1 -1  0  0  0  0  0  0  2  0 -1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  3  3  3\n",
      "  2  2  2 -1 -1 -1 -1 -1  0  0  0  0  0  0  0  0  0  1  3  3  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1  1  1\n",
      "  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "plot_reaction_times(events_path, deltas_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: see how to make K-Means have all uninterfering deltas (if I can even do that)\n",
    "# (e.g. in cluser 1, centroid 2, there is a delta of 4.4 and in cluster 2, centroid 15, there is a delta of 3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: document new functions, explain how I sorted the columns in graphs for all events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: see what DBSCAN does exactly and how it is different than K-Means (we can see sub clusters, we usually have more clusters etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
