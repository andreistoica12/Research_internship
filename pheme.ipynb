{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "import math\n",
    "from kneed import KneeLocator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = '/home/andreistoica12/research-internship'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where we store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/andreistoica12/research-internship/data/PhemeDataset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: After running the code, some files from the dataset will be different from the original versions, i.e. the \"retweets.json\" files inside each story folder were initially invalid. In order to consider and process the retweets in the longitudinal analysis, I formatted these files so that they are valid, parsable JSON files. If the file contained only one retweet object, it has not been modified. If the file contained multiple retweets, the file now contains a list of retweet objects, separated by a comma, as per the JSON syntax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paths to the folder containing all subfolders corresponding to each event of major interest (the Charlie Hebdo shooting, footballer Essien having Ebola, etc.). Tweets here are all written in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = data_path + \"/threads/en\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 subfolders to store important files and graphs, respectively. If they already existed (from previous runnings of the project), delete the folders and their contents and create empty folders to store the current files and graphs, relevant to the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = os.path.join(rootdir_path, 'files')\n",
    "if os.path.exists(files_path):\n",
    "   shutil.rmtree(files_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(files_path)\n",
    "\n",
    "graphs_path = os.path.join(rootdir_path, 'graphs')\n",
    "if os.path.exists(graphs_path):\n",
    "   shutil.rmtree(graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_graphs_path = os.path.join(graphs_path, 'pheme')\n",
    "if os.path.exists(pheme_graphs_path):\n",
    "   shutil.rmtree(pheme_graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_longitudinal_analysis_graphs = os.path.join(pheme_graphs_path, 'longitudinal-analysis')\n",
    "if os.path.exists(pheme_longitudinal_analysis_graphs):\n",
    "   shutil.rmtree(pheme_longitudinal_analysis_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_longitudinal_analysis_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_reaction_times_graphs = os.path.join(pheme_graphs_path, 'reaction-times')\n",
    "if os.path.exists(pheme_reaction_times_graphs):\n",
    "   shutil.rmtree(pheme_reaction_times_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_reaction_times_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function that first reads the JSON file and stores it into a dictionary, then parses the date contained at the \"created_at\" key. The number returned is an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_hour(tweet_path):\n",
    "    with open(tweet_path) as f:\n",
    "        tweet = json.load(f)\n",
    "    date = parser.parse(tweet['created_at'])\n",
    "    \n",
    "    return date.hour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return the source path, given the story path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_tweet_path(story_path):\n",
    "    source_dir_path = story_path + \"/source-tweets\"\n",
    "    source_path = source_dir_path + \"/\" + os.listdir(source_dir_path)[0]\n",
    "    \n",
    "    return source_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of all reactions' paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaction_tweets_paths(story_path):\n",
    "    reactions_paths_list = []\n",
    "    reactions_dir_path = story_path + \"/reactions\"\n",
    "    for reaction_name in os.listdir(reactions_dir_path):\n",
    "        reaction_path = reactions_dir_path + \"/\" + reaction_name\n",
    "        reactions_paths_list.append(reaction_path)\n",
    "        \n",
    "    return reactions_paths_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to validate whether a given JSON file is valid or not. Unmodified retweets files (the ones from teh original dataset) are not valid. We only want ot modify them when they are invalid, otherwise they will become invalid again, as we will have duplicate characters (\"[[\" / \"]]\" / \",,\") ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateJSON(JSON_path):\n",
    "    try:\n",
    "        with open(JSON_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    except ValueError as err:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to modify/format invalid JSON files for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retweets_json(retweets_path):\n",
    "    if not validateJSON(retweets_path):\n",
    "        with open(retweets_path, 'r') as invalid_json:\n",
    "            data = invalid_json.read()\n",
    "        data = \"[\\n\" + data.replace(\"}\\n{\", \"},\\n{\") + \"]\"\n",
    "        with open(retweets_path,'w') as valid_json:\n",
    "            valid_json.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of the hours when all retweets of a specific story have been posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_retweets(story_path):\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    hours = []\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            hours = [ parser.parse(retweet['created_at']).hour for retweet in retweets_list ]\n",
    "        else:   # we have this case when the JSON file contains one object, but we need to pass a list forward, so we'll have a 1-length list\n",
    "            hours = [parser.parse(retweets_list['created_at']).hour]\n",
    "\n",
    "    return hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define a function to store all occurences of dates (only the hours) in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_story(story_path):\n",
    "    # I create a list with all occurences of dates corresponding to the source tweet, reactions (replies) and retweets.\n",
    "    hours = []\n",
    "\n",
    "    # source hour\n",
    "    source_path = source_tweet_path(story_path)\n",
    "    hour = tweet_hour(source_path)\n",
    "    hours.append(hour)\n",
    "\n",
    "    # reactions hours\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        hour = tweet_hour(reaction_path)\n",
    "        hours.append(hour)\n",
    "    \n",
    "    # retweets hours\n",
    "    hours.extend(hours_list_retweets(story_path))\n",
    "    \n",
    "    return hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function to return a pandas Series, representing the distribution of the hours of tweets (source tweets, reactions and retweets) posted regarding a specific event given as an input parameter. I chose to convert the list to a pandas Series due to the ease in creating a distribution and corresponding box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distribution_event(event_path):\n",
    "    hours = []\n",
    "    for story_id in os.listdir(event_path):\n",
    "        story_path = event_path + \"/\" + story_id\n",
    "        hours.extend(hours_list_story(story_path))\n",
    "    hours.sort()\n",
    "    hours_series = pd.Series(hours)\n",
    "    distribution = hours_series.value_counts()[hours_series.unique()]\n",
    "    \n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is delegated to plot the distribution per hour of the tweets sent about a specific topic/event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_distribution(event_name, distribution):\n",
    "    axes = distribution.plot(kind='bar')\n",
    "    figure_path = f\"{pheme_longitudinal_analysis_graphs}/{event_name}_distribution.png\"\n",
    "    axes.figure.savefig(figure_path)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot and save in the local graphs/ folder the distributions corresponding to all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_event_distributions(events_path):\n",
    "    for event in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event\n",
    "        distribution = time_distribution_event(event_path)\n",
    "        plot_event_distribution(event, distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_event_distributions(events_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the reaction times to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_types = ['hours', 'minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between 'hours' and 'minutes' if you want to plot the final graph in hours or minutes\n",
    "NOTE: This variable needs to have either one of the values in the deltas_types list, otherwise the deltas will be an empty list,\n",
    "so the plots will make no sense in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_type = deltas_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_in_minutes_story(story_path):\n",
    "    deltas = []\n",
    "    # Step 1: get t0 datetime object from the source timestamp\n",
    "    source_path = source_tweet_path(story_path)\n",
    "    with open(source_path) as file:\n",
    "        source = json.load(file)\n",
    "    t0 = parser.parse(source['created_at'])\n",
    "\n",
    "    # Step 2: for all reactions, get the difference in minutes from the time the source was posted and the time each reaction was posted\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        with open(reaction_path) as file:\n",
    "            reaction = json.load(file)\n",
    "        deltas.append((parser.parse(reaction['created_at']) - t0).total_seconds() / 60)\n",
    "\n",
    "    # Step 3: for all retweets, get the same time difference in miuntes as above\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            deltas.extend([ (parser.parse(retweet['created_at']) - t0).total_seconds() / 60 for retweet in retweets_list ])\n",
    "        else:   # we have this case when the JSON file contains one object, but we need to pass a list forward, so we'll have a 1-length list\n",
    "            deltas.extend([ (parser.parse(retweets_list['created_at']) - t0).total_seconds() / 60 ])\n",
    "    \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_event(event_path, deltas_type):\n",
    "    deltas = []\n",
    "    if deltas_type == 'hours':\n",
    "        for story_name in os.listdir(event_path):\n",
    "            story_path = event_path + \"/\" + story_name\n",
    "            deltas.extend([min / 60 for min in deltas_in_minutes_story(story_path)])\n",
    "    elif deltas_type == 'minutes':\n",
    "        for story_name in os.listdir(event_path):\n",
    "            story_path = event_path + \"/\" + story_name\n",
    "            deltas.extend(deltas_in_minutes_story(story_path))\n",
    "    else:\n",
    "        pass    # TODO: evaluate this case maybe differently\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_all_events(events_path, deltas_type):\n",
    "    deltas_all_events = {}\n",
    "    for event_name in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event_name\n",
    "        deltas_all_events[event_name] = deltas_event(event_path, deltas_type)\n",
    "    \n",
    "    return deltas_all_events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the number of clusters we want to have, as it needs to be passed onto the K-Means algorithm. Therefore, one of the easiest methods is to look for the \"elbow\" point in the plot above, i.e. the point where the rate of decrease in WCSS begins to level off. The optimal number of clusters can be found through teh KneeLocator function within the kneed Python module. Curves with positive concavity are called \"elbows\", the ones with negative concavity are called \"knees\". Our K-Means inertia values are decreasing when we increase the number of clusters, because the inertia is calculated by measuring the distance between each data point and its centroid, so having more clusters means points will be closer to their clusters' centroids. So we will have a decreasing convex curve and thus we will pass on this information as arguments in the KneeLocator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knee_elbow_method(deltas):\n",
    "    wcss = []\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "    for k in range(1, 11):  # loop over maximum 10 clusters, as this is a range where you usually find the optimum number of clusters\n",
    "        kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=0)\n",
    "        kmeans.fit(deltas)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    kn = KneeLocator(range(1, 11), wcss, curve='convex', direction='decreasing')\n",
    "\n",
    "    # # If you want to visualize the knee point of the graph, you can plot the graph using the following function, \n",
    "    # # specifically designed to highlight the knee point\n",
    "    # kn.plot_knee()\n",
    "\n",
    "    # # Otherwise, you can plot a graph to your own liking, e.g. the one below\n",
    "    # # Plot the within-cluster sum of squares against the number of clusters\n",
    "    # plt.plot(range(1, 11), wcss)\n",
    "    # plt.title('Elbow Method')\n",
    "    # plt.xlabel('Number of clusters')\n",
    "    # plt.ylabel('WCSS')\n",
    "    # plt.show()\n",
    "\n",
    "    return kn.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(k, deltas):\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    # Create a KMeans object with the specified number of clusters\n",
    "    kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=0)\n",
    "    kmeans.fit(deltas)\n",
    "\n",
    "    # Get the labels assigned to each data point\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Get the centroids of each cluster\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Print an overview of the resulting clusters\n",
    "    for i in range(k):\n",
    "        cluster_data = deltas[labels == i]\n",
    "        print(f\"Cluster {i+1} has {len(cluster_data)} data points and a centroid of {centroids[i][0]}\")\n",
    "\n",
    "    # Create a dictionary to store the clustered data\n",
    "    clusters_dict = {}\n",
    "    labels_list = []\n",
    "    for label in labels:\n",
    "        labels_list.append(label)\n",
    "    \n",
    "    for index, label in enumerate(labels_list):\n",
    "        if label not in clusters_dict:\n",
    "            clusters_dict[label] = [deltas[index][0]]\n",
    "        else:\n",
    "            clusters_dict[label].append(deltas[index][0])\n",
    "    \n",
    "    return clusters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_all_events(events_path, deltas_type):\n",
    "    all_events_deltas = deltas_all_events(events_path, deltas_type)\n",
    "    all_events_clusters = {}\n",
    "    for event_name, event_deltas in all_events_deltas.items():\n",
    "        k = knee_elbow_method(event_deltas)\n",
    "        print(f\"\\nEvent {event_name}:\")\n",
    "        print(f\"NOTE: Numbers represent {deltas_type}\\n\")\n",
    "        all_events_clusters[event_name] = k_means_clustering(k, event_deltas)\n",
    "        \n",
    "    return all_events_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reaction_times(events_path, deltas_type):\n",
    "    all_clusters = clusters_all_events(events_path, deltas_type)\n",
    "\n",
    "    for event_name, clusters in all_clusters.items():\n",
    "        clusters_for_plot = {}\n",
    "        for key, value in clusters.items():\n",
    "            clusters_for_plot[f\"{math.floor(min(value))} - {math.ceil(max(value))}\"] = len(value)\n",
    "\n",
    "        intervals_unsorted = list(clusters_for_plot.keys())\n",
    "        values_unsorted = list(clusters_for_plot.values())\n",
    "        df_plot = pd.DataFrame(\n",
    "            dict(\n",
    "                Interval=intervals_unsorted,\n",
    "                Value=values_unsorted\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        df_plot['Start of interval'] = df_plot['Interval'].str.split(' - ').str[0]\n",
    "        df_plot[['Start of interval']] = df_plot[['Start of interval']].apply(pd.to_numeric) \n",
    "        df_plot_sorted = df_plot.sort_values('Start of interval')\n",
    "        intervals = list(df_plot_sorted['Interval'])\n",
    "        values = list(df_plot_sorted['Value'])\n",
    "\n",
    "\n",
    "        plt.bar(range(len(clusters_for_plot)), values, tick_label=intervals)\n",
    "        plt.title(f'Distribution of reaction times in {deltas_type}')\n",
    "        plt.xlabel(f'Reaction times (between x and y {deltas_type})')\n",
    "        plt.ylabel('Number of reactions')\n",
    "        plt.savefig(pheme_reaction_times_graphs + f\"/{event_name}_reaction_times.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Event germanwings-crash:\n",
      "NOTE: Numbers represent hours\n",
      "\n",
      "Cluster 1 has 4376 data points and a centroid of 1.3818614792809263\n",
      "Cluster 2 has 118 data points and a centroid of 34.01820621468927\n",
      "Cluster 3 has 6 data points and a centroid of 122.24287037037037\n",
      "\n",
      "Event ottawashooting:\n",
      "NOTE: Numbers represent hours\n",
      "\n",
      "Cluster 1 has 805 data points and a centroid of 6.109261207370737\n",
      "Cluster 2 has 15520 data points and a centroid of 0.47379596213471975\n",
      "Cluster 3 has 73 data points and a centroid of 23.894132420091324\n",
      "\n",
      "Event ferguson:\n",
      "NOTE: Numbers represent hours\n",
      "\n",
      "Cluster 1 has 7490 data points and a centroid of 2.20710536270583\n",
      "Cluster 2 has 175 data points and a centroid of 70.24017619047618\n",
      "Cluster 3 has 1 data points and a centroid of 728.7561111111112\n",
      "\n",
      "Event sydneysiege:\n",
      "NOTE: Numbers represent hours\n",
      "\n",
      "Cluster 1 has 12036 data points and a centroid of 0.4261060799084233\n",
      "Cluster 2 has 629 data points and a centroid of 6.176699788023319\n",
      "Cluster 3 has 140 data points and a centroid of 20.708478174603172\n",
      "\n",
      "Event charliehebdo:\n",
      "NOTE: Numbers represent hours\n",
      "\n",
      "Cluster 1 has 23174 data points and a centroid of 0.684136126093732\n",
      "Cluster 2 has 263 data points and a centroid of 28.708329092451226\n",
      "Cluster 3 has 58 data points and a centroid of 82.04913314176245\n",
      "\n",
      "Event prince-toronto:\n",
      "NOTE: Numbers represent hours\n",
      "\n",
      "Cluster 1 has 3 data points and a centroid of 16.946851851851854\n",
      "Cluster 2 has 196 data points and a centroid of 0.5289611678004535\n",
      "Cluster 3 has 16 data points and a centroid of 6.367951388888889\n",
      "\n",
      "Event ebola-essien:\n",
      "NOTE: Numbers represent hours\n",
      "\n",
      "Cluster 1 has 381 data points and a centroid of 1.6405526392534266\n",
      "Cluster 2 has 7 data points and a centroid of 58.856785714285714\n",
      "Cluster 3 has 17 data points and a centroid of 18.629264705882353\n",
      "\n",
      "Event putinmissing:\n",
      "NOTE: Numbers represent hours\n",
      "\n",
      "Cluster 1 has 286 data points and a centroid of 1.8814423076923084\n",
      "Cluster 2 has 12 data points and a centroid of 37.247314814814814\n",
      "Cluster 3 has 2 data points and a centroid of 104.27888888888889\n"
     ]
    }
   ],
   "source": [
    "plot_reaction_times(events_path, deltas_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: see how to make K-Means have all uninterfering deltas \n",
    "# (e.g. in cluser 1, centroid 2, there is a delta of 4.4 and in cluster 2, centroid 15, there is a delta of 3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: document new functions, explain how I sorted the columns in graphs for all events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
