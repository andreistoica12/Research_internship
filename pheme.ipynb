{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = '/home/andreistoica12/research-internship'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where we store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/andreistoica12/research-internship/data/PhemeDataset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: After running the code, some files from the dataset will be different from the original versions, i.e. the \"retweets.json\" files inside each story folder were initially invalid. In order to consider and process the retweets in the longitudinal analysis, I formatted these files so that they are valid, parsable JSON files. If the file contained only one retweet object, it has not been modified. If the file contained multiple retweets, the file now contains a list of retweet objects, separated by a comma, as per the JSON syntax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paths to the folder containing all subfolders corresponding to each event of major interest (the Charlie Hebdo shooting, footballer Essien having Ebola, etc.). Tweets here are all written in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = data_path + \"/threads/en\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 subfolders to store important files and graphs, respectively. If they already existed (from previous runnings of the project), delete the folders and their contents and create empty folders to store the current files and graphs, relevant to the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = os.path.join(rootdir_path, 'files')\n",
    "if os.path.exists(files_path):\n",
    "   shutil.rmtree(files_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(files_path)\n",
    "\n",
    "graphs_path = os.path.join(rootdir_path, 'graphs')\n",
    "if os.path.exists(graphs_path):\n",
    "   shutil.rmtree(graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(graphs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function that first reads the JSON file and stores it into a dictionary, then parses the date contained at the \"created_at\" key. The number returned is an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_hour(tweet_path):\n",
    "    with open(tweet_path) as f:\n",
    "        tweet = json.load(f)\n",
    "    date = parser.parse(tweet['created_at'])\n",
    "    return date.hour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return the source path, given the story path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_tweet_path(story_path):\n",
    "    source_dir_path = story_path + \"/source-tweets\"\n",
    "    source_path = source_dir_path + \"/\" + os.listdir(source_dir_path)[0]\n",
    "    \n",
    "    return source_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of all reactions' paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaction_tweets_paths(story_path):\n",
    "    reactions_paths_list = []\n",
    "    reactions_dir_path = story_path + \"/reactions\"\n",
    "    for reaction_name in os.listdir(reactions_dir_path):\n",
    "        reaction_path = reactions_dir_path + \"/\" + reaction_name\n",
    "        reactions_paths_list.append(reaction_path)\n",
    "        \n",
    "    return reactions_paths_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to validate whether a given JSON file is valid or not. Unmodified retweets files (the ones from teh original dataset) are not valid. We only want ot modify them when they are invalid, otherwise they will become invalid again, as we will have duplicate characters (\"[[\" / \"]]\" / \",,\") ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateJSON(JSON_path):\n",
    "    try:\n",
    "        with open(JSON_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    except ValueError as err:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to modify/format invalid JSON files for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retweets_json(retweets_path):\n",
    "    if not validateJSON(retweets_path):\n",
    "        with open(retweets_path, 'r') as invalid_json:\n",
    "            data = invalid_json.read()\n",
    "        data = \"[\\n\" + data.replace(\"}\\n{\", \"},\\n{\") + \"]\"\n",
    "        with open(retweets_path,'w') as valid_json:\n",
    "            valid_json.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_retweets(story_path):\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    hours = []\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            hours = [ parser.parse(retweet['created_at']).hour for retweet in retweets_list ]\n",
    "        else:   # we haev this case when the JSON file contains one object, but we need to pass a list forward, so we'll have a 1-length list\n",
    "            hours = [parser.parse(retweets_list['created_at']).hour]\n",
    "\n",
    "    return hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define a function to store all occurences of dates (only the hours) in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_story(story_path):\n",
    "    # I create a list with all occurences of dates corresponding to the source tweet, reactions (replies) and retweets.\n",
    "    hours = []\n",
    "\n",
    "    # source hour\n",
    "    source_path = source_tweet_path(story_path)\n",
    "    hour = tweet_hour(source_path)\n",
    "    hours.append(hour)\n",
    "\n",
    "    # reactions hours\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        hour = tweet_hour(reaction_path)\n",
    "        hours.append(hour)\n",
    "    \n",
    "    # retweets hours\n",
    "    hours.extend(hours_list_retweets(story_path))\n",
    "    \n",
    "    return hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function to return a pandas Series, representing the distribution of the hours of tweets (source tweets, reactions and retweets) posted regarding a specific event given as an input parameter. I chose to convert the list to a pandas Series due to the ease in creating a distribution and corresponding box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distribution_event(event_path):\n",
    "    hours = []\n",
    "    for story_id in os.listdir(event_path):\n",
    "        story_path = event_path + \"/\" + story_id\n",
    "        hours.extend(hours_list_story(story_path))\n",
    "    hours.sort()\n",
    "    hours_series = pd.Series(hours)\n",
    "    distribution = hours_series.value_counts()[hours_series.unique()]\n",
    "    \n",
    "    return distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is delegated to plot the distribution per hour of the tweets sent about a specific topic/event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_distribution(event_name, distribution):\n",
    "    axes = distribution.plot(kind='bar')\n",
    "    figure_path = \"{graphs_path}/{event}_distribution.png\".format(graphs_path = graphs_path, event = event_name)\n",
    "    axes.figure.savefig(figure_path)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot and save in the local graphs/ folder the distributions corresponding to all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_event_distributions(events_path):\n",
    "    for event in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event\n",
    "        distribution = time_distribution_event(event_path)\n",
    "        plot_event_distribution(event, distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_event_distributions(events_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
