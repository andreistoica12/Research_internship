{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from kneed import KneeLocator\n",
    "import textwrap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = '/home/andreistoica12/research-internship'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where we store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/andreistoica12/research-internship/data/PhemeDataset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: After running the code, some files from the dataset will be different from the original versions, i.e. the \"retweets.json\" files inside each story folder were initially invalid. In order to consider and process the retweets in the longitudinal analysis, I formatted these files so that they are valid, parsable JSON files. If the file contained only one retweet object, it has not been modified. If the file contained multiple retweets, the file now contains a list of retweet objects, separated by a comma, as per the JSON syntax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paths to the folder containing all subfolders corresponding to each event of major interest (the Charlie Hebdo shooting, footballer Essien having Ebola, etc.). Tweets here are all written in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = data_path + \"/threads/en\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code snippets that create 2 folders, along with respective subfolders, within the root folder of the project.\n",
    "Those folders will store locally graphs obtained through running the code, as well as important auxiliary files.\n",
    "If they already existed (from previous runnings of the project), I delete the tree strarting from those folders \n",
    "(all subfolders and contents) and create empty folders and subfolders, relevant for the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_subfolders(root_folder, subfolders_tree):\n",
    "    for key, value in subfolders_tree.items():\n",
    "        globals()[key] = os.path.join(root_folder, value)\n",
    "        if os.path.exists(globals()[key]):\n",
    "            shutil.rmtree(globals()[key], ignore_errors=False, onerror=None)\n",
    "        os.makedirs(globals()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_subfolders_tree = {\n",
    "    'files_path': 'files',\n",
    "    'graphs_path': 'graphs'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_subfolders(rootdir_path, rootdir_subfolders_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_subfolders_tree = {\n",
    "    'pheme_graphs_path': 'pheme'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_subfolders(globals()['graphs_path'], graphs_subfolders_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_graphs_subfolders_tree = {\n",
    "    'pheme_time_distributions_graphs': 'time-distributions',\n",
    "    'pheme_reaction_times_graphs': 'reaction-times',\n",
    "    'pheme_reaction_counts_graphs': 'reaction-counts',\n",
    "    'pheme_avg_reaction_counts_graphs': 'avg-reaction-counts',\n",
    "    'pheme_influencers_source_graphs': 'influencers-source',\n",
    "    'pheme_influencers_all_graphs': 'influencers-all',\n",
    "    'pheme_avg_followers_counts_source_graphs': 'avg-followers-counts-source',\n",
    "    'pheme_avg_followers_counts_all_graphs': 'avg-followers-counts-all',\n",
    "    'pheme_following_graphs': 'following',\n",
    "    'pheme_avg_following_graphs': 'avg-following',\n",
    "    'pheme_following_percentages_graphs': 'following-percentages'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_subfolders(globals()['pheme_graphs_path'], pheme_graphs_subfolders_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function that first reads the JSON file and stores it into a dictionary, then parses the date contained at the \"created_at\" key. The number returned is an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_hour(tweet_path):\n",
    "    \"\"\"Function that parses a JSON file associated with a tweet in the PhemeDataset and returns the posting hour.\n",
    "\n",
    "    Args:\n",
    "        tweet_path (str): path to the JSON file associated with a tweet\n",
    "\n",
    "    Returns:\n",
    "        int: posting hour of a tweet\n",
    "    \"\"\"    \n",
    "    with open(tweet_path) as f:\n",
    "        tweet = json.load(f)\n",
    "    date = parser.parse(tweet['created_at'])\n",
    "    \n",
    "    return date.hour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return the source path, given the story path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_tweet_path(story_path):\n",
    "    \"\"\"Function that, given the path to a story, gets the path to the JSON file corresponding to the source tweet.\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        str: path to the source tweet JSON file\n",
    "    \"\"\"    \n",
    "    source_dir_path = story_path + \"/source-tweets\"\n",
    "    source_path = source_dir_path + \"/\" + os.listdir(source_dir_path)[0]\n",
    "    \n",
    "    return source_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of all reactions' paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaction_tweets_paths(story_path):\n",
    "    \"\"\"Function that generates a list of reactions (replies) to a tweet within a story.\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        list: list of paths(strings) for the reactions to the source tweet of a story\n",
    "    \"\"\"    \n",
    "    reactions_paths_list = []\n",
    "    reactions_dir_path = story_path + \"/reactions\"\n",
    "    for reaction_name in os.listdir(reactions_dir_path):\n",
    "        reaction_path = reactions_dir_path + \"/\" + reaction_name\n",
    "        reactions_paths_list.append(reaction_path)\n",
    "        \n",
    "    return reactions_paths_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to validate whether a given JSON file is valid or not. Unmodified retweets files (the ones from the original dataset) are not valid. We only want ot modify them when they are invalid, otherwise they will become invalid again, as we will have duplicate characters (\"[[\" / \"]]\" / \",,\") ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateJSON(JSON_path):\n",
    "    \"\"\"Function that checks whether a JSON file is valid or invalid.\n",
    "\n",
    "    Args:\n",
    "        JSON_path (str): path to a JSON file\n",
    "\n",
    "    Returns:\n",
    "        bool: True for a valid JSON file, False otherwise\n",
    "    \"\"\"    \n",
    "    try:\n",
    "        with open(JSON_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    except ValueError as err:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to modify/format invalid JSON files for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retweets_json(retweets_path):\n",
    "    \"\"\"Function that transforms an invalid JSON file which contains multiple objects, not correctly separated through a comma and\n",
    "    not having square brackets at the beginning and end of the file, into a valid JSON file.\n",
    "\n",
    "    Args:\n",
    "        retweets_path (str): path to the invalid JSON file\n",
    "    \"\"\"    \n",
    "    if not validateJSON(retweets_path):\n",
    "        with open(retweets_path, 'r') as invalid_json:\n",
    "            data = invalid_json.read()\n",
    "        data = \"[\\n\" + data.replace(\"}\\n{\", \"},\\n{\") + \"]\"\n",
    "        with open(retweets_path,'w') as valid_json:\n",
    "            valid_json.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of the hours when all retweets of a specific story have been posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_retweets(story_path):\n",
    "    \"\"\"Function that generates a list of the posting hours of all retweets.\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        list: list of the posting hours for all retweet occurences.\n",
    "    \"\"\"    \n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    hours = []\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            hours = [ parser.parse(retweet['created_at']).hour for retweet in retweets_list ]\n",
    "        else:   # we have this case when the JSON file contains one object, but we need to pass a list forward, so we'll have a 1-length list\n",
    "            hours = [parser.parse(retweets_list['created_at']).hour]\n",
    "\n",
    "    return hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define a function to store all occurences of dates (only the hours) in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_story(story_path):\n",
    "    \"\"\"Function that generates a list of the posting hours of all tweets (source tweet, replies, retweets) within a story.\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        list: list of the posting hours for all tweets (source tweet, replies, retweets) within a story.\n",
    "    \"\"\"    \n",
    "    # I create a list with all occurences of dates corresponding to the source tweet, reactions (replies) and retweets.\n",
    "    hours = []\n",
    "\n",
    "    # source hour\n",
    "    source_path = source_tweet_path(story_path)\n",
    "    hour = tweet_hour(source_path)\n",
    "    hours.append(hour)\n",
    "\n",
    "    # reactions hours\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        hour = tweet_hour(reaction_path)\n",
    "        hours.append(hour)\n",
    "    \n",
    "    # retweets hours\n",
    "    hours.extend(hours_list_retweets(story_path))\n",
    "    \n",
    "    return hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function to return a pandas Series, representing the distribution of the hours of tweets (source tweets, reactions and retweets) posted regarding a specific event given as an input parameter. I chose to convert the list to a pandas Series due to the ease in creating a distribution and corresponding box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distribution_event(event_path):\n",
    "    \"\"\"Function that creates a distribution of the posting hours of the tweets (source tweets, replies, retweets) related to an event.\n",
    "\n",
    "    Args:\n",
    "        event_path (str): path to the root folder of a event (e.g. /your/path/to/charliehebdo)\n",
    "\n",
    "    Returns:\n",
    "        pandas.core.series.Series: distribution of the posting hours of all tweets related to an event\n",
    "    \"\"\"    \n",
    "    hours = []\n",
    "    for story_id in os.listdir(event_path):\n",
    "        story_path = event_path + \"/\" + story_id\n",
    "        hours.extend(hours_list_story(story_path))\n",
    "    hours.sort()\n",
    "    hours_series = pd.Series(hours)\n",
    "    distribution = hours_series.value_counts()[hours_series.unique()]\n",
    "    \n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is delegated to plot the distribution per hour of the tweets sent about a specific topic/event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_distribution_event(event_name, distribution):\n",
    "    \"\"\"Function that saves the bar chart plot with the distribution of the posting hours of all tweets related to an event, locally.\n",
    "\n",
    "    Args:\n",
    "        event_name (str): name of an event (e.g. charliehebdo)\n",
    "        distribution (pandas.core.series.Series): distribution of the posting hours of all tweets related to an event\n",
    "    \"\"\"    \n",
    "    axes = distribution.plot(kind='bar')\n",
    "    figure_path = f\"{globals()['pheme_time_distributions_graphs']}/{event_name}_distribution.png\"\n",
    "    axes.figure.savefig(figure_path)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot and save in the local graphs/ folder the distributions corresponding to all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_distributions(events_path):\n",
    "    \"\"\"Function that saves a bar chart plot with the distribution of the posting hours of all tweets related to each event, locally.\n",
    "\n",
    "    Args:\n",
    "        events_path (str): path to the root folder of the events (e.g. /your/path/to/threads/en)\n",
    "    \"\"\"    \n",
    "    for event_name in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event_name\n",
    "        distribution = time_distribution_event(event_path)\n",
    "        plot_time_distribution_event(event_name, distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_time_distributions(events_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the reaction times to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_types = ['hours', 'minutes']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between 'hours' and 'minutes' if you want to plot the final graph in hours or minutes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This variable needs to have either one of the values in the deltas_types list, otherwise the deltas will be an empty list,\n",
    "so the plots will make no sense in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_type = deltas_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_story(story_path, deltas_type):\n",
    "    \"\"\"Function that generates a list of time differences between the moment a reaction (reply or retweet) has been posted\n",
    "    and the moment the source tweet of a story has been posted. Depending on the time unit of such time difference \n",
    "    (e.g. 'hours' or 'minutes'), the function outputs a list with numbers expressed in the respective type.\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "        deltas_type (str): time unit for time differences\n",
    "\n",
    "    Returns:\n",
    "        list: list of time differences expressed in deltas_type time units\n",
    "    \"\"\"    \n",
    "    if deltas_type not in ['minutes', 'hours']:\n",
    "        print(\"Deltas type doesn't have a valid value - it should be either 'hours' or 'minutes' !\")\n",
    "        return []\n",
    "    \n",
    "    factor = 60 * 60\n",
    "    factor = 60 if deltas_type == 'minutes' else factor\n",
    "\n",
    "    deltas = []\n",
    "\n",
    "    # Step 1: get t0 datetime object from the source timestamp\n",
    "    source_path = source_tweet_path(story_path)\n",
    "    with open(source_path) as file:\n",
    "        source = json.load(file)\n",
    "    t0 = parser.parse(source['created_at'])\n",
    "\n",
    "    # Step 2: for all reactions, get the difference in minutes/hours \n",
    "    # from the time the source was posted and the time each reaction was posted\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        with open(reaction_path) as file:\n",
    "            reaction = json.load(file)\n",
    "        deltas.append((parser.parse(reaction['created_at']) - t0).total_seconds() / factor )\n",
    "\n",
    "    # Step 3: for all retweets, get the same time difference in miuntes as above\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            deltas.extend([ (parser.parse(retweet['created_at']) - t0).total_seconds() / factor for retweet in retweets_list ])\n",
    "        else:   # here, the JSON file contains one object, but we need to pass a list forward, so we'll have a 1-length list\n",
    "            deltas.extend([ (parser.parse(retweets_list['created_at']) - t0).total_seconds() / factor ])\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_event(event_path, deltas_type):\n",
    "    \"\"\"Function that generates a list of time differences between the source tweets of all stories of an event\n",
    "    and their respective reactions (replies and retweets).\n",
    "\n",
    "    Args:\n",
    "        event_path (str): path to the root folder of an event (e.g. /your/path/to/charliehebdo)\n",
    "        deltas_type (str): time unit for time differences (e.g. 'hours', 'minutes')\n",
    "\n",
    "    Returns:\n",
    "        list: list of time differences expressed in the time unit specified as an input parameter (deltas_type)\n",
    "    \"\"\"\n",
    "    deltas = []\n",
    "    for story_name in os.listdir(event_path):\n",
    "            story_path = event_path + \"/\" + story_name\n",
    "            deltas.extend(deltas_story(story_path, deltas_type))\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_all_events(events_path, deltas_type):\n",
    "    \"\"\"Function that generates a dictionary containing all time differences between the source tweets and their respective reactions\n",
    "    (replies and retweets) for all events.\n",
    "\n",
    "    Args:\n",
    "        events_path (str): path to the root folder of the events (e.g. /your/path/to/threads/en)\n",
    "        deltas_type (str): time unit for time differences (e.g. 'hours', 'minutes')\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary where keys are the event names and the values are lists of time differences\n",
    "              expressed in the time unit specified as an input parameter (deltas_type)\n",
    "    \"\"\"    \n",
    "    deltas_all_events = {}\n",
    "    for event_name in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event_name\n",
    "        deltas_all_events[event_name] = deltas_event(event_path, deltas_type)\n",
    "    \n",
    "    return deltas_all_events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the number of clusters we want to have, as it needs to be passed onto the K-Means algorithm. Therefore, one of the easiest methods is to look for the \"elbow\" point in the plot above, i.e. the point where the rate of decrease in WCSS begins to level off. The optimal number of clusters can be found through the KneeLocator function within the kneed Python module. Curves with positive concavity are called \"elbows\", the ones with negative concavity are called \"knees\". Our K-Means inertia values are decreasing when we increase the number of clusters, because the inertia is calculated by measuring the distance between each data point and its centroid, so having more clusters means points will be closer to their clusters' centroids. So we will have a decreasing convex curve and thus we will pass on this information as arguments in the KneeLocator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method_k_means_clusters(deltas, max_nr_clusters):\n",
    "    \"\"\"Function that calculates the optimal number of clusters that the K-Means clustering algorithm should create out of a list\n",
    "    of one-dimensional elements. The function implements the Elbow Method of choosing the optimal number of clusters, meaning it fits \n",
    "    multiple K-Means models with different numbers of clusters (from 1 to max_nr_clusters) and compute the within-cluster sum of squares\n",
    "    (wcss) for each model. The next step is to plot the wcss values against the number of clusters and look for the \"elbow\" point,\n",
    "    where the rate of decrease in wcss begins to level off. This is a good indication of the optimal number of clusters. This step is\n",
    "    performed with the help of the KneeLocator function within the kneed module.\n",
    "\n",
    "    Args:\n",
    "        deltas (list): one-dimensionaal list (e.g. time differences between the posting hour of a source tweet\n",
    "                       and the posting hours of the reactions - replies and retweets)\n",
    "        max_nr_clusters (int): maximum number of clusters that we take into account as potential optimal number to be returned\n",
    "\n",
    "    Returns:\n",
    "        int: optimal number of clusters for a K-Means clusterization\n",
    "    \"\"\"    \n",
    "    wcss = []   # within-cluster sum of squares\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "    for k in range(1, max_nr_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=0)\n",
    "        kmeans.fit(deltas)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    kn = KneeLocator(range(1, max_nr_clusters+1), wcss, curve='convex', direction='decreasing')\n",
    "\n",
    "    # # Plot the within-cluster sum of squares against the number of clusters\n",
    "    # # If you want to visualize the knee point of the graph, you can plot the graph using the following function, \n",
    "    # # specifically designed to highlight the knee point\n",
    "    # kn.plot_knee()\n",
    "\n",
    "    return kn.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(k, deltas):\n",
    "    \"\"\"Function to generate the clusters after performing the K-Means clustering algoithm to a one-dimensional array.\n",
    "\n",
    "    Args:\n",
    "        k (int): number of clusters\n",
    "        deltas (list): one-dimensional list (e.g. of time differences between the posting hour of a source tweet\n",
    "                       and the posting hours of the reactions - replies and retweets)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with keys as auto-generated labels of the clusters and values as lists of elements in each cluster\n",
    "    \"\"\"    \n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    # Create a KMeans object with the specified number of clusters\n",
    "    kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=0)\n",
    "    kmeans.fit(deltas)\n",
    "\n",
    "    # Get the labels assigned to each data point\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Get the centroids of each cluster\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # # Print an overview of the resulting clusters\n",
    "    # for i in range(k):\n",
    "    #     cluster_data = deltas[labels == i]\n",
    "    #     print(f\"Cluster {i+1} has {len(cluster_data)} data points and a centroid of {centroids[i][0]}\")\n",
    "\n",
    "    # Create a dictionary to store the clustered data\n",
    "    clusters_dict = {}\n",
    "    labels_list = []\n",
    "    for label in labels:\n",
    "        labels_list.append(label)\n",
    "    \n",
    "    for index, label in enumerate(labels_list):\n",
    "        if label not in clusters_dict:\n",
    "            clusters_dict[label] = [deltas[index][0]]\n",
    "        else:\n",
    "            clusters_dict[label].append(deltas[index][0])\n",
    "    \n",
    "    return clusters_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below makes use of the Nearest Neighbors algorithm in order to find the optimal value for epsilon, as per:\n",
    "https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc .\n",
    "However, the distances plot seems odd and the knee obtained through the KneeLocator function doesn't yield the best results. For now, I set the eps parameter to be 0.3 so that we have good clustering results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, I tried to cluster the deltas from all the stories within the Germanwings crash event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = deltas_event('/home/andreistoica12/research-internship/data/PhemeDataset/threads/en/prince-toronto', deltas_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_DBSCAN(deltas):\n",
    "    \"\"\"Function to compute the optimal value of the epsilon parameter - the most important input parameter - of the \n",
    "    DBSCAN clustering algorithm. The function first computes the distances between each point and its n-th (in our case, 5th)\n",
    "    nearest neighbor and chooses the optimal value for the distance using the Elbow Method (it creates a graph of the distances\n",
    "    against each data point and chooses the point where the curve starts to rise steeply).\n",
    "\n",
    "    Args:\n",
    "        deltas (list): one-dimensional list (e.g. of time differences between the posting hour of a source tweet\n",
    "                       and the posting hours of the reactions - replies and retweets)\n",
    "\n",
    "    Returns:\n",
    "        float: optimal epsilon value for DBSCAN\n",
    "    \"\"\"    \n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=5) # usually, the kth nearest neighbor is chosen somewhere between 3 and 10\n",
    "    nbrs = neigh.fit(deltas)\n",
    "    distances, indices = nbrs.kneighbors(deltas)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    plt.plot(distances)\n",
    "\n",
    "    kn = KneeLocator(range(1, len(distances)+1), distances, curve='convex', direction='increasing')\n",
    "\n",
    "    return float(distances[kn.knee])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the graph seems odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13027777777777816"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_DBSCAN(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCAN_clustering(deltas):\n",
    "    \"\"\"Function to cluster a list of one-dimensional elements with the help of the DBSCAN clustering algorithm.\n",
    "\n",
    "    Args:\n",
    "        deltas (list): list of one-dimensional elements (e.g. of time differences between the posting hour of a source tweet\n",
    "                       and the posting hours of the reactions - replies and retweets)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with keys as auto-generated labels of the clusters and values as lists of elements in each cluster\n",
    "    \"\"\"    \n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    # # Compute the value for epsilon using the function defined previously\n",
    "    # epsilon = epsilon_DBSCAN(deltas)\n",
    "    epsilon = 0.3\n",
    "\n",
    "    # Create a DBSCAN object with epsilon as the computed value and minimum samples=5\n",
    "    dbscan = DBSCAN(eps=epsilon, min_samples=5)\n",
    "\n",
    "    # Fit the DBSCAN object to the data\n",
    "    dbscan.fit(deltas)\n",
    "\n",
    "    # Get the labels assigned to each data point\n",
    "    labels = dbscan.labels_\n",
    "\n",
    "    # # Print the number of clusters and the labels assigned to each data point\n",
    "    # n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    # print(\"Number of clusters:\", n_clusters)\n",
    "    # print(\"Labels:\", labels)\n",
    "\n",
    "    # Create a dictionary to store the clusters\n",
    "    clusters_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label in clusters_dict:\n",
    "            clusters_dict[label].append(deltas[i][0])\n",
    "        else:\n",
    "            clusters_dict[label] = [deltas[i][0]]\n",
    "    \n",
    "    return clusters_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, I have implemented 2 ways to cluster the deltas. By default, I use the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), because it focuses on the patterns that we observe from the majority of data points, and the outliers don't affect the way it clusters the other points, whereas the K-Means algorithm builds its centroids based on all the data points and it may be deceiving in a plot. Moreover, plots obtained with the DBSCAN clustering algorithm tend to yield more and tighter clusters than the number of clusters obtained with the help of the Elbow Method in the case of the K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algorithms = ['k-means', 'DBSCAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algorithm = clustering_algorithms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_all_events(events_path, deltas_type, clustering_algorithm):\n",
    "    \"\"\"Function that generates clusters for all events, using the clustering algorithm and the time unit\n",
    "    for differences between the posting times of source tweets and their respective reactions' (replies and retweets) posting times,\n",
    "    both given as input parameters.\n",
    "\n",
    "    Args:\n",
    "        events_path (str): path to the root folder of the events (e.g. /your/path/to/threads/en)\n",
    "        deltas_type (str): time unit for time differences (e.g. 'hours', 'minutes')\n",
    "        clustering_algorithm (str): clustering algorithm used (e.g. 'k-means', 'DBSCAN')\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with keys as event names and values as dictionaries with clusters for each event\n",
    "    \"\"\"    \n",
    "    all_events_deltas = deltas_all_events(events_path, deltas_type)\n",
    "    all_events_clusters = {}\n",
    "    for event_name, event_deltas in all_events_deltas.items():\n",
    "        if clustering_algorithm == 'k-means':\n",
    "            # loop over maximum 10 clusters, as this is a range where you usually find the optimum number of clusters\n",
    "            k = elbow_method_k_means_clusters(event_deltas, 10)\n",
    "            # print(f\"\\nEvent {event_name}:\")\n",
    "            # print(f\"NOTE: Numbers represent {deltas_type}\\n\")\n",
    "            all_events_clusters[event_name] = k_means_clustering(k, event_deltas)\n",
    "        elif clustering_algorithm == 'DBSCAN':\n",
    "            all_events_clusters[event_name] = DBSCAN_clustering(event_deltas)\n",
    "        \n",
    "    return all_events_clusters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that receives the first clusters generated either through K-Means or DBSCAN and tries to see whether it can find some subgraphs more relevant for the visualization. A recursive function is defined in order to re-cluster the biggest cluster until the newly-computed biggest cluster contains fewer than 70% of the data points. I considered this a good threshold to stop, because the graph will clearly show, on one hand, which is the overwhelingly biggest cluster, and on the other hand, the next number_of_clusters - 1 biggest clusters/subclusters so we have a better overview of the most important clusters/subclusters in our datasets. I chose to apply K-Means for the reapplication of clustering on the biggest cluster due to its computationally-efficient nature, which is hands-down superior to DBSCAN's. In fact, not only did the algorithm implemented with DBSCAN for subclusters take a lot longer, it even crashed before the end, due to the number of recursive calls exceeding the limit of the kernel. Therefore, considering the trade-off between better clustering and efficiency, the option below yielded the optimal results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides technical reasons, I opted to first cluster the points with DBSCAN, due to its ability to ignore outlier points, they are not taken into account when clusters are created, which is of great help for a good visualization. After the first clusterization, we only have relevant points, so K-Means will yield good results (its problem were outlier points which influenced the computation of centroids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_for_plot(clusters, number_of_clusters):\n",
    "    \"\"\"Function that creates the most populated number_of_clusters (or less, if after completion, the number os clusters is smaller)\n",
    "    clusters or subclusters obtained after repeated clusterization. More specifically, the original list of data points is clustered\n",
    "    with either DBSCAN or K-Means. Then, we analyze the most populated cluster. If it contains more than 70% of all data points,\n",
    "    the function performs another clusterization on the biggest cluster. This operation is repeated until the most populated\n",
    "    cluster or subcluster gather no more than 70% of the data points.\n",
    "\n",
    "    Args:\n",
    "        clusters (dict): clusters of the original data points, obtained through either DBSCAN or K-Means\n",
    "        number_of_clusters (int): number of the biggest clusters or subclusters we want to save\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with keys as integers ranging from 0 to number_of_clusters and values as the biggest number_of_clusters clusters,\n",
    "              starting with the biggest and decreasing\n",
    "    \"\"\"    \n",
    "    total_length = 0\n",
    "    for value in clusters.values():\n",
    "            total_length += len(value)\n",
    "    subclusters_for_plot = {}\n",
    "    \n",
    "    def subclusters(clusters):\n",
    "            if(len(clusters)):\n",
    "                key_for_biggest_cluster = max(clusters, key=lambda k: len(clusters[k]))\n",
    "                if len(clusters[key_for_biggest_cluster]) / total_length < 0.7:\n",
    "                    print(\"Reached base case.\")\n",
    "                    return clusters\n",
    "                else:\n",
    "                    # print(\"We'll go back into subclusters function.\")\n",
    "                    # print(f\"Length of current clusters: {len(clusters)}\")\n",
    "                    # print(\"Current clusters look like this:\")\n",
    "                    # for key, value in clusters.items():\n",
    "                    #      print(f\"{key}: {math.floor(min(value))} - {math.ceil(max(value))}\")\n",
    "\n",
    "                    biggest_cluster = clusters[key_for_biggest_cluster]\n",
    "                    # print(\"Biggest cluster:\")\n",
    "                    # print(biggest_cluster[:3])\n",
    "                    \n",
    "                    del clusters[key_for_biggest_cluster]\n",
    "                    # print(\"Clusters after removing biggest_cluster:\")\n",
    "                    # for key, value in clusters.items():\n",
    "                    #      print(f\"{key}: {math.floor(min(value))} - {math.ceil(max(value))}\")\n",
    "                    \n",
    "                    # K-MEANS CLUSTERING\n",
    "                    k = elbow_method_k_means_clusters(biggest_cluster, 10)\n",
    "                    # print(f\"k = {k}\")\n",
    "                    \n",
    "                    subcl = k_means_clustering(k, biggest_cluster)\n",
    "                    # print(\"subcl - the subclusters dictionary obtained from the biggest_cluster looks like this:\")\n",
    "                    # for key, value in subcl.items():\n",
    "                    #      print(f\"{key}: {math.floor(min(value))} - {math.ceil(max(value))}\")\n",
    "                    \n",
    "                    for key, value in subcl.items():\n",
    "                        clusters[max(clusters.keys())+1] = value\n",
    "                    \n",
    "                    # print(f\"Length of clusters after modifications: {len(clusters)}\")\n",
    "                    # print(\"Clusters look like this:\")\n",
    "                    # for key, value in clusters.items():\n",
    "                    #      print(f\"{key}: {math.floor(min(value))} - {math.ceil(max(value))}\")\n",
    "                    \n",
    "                    subclusters(clusters)\n",
    "            else:\n",
    "                print(\"Clusters dictionary is empty.\")\n",
    "                return clusters\n",
    "            \n",
    "\n",
    "    subclusters(clusters)\n",
    "\n",
    "    if len(clusters) >= number_of_clusters:\n",
    "        for i in range(number_of_clusters):\n",
    "            key_for_biggest_cluster = max(clusters, key=lambda k: len(clusters[k]))\n",
    "            subclusters_for_plot[i] = clusters[key_for_biggest_cluster]\n",
    "            del clusters[key_for_biggest_cluster]\n",
    "    else:\n",
    "        subclusters_for_plot = clusters\n",
    "    \n",
    "    return subclusters_for_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reaction_times(events_path, deltas_type):\n",
    "    \"\"\"Function that saves a plot of the most populated clusters or subclusters for each event, locally, with labels in increasing order,\n",
    "    based on the posting hour of the earliest tweet in the cluster.\n",
    "\n",
    "    Args:\n",
    "        events_path (str): path to the root folder of the events (e.g. /your/path/to/threads/en)\n",
    "        deltas_type (str): time unit for time differences (e.g. 'hours', 'minutes')\n",
    "    \"\"\"    \n",
    "    all_clusters = clusters_all_events(events_path, deltas_type, clustering_algorithm)\n",
    "\n",
    "    for event_name, clusters in all_clusters.items():\n",
    "        print(f\"Event: {event_name}\")\n",
    "        clusters_to_be_plotted = clusters_for_plot(clusters, 5)\n",
    "\n",
    "        final_clusters_for_plot = {}\n",
    "        for key, value in clusters_to_be_plotted.items():\n",
    "            final_clusters_for_plot[f\"{math.floor(min(value))} - {math.ceil(max(value))}\"] = len(value)\n",
    "\n",
    "        intervals_unsorted = list(final_clusters_for_plot.keys())\n",
    "        values_unsorted = list(final_clusters_for_plot.values())\n",
    "        df_plot = pd.DataFrame(\n",
    "            dict(\n",
    "                Interval=intervals_unsorted,\n",
    "                Value=values_unsorted\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        df_plot['Start of interval'] = df_plot['Interval'].str.split(' - ').str[0]\n",
    "        df_plot['Start of interval'] = df_plot['Start of interval'].apply(pd.to_numeric) \n",
    "        df_plot_sorted = df_plot.sort_values('Start of interval')\n",
    "        intervals = list(df_plot_sorted['Interval'])\n",
    "        values = list(df_plot_sorted['Value'])\n",
    "\n",
    "\n",
    "        plt.bar(range(len(final_clusters_for_plot)), values, tick_label=intervals)\n",
    "        # Rotate the x-axis labels by 45 degrees\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f'Distribution of reaction times in {deltas_type}')\n",
    "        plt.xlabel(f'Reaction times (between x and y {deltas_type})')\n",
    "        plt.ylabel('Number of reactions')\n",
    "        plt.savefig(globals()['pheme_reaction_times_graphs'] + f\"/{event_name}_reaction_times.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the cell below runs for ~22 sec on my machine if the session has just started. If you run the code again, it takes ~18 sec, due to probably cached information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: germanwings-crash\n",
      "Reached base case.\n",
      "Event: ottawashooting\n",
      "Reached base case.\n",
      "Event: ferguson\n",
      "Reached base case.\n",
      "Event: sydneysiege\n",
      "Reached base case.\n",
      "Event: charliehebdo\n",
      "Reached base case.\n",
      "Event: prince-toronto\n",
      "Reached base case.\n",
      "Event: ebola-essien\n",
      "Reached base case.\n",
      "Event: putinmissing\n",
      "Reached base case.\n"
     ]
    }
   ],
   "source": [
    "plot_reaction_times(events_path, deltas_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reactions counts for the source tweets - the number of reactions per tweet posted at hour x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_followers_count(tweet_path):\n",
    "    \"\"\"Function that gets the followers count of the author of a tweet\n",
    "\n",
    "    Args:\n",
    "        tweet_path (str): path to a JSON file associated with a tweet \n",
    "        (e.g. /your/path/to/charliehebdo/552783667052167168/source-tweets/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        int: followers count of the user who posted the tweet\n",
    "    \"\"\"    \n",
    "    with open(tweet_path) as f:\n",
    "        tweet = json.load(f)\n",
    "    \n",
    "    return tweet['user']['followers_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_following(tweet_path):\n",
    "    \"\"\"Function that returns the value of the 'following' field of the 'user' in the JSON file associated with a tweet.\n",
    "\n",
    "    Args:\n",
    "        tweet_path (str): path to a JSON file associated with a tweet \n",
    "        (e.g. /your/path/to/charliehebdo/552783667052167168/reactions/552785374507175936.json)\n",
    "\n",
    "    Returns:\n",
    "        bool: true or false\n",
    "    \"\"\"    \n",
    "    with open(tweet_path) as f:\n",
    "        tweet = json.load(f)\n",
    "    \n",
    "    return tweet['user']['following']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_user_id(tweet_path):\n",
    "    \"\"\"Function that returns the value of the 'id' field of the 'user' in the JSON file associated with a tweet.\n",
    "\n",
    "    Args:\n",
    "        tweet_path (str): path to a JSON file associated with a tweet \n",
    "        (e.g. /your/path/to/charliehebdo/552783667052167168/reactions/552785374507175936.json)\n",
    "\n",
    "    Returns:\n",
    "        int: id of the user\n",
    "    \"\"\"    \n",
    "    with open(tweet_path) as f:\n",
    "        tweet = json.load(f)\n",
    "    \n",
    "    return tweet['user']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reactions_count_followers_count_for_source_tweet(story_path):\n",
    "    \"\"\"Function that generates the number of reactions (replies or retweets), as well as the number of followers of the author \n",
    "    for a single source tweet (a story contains one source tweet).\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        int: number of reactions for a source tweet, \n",
    "        int: followers counts of the user who posted the source tweet\n",
    "    \"\"\"  \n",
    "    source_path = source_tweet_path(story_path)\n",
    "    followers_count = tweet_followers_count(source_path)\n",
    "    \n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    reactions_count = len(reactions_paths_list)\n",
    "\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        reactions_count += len(retweets_list)\n",
    "\n",
    "    return reactions_count, followers_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def followers_list_for_user(user_id, who_follows_whom_path):\n",
    "    \"\"\"Function that generates the list of followers for a specified user, out of the users who participated in a thread/story.\n",
    "\n",
    "    Args:\n",
    "        user_id (int): id of the author of a tweet (source tweet, reply or retweet)\n",
    "        who_follows_whom_path (str): path to a 'who-follows-whom.dat' file within a story\n",
    "        (e.g. /your/path/to/charliehebdo/552783667052167168/who-follows-whom.dat)\n",
    "\n",
    "    Returns:\n",
    "        list: list of the id-s of the followers (out of the participants in a certain thread/story)\n",
    "    \"\"\"    \n",
    "    followers_list = []\n",
    "\n",
    "    # Define the regular expression delimiter - we have fixed-width columns in the .dat file, but the id-s have different length.\n",
    "    # Both the first column (the followers) and the second columns (the followed) contain id-s of users,\n",
    "    # delimited by a variable number of whitespaces. So we need a regular expression delimiter to parse the file.\n",
    "    delimiter = r'\\s+'\n",
    "\n",
    "    # Read the .dat file into a dataframe.\n",
    "    df = pd.read_csv(who_follows_whom_path, header=None, delimiter=delimiter, engine='python', names=['Follower', 'Followed'])\n",
    "\n",
    "    # Get the indices of all rows where Followed == user_id\n",
    "    idx = df.index[df['Followed'] == user_id].tolist()\n",
    "\n",
    "    # Create a list of the corresponding values in Follower\n",
    "    followers_list = [df.loc[i, 'Follower'] for i in idx]\n",
    "\n",
    "    return followers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def following_respondents_count_for_source_tweet(story_path):\n",
    "    \"\"\"Function that generates the number of followers for a source tweet author, out of the participants in a thread/story.\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        int: number of followers for a source tweet author, out of the respondents in a story\n",
    "    \"\"\"    \n",
    "    source_path = source_tweet_path(story_path)\n",
    "    source_user_id = tweet_user_id(source_path)\n",
    "\n",
    "    following = []\n",
    "    who_follows_whom_path = story_path + '/who-follows-whom.dat'\n",
    "    if os.path.exists(who_follows_whom_path):\n",
    "        following = followers_list_for_user(source_user_id, who_follows_whom_path)\n",
    "    else:\n",
    "        print(f\"who-follows-whom.dat file doesn't exist in story {os.path.basename(story_path)} - skipped this story!\")\n",
    "\n",
    "    following_respondents_count = 0\n",
    "\n",
    "    # Go through reactions first, check if the authors are followers of the source tweet author.\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        respondent_user_id = tweet_user_id(reaction_path)\n",
    "        if respondent_user_id in following:\n",
    "            following_respondents_count += 1\n",
    "\n",
    "    \n",
    "    # Go through retweets next, check if the authors are followers of the source tweet author.\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            for retweet in retweets_list:\n",
    "                respondent_user_id = retweet['user']['id']\n",
    "                if respondent_user_id in following:\n",
    "                    following_respondents_count += 1\n",
    "        else:\n",
    "            respondent_user_id = retweets_list['user']['id']\n",
    "            if respondent_user_id in following:\n",
    "                following_respondents_count += 1\n",
    "\n",
    "\n",
    "    return following_respondents_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaction_counts_influencers_avg_followers_counts_following_respondents_counts_and_percentages_per_tweet_event(event_path):\n",
    "    \"\"\"Function that generates the dictionaries of:\n",
    "     - average reaction counts (per source tweet) per posting hour\n",
    "     - absolute reaction counts (per all source tweets) per posting hour\n",
    "     - influencers (number of followers of the most followed account who posted a source tweet) per posting hour\n",
    "     - the average number of followers for source tweet authors per posting hour\n",
    "     - the absolute number of followers for source tweet authors, given the respondents to all stories related to an event\n",
    "     - the average number of followers (per source tweet author), given the respondents to all stories related to an event\n",
    "     - the percentage of followers out of respondents per (source tweet) posting hour\n",
    "     Based on this dictionaries, multiple plots can be made.\n",
    "\n",
    "    Args:\n",
    "        event_path (str): path to the root folder of an event (e.g. /your/path/to/charliehebdo)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with keys as posting hours of source tweets and values as the average number of reactions (per tweet) \n",
    "              per key (hour),\n",
    "        dict:  dictionary with keys as posting hours of source tweets and values as the total number of reactions to all source tweets\n",
    "               posted at each hour (key)\n",
    "        dict: dictionary with keys as posting hours of source tweets and values as the number of followers of the most followed user who\n",
    "              posted at each hour (key),\n",
    "        dict: dictionary with keys as posting hours of source tweets and values as the average number of followers of the users who\n",
    "              posted at each hour (key)\n",
    "        dict: dictionary with keys as posting hours of source tweets and values as the total number of followers for all source tweet\n",
    "              authors, given the respondents to all stories related to an event\n",
    "        dict: dictionary with keys as posting hours of source tweets and values as the average number of followers \n",
    "              (per source tweet author), given the respondents to all stories related to an event\n",
    "        dict: dictionary with keys as posting hours of source tweets and values as the percentage of followers out of respondents\n",
    "    \"\"\"    \n",
    "\n",
    "    number_of_source_tweets_per_hour = {}\n",
    "    reactions_count_per_hour = {}\n",
    "    max_source_followers_count_per_hour = {}\n",
    "    sum_source_followers_count_per_hour = {}\n",
    "    following_respondents_count_per_hour = {}\n",
    "    following_respondents_percentage_per_hour = {}\n",
    "    \n",
    "    for story_id in os.listdir(event_path):\n",
    "        story_path = event_path + \"/\" + story_id\n",
    "        source_path = source_tweet_path(story_path)\n",
    "        source_hour = tweet_hour(source_path)\n",
    "\n",
    "        if source_hour in reactions_count_per_hour:\n",
    "            number_of_source_tweets_per_hour[source_hour] += 1\n",
    "            reactions_count, followers_count = reactions_count_followers_count_for_source_tweet(story_path)\n",
    "            reactions_count_per_hour[source_hour] += reactions_count\n",
    "            max_source_followers_count_per_hour[source_hour] = max(max_source_followers_count_per_hour[source_hour], followers_count)\n",
    "            sum_source_followers_count_per_hour[source_hour] += followers_count\n",
    "\n",
    "            following_respondents_count = following_respondents_count_for_source_tweet(story_path)\n",
    "            following_respondents_count_per_hour[source_hour] += following_respondents_count\n",
    "            \n",
    "        else:\n",
    "            number_of_source_tweets_per_hour[source_hour] = 1\n",
    "            reactions_count, followers_count = reactions_count_followers_count_for_source_tweet(story_path)\n",
    "            reactions_count_per_hour[source_hour] = reactions_count\n",
    "            max_source_followers_count_per_hour[source_hour] = followers_count\n",
    "            sum_source_followers_count_per_hour[source_hour] = followers_count\n",
    "            \n",
    "            following_respondents_count = following_respondents_count_for_source_tweet(story_path)\n",
    "            following_respondents_count_per_hour[source_hour] = following_respondents_count\n",
    "\n",
    "            \n",
    "    reactions_per_tweet = {}\n",
    "    avg_source_followers_count_per_hour = {}\n",
    "    avg_following_respondents_count_per_hour = {}\n",
    "    for source_hour in number_of_source_tweets_per_hour:\n",
    "        reactions_per_tweet[source_hour] = round(reactions_count_per_hour[source_hour] / number_of_source_tweets_per_hour[source_hour])\n",
    "        avg_source_followers_count_per_hour[source_hour] = (\n",
    "            round(sum_source_followers_count_per_hour[source_hour] / number_of_source_tweets_per_hour[source_hour])\n",
    "        )\n",
    "        following_respondents_percentage_per_hour[source_hour] = (\n",
    "            round(following_respondents_count_per_hour[source_hour] / reactions_count_per_hour[source_hour] * 100, 1)\n",
    "        )\n",
    "        avg_following_respondents_count_per_hour[source_hour] = (\n",
    "            round(following_respondents_count_per_hour[source_hour] / number_of_source_tweets_per_hour[source_hour])\n",
    "        )\n",
    "\n",
    "\n",
    "    return (reactions_per_tweet, \n",
    "            reactions_count_per_hour, \n",
    "            max_source_followers_count_per_hour, \n",
    "            avg_source_followers_count_per_hour, \n",
    "            following_respondents_count_per_hour,\n",
    "            avg_following_respondents_count_per_hour,\n",
    "            following_respondents_percentage_per_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_avg_all_followers_count_per_hour_event(event_path):\n",
    "    \"\"\"Function that generates the number of followers of the most influential user participating in all threads of an event (per hour),\n",
    "    as well as the average number of followers of users (source tweets, replies or retweets authors) participating in all threads\n",
    "    of an event (per hour). \n",
    "\n",
    "    Args:\n",
    "        event_path (str): path to the root folder of an event (e.g. /your/path/to/charliehebdo)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with keys as posting hours of all tweets and values as the number of followers of the most\n",
    "              influential user per key (hour),\n",
    "        dict: dictionary with keys as posting hours of all tweets and values as the average number of followers out of all users\n",
    "              who participated in a thread per hour\n",
    "    \"\"\"\n",
    "  \n",
    "    def correct_followers_count_value(dict, hour, followers_count):\n",
    "        if type(followers_count) != int:\n",
    "            dict[hour] = 0\n",
    "        else:\n",
    "            dict[hour] = followers_count\n",
    "\n",
    "    number_of_all_tweets_per_hour = {}\n",
    "    sum_all_followers_count_per_hour = {}\n",
    "    max_all_followers_count_per_hour = {}\n",
    "\n",
    "    for story_id in os.listdir(event_path):\n",
    "        story_path = event_path + \"/\" + story_id\n",
    "        source_path = source_tweet_path(story_path)\n",
    "        source_hour = tweet_hour(source_path)\n",
    "        source_followers_count = tweet_followers_count(source_path)\n",
    "\n",
    "        if source_hour not in max_all_followers_count_per_hour:\n",
    "            number_of_all_tweets_per_hour[source_hour] = 1\n",
    "            correct_followers_count_value(sum_all_followers_count_per_hour, source_hour, source_followers_count)\n",
    "            correct_followers_count_value(max_all_followers_count_per_hour, source_hour, source_followers_count)\n",
    "        \n",
    "        # Go through reactions first.\n",
    "        reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "        for reaction_path in reactions_paths_list:\n",
    "            reaction_hour = tweet_hour(reaction_path)\n",
    "            reaction_followers_count = tweet_followers_count(reaction_path)\n",
    "            if reaction_hour not in max_all_followers_count_per_hour:\n",
    "                number_of_all_tweets_per_hour[reaction_hour] = 1\n",
    "                correct_followers_count_value(sum_all_followers_count_per_hour, reaction_hour, reaction_followers_count)\n",
    "                correct_followers_count_value(max_all_followers_count_per_hour, reaction_hour, reaction_followers_count)\n",
    "            else:\n",
    "                number_of_all_tweets_per_hour[reaction_hour] += 1\n",
    "                correct_followers_count_value(sum_all_followers_count_per_hour, reaction_hour,\n",
    "                                              sum_all_followers_count_per_hour[reaction_hour] + reaction_followers_count)\n",
    "                correct_followers_count_value(max_all_followers_count_per_hour, reaction_hour, \n",
    "                                              max(max_all_followers_count_per_hour[reaction_hour], reaction_followers_count))\n",
    "\n",
    "        # Next, go through retweets.\n",
    "        retweets_path = story_path + \"/retweets.json\"\n",
    "        if os.path.exists(retweets_path):\n",
    "            format_retweets_json(retweets_path)\n",
    "            with open(retweets_path, 'r') as file:\n",
    "                retweets_list = json.load(file)\n",
    "            if type(retweets_list) == list:\n",
    "                for retweet in retweets_list:\n",
    "                    retweet_hour = parser.parse(retweet['created_at']).hour\n",
    "                    retweet_followers_count = retweet['user']['followers_count']\n",
    "                    if retweet_hour not in max_all_followers_count_per_hour:\n",
    "                        number_of_all_tweets_per_hour[retweet_hour] = 1\n",
    "                        correct_followers_count_value(sum_all_followers_count_per_hour, retweet_hour, retweet_followers_count)\n",
    "                        correct_followers_count_value(max_all_followers_count_per_hour, retweet_hour, retweet_followers_count)\n",
    "                    else:\n",
    "                        number_of_all_tweets_per_hour[retweet_hour] += 1\n",
    "                        correct_followers_count_value(sum_all_followers_count_per_hour, retweet_hour,\n",
    "                                                      sum_all_followers_count_per_hour[retweet_hour] + retweet_followers_count)\n",
    "                        correct_followers_count_value(max_all_followers_count_per_hour, retweet_hour,\n",
    "                                                      max(max_all_followers_count_per_hour[retweet_hour], retweet_followers_count))\n",
    "            else:\n",
    "                retweet_hour = parser.parse(retweets_list['created_at']).hour\n",
    "                retweet_followers_count = retweets_list['user']['followers_count']\n",
    "                if retweet_hour not in max_all_followers_count_per_hour:\n",
    "                    number_of_all_tweets_per_hour[retweet_hour] = 1\n",
    "                    correct_followers_count_value(sum_all_followers_count_per_hour, retweet_hour, retweet_followers_count)\n",
    "                    correct_followers_count_value(max_all_followers_count_per_hour, retweet_hour, retweet_followers_count)\n",
    "                else:\n",
    "                    number_of_all_tweets_per_hour[retweet_hour] += 1\n",
    "                    correct_followers_count_value(sum_all_followers_count_per_hour, retweet_hour,\n",
    "                                                  sum_all_followers_count_per_hour[retweet_hour] + retweet_followers_count)\n",
    "                    correct_followers_count_value(max_all_followers_count_per_hour, retweet_hour,\n",
    "                                                  max(max_all_followers_count_per_hour[retweet_hour], retweet_followers_count))\n",
    "        \n",
    "    \n",
    "    \n",
    "    avg_all_followers_count_per_hour = {}\n",
    "    for hour in number_of_all_tweets_per_hour:\n",
    "        avg_all_followers_count_per_hour[hour] = round(sum_all_followers_count_per_hour[hour] / number_of_all_tweets_per_hour[hour])\n",
    "\n",
    "    \n",
    "\n",
    "    return max_all_followers_count_per_hour, avg_all_followers_count_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add value labels - adds the value of y\n",
    "def add_labels_y_value(x,y):\n",
    "    \"\"\"Function that takes the x and y-axis to be passed onto a plot function and generates labels,\n",
    "    such that on top of each y value, it is displayed centrally.\n",
    "\n",
    "    Args:\n",
    "        x (list): list of labels for x-axis of a plot\n",
    "        y (list): list of values for y-axis of a plot\n",
    "    \"\"\"    \n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i], y[i], ha = 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels_percentage(x, y):\n",
    "    \"\"\"Function that takes the x and y-axis to be passed onto a plot function and generates labels,\n",
    "    such that on top of each y value, the percentage of the y value out of the sum of all y-s is displayed as a red bounding box.\n",
    "    Useful when y-s represent the counts of occurences for some values.\n",
    "\n",
    "    Args:\n",
    "        x (list): list of labels for x-axis of a plot\n",
    "        y (list): list of values for y-axis of a plot\n",
    "    \"\"\"    \n",
    "    for i in range(len(x)):\n",
    "        percentage = y[i] / sum(y) * 100\n",
    "        plt.text(i, y[i], f\"{round(percentage, 1)}%\", ha = 'center',\n",
    "                 bbox = dict(facecolor = 'red', alpha =.7, pad=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_plot_labels_from_dict(dictionary):\n",
    "        \"\"\"Function that returns the sorted labels for a plot from a dictionary\n",
    "\n",
    "        Args:\n",
    "            dictionary (dict): dictionary ready to be plotted in th shape of a bar chart, \n",
    "            where the keys represent the x-axis and the values represent the y-axis\n",
    "\n",
    "        Returns:\n",
    "            tuple: the sorted lists of x and y-values for the plot\n",
    "        \"\"\"        \n",
    "        # sort the dictionary by keys\n",
    "        sorted_dict = sorted(dictionary.items())\n",
    "\n",
    "        # extract the sorted keys and values\n",
    "        sorted_x = [k for k, v in sorted_dict]\n",
    "        sorted_y = [v for k, v in sorted_dict]\n",
    "\n",
    "        return sorted_x, sorted_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reaction_counts_influencers_avg_followers_count_following_and_percentages_per_tweet_event(event_path):\n",
    "    \"\"\"Function that creates and saves locally multiple plots for an event in the dataset. \n",
    "    The plots show bar chart plots for all dictionaries returned by the: \n",
    "    - reaction_counts_influencers_avg_followers_counts_following_respondents_counts_and_percentages_per_tweet_event() \n",
    "    - max_avg_all_followers_count_per_hour_event()\n",
    "    functions, where the x-axis represents the posting hours for the tweets taken into account and the y-axis\n",
    "    is best described in the docstring of each of those functions.\n",
    "\n",
    "    Args:\n",
    "        event_path (str): path to the root folder of an event (e.g. /your/path/to/charliehebdo)\n",
    "    \"\"\"    \n",
    "    (reactions_per_tweet_dict, \n",
    "     reactions_count_per_hour_dict,\n",
    "     max_source_followers_count_per_hour_dict, \n",
    "     avg_source_followers_count_per_hour_dict, \n",
    "     followings_per_hour_dict,\n",
    "     avg_followings_per_hour_dict, \n",
    "     percentages_per_hour_dict) = (\n",
    "        reaction_counts_influencers_avg_followers_counts_following_respondents_counts_and_percentages_per_tweet_event(event_path)\n",
    "    )\n",
    "\n",
    "    max_all_followers_count_per_hour_dict, avg_all_followers_count_per_hour_dict = max_avg_all_followers_count_per_hour_event(event_path)\n",
    "\n",
    "\n",
    "    # PROCESS AVERAGE (PER TWEET) REACTION COUNTS\n",
    "    sorted_posting_hours, sorted_average_counts = sorted_plot_labels_from_dict(reactions_per_tweet_dict)\n",
    "\n",
    "    plt.bar(range(len(reactions_per_tweet_dict)), sorted_average_counts, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    # calling the function to add value labels - exact number of reactions\n",
    "    add_labels_y_value(sorted_posting_hours, sorted_average_counts)\n",
    "    graph_type = 'per tweet'\n",
    "    plt.title(f\"Reaction counts {graph_type} - {os.path.basename(event_path)} event\", loc=\"center\", pad=10)\n",
    "    plt.xlabel(\"Posting hour of source tweet\")\n",
    "    plt.ylabel(f\"Number of reactions {graph_type}\")\n",
    "    plt.savefig(globals()['pheme_avg_reaction_counts_graphs'] + f\"/{os.path.basename(event_path)}_reaction_counts_{graph_type}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # PROCESS ABSOLUTE REACTION COUNTS\n",
    "    sorted_posting_hours, sorted_absolute_counts = sorted_plot_labels_from_dict(reactions_count_per_hour_dict)\n",
    "\n",
    "    plt.bar(range(len(reactions_per_tweet_dict)), sorted_absolute_counts, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    # calling the function to add value labels - exact number of reactions\n",
    "    add_labels_y_value(sorted_posting_hours, sorted_absolute_counts)\n",
    "    graph_type = 'Absolute'\n",
    "    plt.title(f\"{graph_type} reaction counts - {os.path.basename(event_path)} event\", loc=\"center\", pad=10)\n",
    "    plt.xlabel(\"Posting hour of source tweet\")\n",
    "    plt.ylabel(f\"Number of reactions\")\n",
    "    plt.savefig(globals()['pheme_reaction_counts_graphs'] + f\"/{os.path.basename(event_path)}_reaction_counts.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # PROCESS INFLUENCERS - SOURCE TWEETS AUTHORS\n",
    "    sorted_posting_hours, sorted_source_follower_counts = sorted_plot_labels_from_dict(max_source_followers_count_per_hour_dict)\n",
    "\n",
    "    plt.bar(range(len(max_source_followers_count_per_hour_dict)), sorted_source_follower_counts, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(f\"Most influential source tweets authors - {os.path.basename(event_path)} event\", loc=\"center\", pad=10)\n",
    "    plt.xlabel(\"Posting hour of source tweet\")\n",
    "    plt.ylabel(f\"Number of followers for most followed user\")\n",
    "    plt.savefig(globals()['pheme_influencers_source_graphs'] + f\"/{os.path.basename(event_path)}_source_influencers.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # PROCESS INFLUENCERS - ALL TWEETS AUTHORS\n",
    "    sorted_posting_hours, sorted_all_follower_counts = sorted_plot_labels_from_dict(max_all_followers_count_per_hour_dict)\n",
    "\n",
    "    plt.bar(range(len(max_all_followers_count_per_hour_dict)), sorted_all_follower_counts, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    long_title = f\"Most influential tweets authors (sources/replies/retweets) - {os.path.basename(event_path)} event\"\n",
    "    # Wrap the title onto multiple lines\n",
    "    wrapped_title = textwrap.fill(long_title, width=50)\n",
    "    plt.title(wrapped_title, loc=\"center\", pad=10)\n",
    "    plt.xlabel(\"Posting hour of tweet (sources/replies/retweets)\")\n",
    "    plt.ylabel(f\"Number of followers for most followed user\")\n",
    "    plt.savefig(globals()['pheme_influencers_all_graphs'] + f\"/{os.path.basename(event_path)}_all_influencers.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # PROCESS AVERAGE FOLLOWERS COUNTS - SOURCE TWEET AUTHORS\n",
    "    sorted_posting_hours, sorted_avg_source_follower_counts = sorted_plot_labels_from_dict(avg_source_followers_count_per_hour_dict)\n",
    "\n",
    "    plt.bar(range(len(avg_source_followers_count_per_hour_dict)), sorted_avg_source_follower_counts, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    long_title = f\"Average followers counts (source tweet authors) - {os.path.basename(event_path)} event\"\n",
    "    # Wrap the title onto multiple lines\n",
    "    wrapped_title = textwrap.fill(long_title, width=50)\n",
    "    plt.title(wrapped_title, loc=\"center\", pad=10)\n",
    "    plt.xlabel(\"Posting hour of source tweet\")\n",
    "    plt.ylabel(f\"Average number of followers\")\n",
    "    plt.savefig(globals()['pheme_avg_followers_counts_source_graphs'] + f\"/{os.path.basename(event_path)}_source_avg_followers_count.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # PROCESS AVERAGE FOLLOWERS COUNTS - ALL TWEET AUTHORS\n",
    "    sorted_posting_hours, sorted_avg_all_follower_counts = sorted_plot_labels_from_dict(avg_all_followers_count_per_hour_dict)\n",
    "\n",
    "    plt.bar(range(len(avg_all_followers_count_per_hour_dict)), sorted_avg_all_follower_counts, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    long_title = f\"Average followers counts (authors of source tweets/replies/retweets) - {os.path.basename(event_path)} event\"\n",
    "    # Wrap the title onto multiple lines\n",
    "    wrapped_title = textwrap.fill(long_title, width=50)\n",
    "    plt.title(wrapped_title, loc=\"center\", pad=10)\n",
    "    plt.xlabel(\"Posting hour of tweet\")\n",
    "    plt.ylabel(f\"Average number of followers\")\n",
    "    plt.savefig(globals()['pheme_avg_followers_counts_all_graphs'] + f\"/{os.path.basename(event_path)}_all_avg_followers_count.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # PROCESS NUMBER OF FOLLOWERS WHO REACTED TO SOURCE TWEETS\n",
    "    sorted_posting_hours, sorted_followings = sorted_plot_labels_from_dict(followings_per_hour_dict)\n",
    "\n",
    "    plt.bar(range(len(followings_per_hour_dict)), sorted_followings, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    # calling the function to add value labels - exact number of followers out of respondents\n",
    "    add_labels_y_value(sorted_posting_hours, sorted_followings)\n",
    "    long_title = f\"Number of followers who reacted to source tweets posted at each hour - {os.path.basename(event_path)} event\"\n",
    "    # Wrap the title onto multiple lines\n",
    "    wrapped_title = textwrap.fill(long_title, width=50)\n",
    "    plt.title(wrapped_title, loc=\"center\", pad=10)\n",
    "    plt.xlabel(\"Posting hour of source tweet\")\n",
    "    plt.ylabel(f\"Number of followers who reacted\")\n",
    "    plt.savefig(globals()['pheme_following_graphs'] + f\"/{os.path.basename(event_path)}_following.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # PROCESS AVERAGE NUMBER OF FOLLOWERS (PER SOURCE TWEET) WHO REACTED TO SOURCE TWEETS\n",
    "    sorted_posting_hours, sorted_avg_followings = sorted_plot_labels_from_dict(avg_followings_per_hour_dict)\n",
    "\n",
    "    plt.bar(range(len(avg_followings_per_hour_dict)), sorted_avg_followings, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    # calling the function to add value labels - exact number of followers out of respondents\n",
    "    add_labels_y_value(sorted_posting_hours, sorted_avg_followings)\n",
    "    long_title = f\"Average number of followers (per source tweet) who reacted to source tweets posted at each hour - {os.path.basename(event_path)} event\"\n",
    "    # Wrap the title onto multiple lines\n",
    "    wrapped_title = textwrap.fill(long_title, width=60)\n",
    "    plt.title(wrapped_title, loc=\"center\", pad=10)\n",
    "    plt.xlabel(\"Posting hour of source tweet\")\n",
    "    plt.ylabel(f\"Average number of followers who reacted (per source tweet)\")\n",
    "    plt.savefig(globals()['pheme_avg_following_graphs'] + f\"/{os.path.basename(event_path)}_avg_following.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    # PROCESS PERCENTAGE OF FOLLOWERS OUT OF USERS WHO REACTED TO SOURCE TWEETS\n",
    "    sorted_posting_hours, sorted_following_percentages = sorted_plot_labels_from_dict(percentages_per_hour_dict)\n",
    "\n",
    "    plt.bar(range(len(percentages_per_hour_dict)), sorted_following_percentages, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    for i in range(len(sorted_posting_hours)):\n",
    "        plt.text(i, sorted_following_percentages[i], f\"{sorted_following_percentages[i]}%\", ha = 'center',\n",
    "                    bbox = dict(facecolor = 'red', alpha =.7, pad=2))\n",
    "    long_title = f\"Percentage of followers out of respondents to source tweets posted at each hour - {os.path.basename(event_path)} event\"\n",
    "    # Wrap the title onto multiple lines\n",
    "    wrapped_title = textwrap.fill(long_title, width=60)\n",
    "    plt.title(wrapped_title, loc=\"center\", pad=10)\n",
    "    plt.xlabel(\"Posting hour of source tweet\")\n",
    "    plt.ylabel(f\"Percentage of followers out of respondents\")\n",
    "    plt.savefig(globals()['pheme_following_percentages_graphs'] + f\"/{os.path.basename(event_path)}_following_percentages.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reaction_counts_influencers_avg_followers_counts_following_and_percentages_per_tweet(events_path):\n",
    "    \"\"\"Function that saves all plots, corresponding to all events, locally.\n",
    "\n",
    "    Args:\n",
    "        events_path (str): path to the root folder of all events in the dataset(e.g. /your/path/to/threads/en)\n",
    "    \"\"\"    \n",
    "    for event_name in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event_name\n",
    "        print(f\"Event: {os.path.basename(event_path)} - drawing plots...\")\n",
    "        plot_reaction_counts_influencers_avg_followers_count_following_and_percentages_per_tweet_event(event_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: germanwings-crash - drawing plots...\n",
      "who-follows-whom.dat file doesn't exist in story 580333909008871424 - skipped this story!\n",
      "who-follows-whom.dat file doesn't exist in story 580360165540642816 - skipped this story!\n",
      "who-follows-whom.dat file doesn't exist in story 580321156508577792 - skipped this story!\n",
      "Event: ottawashooting - drawing plots...\n",
      "Event: ferguson - drawing plots...\n",
      "Event: sydneysiege - drawing plots...\n",
      "Event: charliehebdo - drawing plots...\n",
      "Event: prince-toronto - drawing plots...\n",
      "Event: ebola-essien - drawing plots...\n",
      "Event: putinmissing - drawing plots...\n"
     ]
    }
   ],
   "source": [
    "plot_reaction_counts_influencers_avg_followers_counts_following_and_percentages_per_tweet(events_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
