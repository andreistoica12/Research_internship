{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete all variables in the current environment (if you have already run some cells) - clean state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from kneed import KneeLocator\n",
    "import networkx as nx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the root folder of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir_path = '/home/andreistoica12/research-internship'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with the path to the folder where we store the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/andreistoica12/research-internship/data/PhemeDataset'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: After running the code, some files from the dataset will be different from the original versions, i.e. the \"retweets.json\" files inside each story folder were initially invalid. In order to consider and process the retweets in the longitudinal analysis, I formatted these files so that they are valid, parsable JSON files. If the file contained only one retweet object, it has not been modified. If the file contained multiple retweets, the file now contains a list of retweet objects, separated by a comma, as per the JSON syntax."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paths to the folder containing all subfolders corresponding to each event of major interest (the Charlie Hebdo shooting, footballer Essien having Ebola, etc.). Tweets here are all written in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = data_path + \"/threads/en\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 2 subfolders to store important files and graphs, respectively. If they already existed (from previous runnings of the project), delete the folders and their contents and create empty folders to store the current files and graphs, relevant to the current state of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_path = os.path.join(rootdir_path, 'files')\n",
    "if os.path.exists(files_path):\n",
    "   shutil.rmtree(files_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(files_path)\n",
    "\n",
    "graphs_path = os.path.join(rootdir_path, 'graphs')\n",
    "if os.path.exists(graphs_path):\n",
    "   shutil.rmtree(graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_graphs_path = os.path.join(graphs_path, 'pheme')\n",
    "if os.path.exists(pheme_graphs_path):\n",
    "   shutil.rmtree(pheme_graphs_path, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_graphs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_longitudinal_analysis_graphs = os.path.join(pheme_graphs_path, 'longitudinal-analysis')\n",
    "if os.path.exists(pheme_longitudinal_analysis_graphs):\n",
    "   shutil.rmtree(pheme_longitudinal_analysis_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_longitudinal_analysis_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_reaction_times_graphs = os.path.join(pheme_graphs_path, 'reaction-times')\n",
    "if os.path.exists(pheme_reaction_times_graphs):\n",
    "   shutil.rmtree(pheme_reaction_times_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_reaction_times_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheme_reaction_counts_graphs = os.path.join(pheme_graphs_path, 'reaction-counts')\n",
    "if os.path.exists(pheme_reaction_counts_graphs):\n",
    "   shutil.rmtree(pheme_reaction_counts_graphs, ignore_errors=False, onerror=None)\n",
    "os.makedirs(pheme_reaction_counts_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function that first reads the JSON file and stores it into a dictionary, then parses the date contained at the \"created_at\" key. The number returned is an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_hour(tweet_path):\n",
    "    \"\"\"Function that parses a JSON file associated with a tweet in the PhemeDataset and returns the posting hour.\n",
    "\n",
    "    Args:\n",
    "        tweet_path (str): path to the JSON file associated with a tweet\n",
    "\n",
    "    Returns:\n",
    "        int: posting hour of a tweet\n",
    "    \"\"\"    \n",
    "    with open(tweet_path) as f:\n",
    "        tweet = json.load(f)\n",
    "    date = parser.parse(tweet['created_at'])\n",
    "    \n",
    "    return date.hour"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return the source path, given the story path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_tweet_path(story_path):\n",
    "    \"\"\"Function that, given the path to a story, gets the path to the JSON file corresponding to the source tweet\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        str: path to the source tweet JSON file\n",
    "    \"\"\"    \n",
    "    source_dir_path = story_path + \"/source-tweets\"\n",
    "    source_path = source_dir_path + \"/\" + os.listdir(source_dir_path)[0]\n",
    "    \n",
    "    return source_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of all reactions' paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaction_tweets_paths(story_path):\n",
    "    \"\"\"Function that generates a list of reactions (replies) to a tweet within a story.\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        list: list of paths(strings) for the reactions to the source tweet of a story\n",
    "    \"\"\"    \n",
    "    reactions_paths_list = []\n",
    "    reactions_dir_path = story_path + \"/reactions\"\n",
    "    for reaction_name in os.listdir(reactions_dir_path):\n",
    "        reaction_path = reactions_dir_path + \"/\" + reaction_name\n",
    "        reactions_paths_list.append(reaction_path)\n",
    "        \n",
    "    return reactions_paths_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to validate whether a given JSON file is valid or not. Unmodified retweets files (the ones from teh original dataset) are not valid. We only want ot modify them when they are invalid, otherwise they will become invalid again, as we will have duplicate characters (\"[[\" / \"]]\" / \",,\") ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateJSON(JSON_path):\n",
    "    try:\n",
    "        with open(JSON_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "    except ValueError as err:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to modify/format invalid JSON files for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retweets_json(retweets_path):\n",
    "    if not validateJSON(retweets_path):\n",
    "        with open(retweets_path, 'r') as invalid_json:\n",
    "            data = invalid_json.read()\n",
    "        data = \"[\\n\" + data.replace(\"}\\n{\", \"},\\n{\") + \"]\"\n",
    "        with open(retweets_path,'w') as valid_json:\n",
    "            valid_json.write(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to return a list of the hours when all retweets of a specific story have been posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_retweets(story_path):\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    hours = []\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            hours = [ parser.parse(retweet['created_at']).hour for retweet in retweets_list ]\n",
    "        else:   # we have this case when the JSON file contains one object, but we need to pass a list forward, so we'll have a 1-length list\n",
    "            hours = [parser.parse(retweets_list['created_at']).hour]\n",
    "\n",
    "    return hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define a function to store all occurences of dates (only the hours) in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hours_list_story(story_path):\n",
    "    # I create a list with all occurences of dates corresponding to the source tweet, reactions (replies) and retweets.\n",
    "    hours = []\n",
    "\n",
    "    # source hour\n",
    "    source_path = source_tweet_path(story_path)\n",
    "    hour = tweet_hour(source_path)\n",
    "    hours.append(hour)\n",
    "\n",
    "    # reactions hours\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        hour = tweet_hour(reaction_path)\n",
    "        hours.append(hour)\n",
    "    \n",
    "    # retweets hours\n",
    "    hours.extend(hours_list_retweets(story_path))\n",
    "    \n",
    "    return hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I define a function to return a pandas Series, representing the distribution of the hours of tweets (source tweets, reactions and retweets) posted regarding a specific event given as an input parameter. I chose to convert the list to a pandas Series due to the ease in creating a distribution and corresponding box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_distribution_event(event_path):\n",
    "    hours = []\n",
    "    for story_id in os.listdir(event_path):\n",
    "        story_path = event_path + \"/\" + story_id\n",
    "        hours.extend(hours_list_story(story_path))\n",
    "    hours.sort()\n",
    "    hours_series = pd.Series(hours)\n",
    "    distribution = hours_series.value_counts()[hours_series.unique()]\n",
    "    \n",
    "    return distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is delegated to plot the distribution per hour of the tweets sent about a specific topic/event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event_distribution(event_name, distribution):\n",
    "    axes = distribution.plot(kind='bar')\n",
    "    figure_path = f\"{pheme_longitudinal_analysis_graphs}/{event_name}_distribution.png\"\n",
    "    axes.figure.savefig(figure_path)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot and save in the local graphs/ folder the distributions corresponding to all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_event_distributions(events_path):\n",
    "    for event in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event\n",
    "        distribution = time_distribution_event(event_path)\n",
    "        plot_event_distribution(event, distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_event_distributions(events_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the reaction times to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_types = ['hours', 'minutes']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose between 'hours' and 'minutes' if you want to plot the final graph in hours or minutes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: This variable needs to have either one of the values in the deltas_types list, otherwise the deltas will be an empty list,\n",
    "so the plots will make no sense in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas_type = deltas_types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_story(story_path, deltas_type):\n",
    "    \"\"\"Function that generates a list of time differences between the moment a reaction (reply or retweet) has been posted\n",
    "    and the moment the source tweet of a story has been posted. Depending on the time unit of such time difference \n",
    "    (e.g. 'hours' or 'minutes'), the function outputs a list with numbers expressed in the respective type\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "        deltas_type (str): time unit for time differences\n",
    "\n",
    "    Returns:\n",
    "        list: list of time differences expressed in deltas_type time units\n",
    "    \"\"\"    \n",
    "    if deltas_type not in ['minutes', 'hours']:\n",
    "        print(\"Deltas type doesn't have a valid value - it should be either 'hours' or 'minutes' !\")\n",
    "        return []\n",
    "    \n",
    "    factor = 60 * 60\n",
    "    factor = 60 if deltas_type == 'minutes' else factor\n",
    "\n",
    "    deltas = []\n",
    "\n",
    "    # Step 1: get t0 datetime object from the source timestamp\n",
    "    source_path = source_tweet_path(story_path)\n",
    "    with open(source_path) as file:\n",
    "        source = json.load(file)\n",
    "    t0 = parser.parse(source['created_at'])\n",
    "\n",
    "    # Step 2: for all reactions, get the difference in minutes/hours \n",
    "    # from the time the source was posted and the time each reaction was posted\n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    for reaction_path in reactions_paths_list:\n",
    "        with open(reaction_path) as file:\n",
    "            reaction = json.load(file)\n",
    "        deltas.append((parser.parse(reaction['created_at']) - t0).total_seconds() / factor )\n",
    "\n",
    "    # Step 3: for all retweets, get the same time difference in miuntes as above\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        if type(retweets_list) == list:\n",
    "            deltas.extend([ (parser.parse(retweet['created_at']) - t0).total_seconds() / factor for retweet in retweets_list ])\n",
    "        else:   # here, the JSON file contains one object, but we need to pass a list forward, so we'll have a 1-length list\n",
    "            deltas.extend([ (parser.parse(retweets_list['created_at']) - t0).total_seconds() / factor ])\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_event(event_path, deltas_type):\n",
    "    deltas = []\n",
    "    for story_name in os.listdir(event_path):\n",
    "            story_path = event_path + \"/\" + story_name\n",
    "            deltas.extend(deltas_story(story_path, deltas_type))\n",
    "\n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deltas_all_events(events_path, deltas_type):\n",
    "    deltas_all_events = {}\n",
    "    for event_name in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event_name\n",
    "        deltas_all_events[event_name] = deltas_event(event_path, deltas_type)\n",
    "    \n",
    "    return deltas_all_events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the number of clusters we want to have, as it needs to be passed onto the K-Means algorithm. Therefore, one of the easiest methods is to look for the \"elbow\" point in the plot above, i.e. the point where the rate of decrease in WCSS begins to level off. The optimal number of clusters can be found through teh KneeLocator function within the kneed Python module. Curves with positive concavity are called \"elbows\", the ones with negative concavity are called \"knees\". Our K-Means inertia values are decreasing when we increase the number of clusters, because the inertia is calculated by measuring the distance between each data point and its centroid, so having more clusters means points will be closer to their clusters' centroids. So we will have a decreasing convex curve and thus we will pass on this information as arguments in the KneeLocator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method_k_means_clusters(deltas, max_nr_clusters):\n",
    "    wcss = []   # within-cluster sum of squares\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "    for k in range(1, max_nr_clusters+1):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=0)\n",
    "        kmeans.fit(deltas)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    kn = KneeLocator(range(1, max_nr_clusters+1), wcss, curve='convex', direction='decreasing')\n",
    "\n",
    "    # # Plot the within-cluster sum of squares against the number of clusters\n",
    "    # # If you want to visualize the knee point of the graph, you can plot the graph using the following function, \n",
    "    # # specifically designed to highlight the knee point\n",
    "    # kn.plot_knee()\n",
    "\n",
    "    return kn.knee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(k, deltas):\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    # Create a KMeans object with the specified number of clusters\n",
    "    kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=0)\n",
    "    kmeans.fit(deltas)\n",
    "\n",
    "    # Get the labels assigned to each data point\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Get the centroids of each cluster\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # # Print an overview of the resulting clusters\n",
    "    # for i in range(k):\n",
    "    #     cluster_data = deltas[labels == i]\n",
    "    #     print(f\"Cluster {i+1} has {len(cluster_data)} data points and a centroid of {centroids[i][0]}\")\n",
    "\n",
    "    # Create a dictionary to store the clustered data\n",
    "    clusters_dict = {}\n",
    "    labels_list = []\n",
    "    for label in labels:\n",
    "        labels_list.append(label)\n",
    "    \n",
    "    for index, label in enumerate(labels_list):\n",
    "        if label not in clusters_dict:\n",
    "            clusters_dict[label] = [deltas[index][0]]\n",
    "        else:\n",
    "            clusters_dict[label].append(deltas[index][0])\n",
    "    \n",
    "    return clusters_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation below makes use of the Nearest Neighbors algorithm in order to find the optimal value for epsilon, as per:\n",
    "https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc .\n",
    "However, the distances plot seems odd and the knee obtained through the KneeLocator function doesn't yield the best results. For now, I set the eps parameter to be 0.3 so that we have good clustering results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, I tried to cluster the deltas from all the stories within the Germanwings crash event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = deltas_event('/home/andreistoica12/research-internship/data/PhemeDataset/threads/en/prince-toronto', deltas_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_DBSCAN(deltas):\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=5) # usually, the kth nearest neighbor is chosen somewhere between 3 and 10\n",
    "    nbrs = neigh.fit(deltas)\n",
    "    distances, indices = nbrs.kneighbors(deltas)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    plt.plot(distances)\n",
    "\n",
    "    kn = KneeLocator(range(1, len(distances)+1), distances, curve='convex', direction='increasing')\n",
    "\n",
    "    return float(distances[kn.knee])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the graph seems odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13027777777777816"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_DBSCAN(deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCAN_clustering(deltas):\n",
    "    deltas = np.array(deltas)\n",
    "    deltas = deltas.reshape(-1, 1)\n",
    "\n",
    "    # # Compute the value for epsilon using the function defined previously\n",
    "    # epsilon = epsilon_DBSCAN(deltas)\n",
    "    epsilon = 0.3\n",
    "\n",
    "    # Create a DBSCAN object with epsilon as the computed value and minimum samples=5\n",
    "    dbscan = DBSCAN(eps=epsilon, min_samples=5)\n",
    "\n",
    "    # Fit the DBSCAN object to the data\n",
    "    dbscan.fit(deltas)\n",
    "\n",
    "    # Get the labels assigned to each data point\n",
    "    labels = dbscan.labels_\n",
    "\n",
    "    # # Print the number of clusters and the labels assigned to each data point\n",
    "    # n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    # print(\"Number of clusters:\", n_clusters)\n",
    "    # print(\"Labels:\", labels)\n",
    "\n",
    "    # Create a dictionary to store the clusters\n",
    "    clusters_dict = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label in clusters_dict:\n",
    "            clusters_dict[label].append(deltas[i][0])\n",
    "        else:\n",
    "            clusters_dict[label] = [deltas[i][0]]\n",
    "    \n",
    "    return clusters_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, I have implemented 2 ways to cluster the deltas. By default, I use the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), because it focuses on the patterns that we observe from the majority of data points, and the outliers don't affect the way it clusters the other points, whereas the K-Means algorithm builds its centroids based on all the data points and it may be deceiving in a plot. Moreover, plots obtained with the DBSCAN clustering algorithm tend to yield more and tighter clusters than the number of clusters obtained with the help of the Elbow Method in the case of the K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algorithms = ['k-means', 'DBSCAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_algorithm = clustering_algorithms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_all_events(events_path, deltas_type, clustering_algorithm):\n",
    "    all_events_deltas = deltas_all_events(events_path, deltas_type)\n",
    "    all_events_clusters = {}\n",
    "    for event_name, event_deltas in all_events_deltas.items():\n",
    "        if clustering_algorithm == 'k-means':\n",
    "            # loop over maximum 10 clusters, as this is a range where you usually find the optimum number of clusters\n",
    "            k = elbow_method_k_means_clusters(event_deltas, 10)\n",
    "            # print(f\"\\nEvent {event_name}:\")\n",
    "            # print(f\"NOTE: Numbers represent {deltas_type}\\n\")\n",
    "            all_events_clusters[event_name] = k_means_clustering(k, event_deltas)\n",
    "        elif clustering_algorithm == 'DBSCAN':\n",
    "            all_events_clusters[event_name] = DBSCAN_clustering(event_deltas)\n",
    "        \n",
    "    return all_events_clusters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that receives the first clusters generated either through K-Means or DBSCAN and tries to see whether it can find some subgraphs more relevant for the visualization. A recursive function is defined in order to re-cluster the biggest cluster until the newly-computed biggest cluster contains fewer than 70% of the data points. I considered this a good threshold to stop, because the graph will clearly show, on one hand, which is the overwhelingly biggest cluster, and on the other hand, the next number_of_clusters - 1 biggest clusters/subclusters so we have a better overview of the most important clusters/subclusters in our datasets. I chose to apply K-Means for the reapplication of clustering on the biggest cluster due to its computationally-efficient nature, which is hands-down superior to DBSCAN's. In fact, not only did the algorithm implemented with DBSCAN for subclusters take a lot longer, it even crashed before the end, due to the number of recursive calls exceeding the limit of the kernel. Therefore, considering the trade-off between better clustering and efficiency, the option below yielded the optimal results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides technical reasons, I opted to first cluster the points with DBSCAN, due to its ability to ignore outlier points, they are not taken into account when clusters are created, which is of great help for a good visualization. After the first clusterization, we only have relevant points, so K-Means will yield good results (its problem were outlier points which influenced the computation of centroids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_for_plot(clusters, number_of_clusters):\n",
    "    total_length = 0\n",
    "    for value in clusters.values():\n",
    "            total_length += len(value)\n",
    "    subclusters_for_plot = {}\n",
    "    \n",
    "    def subclusters(clusters):\n",
    "            if(len(clusters)):\n",
    "                key_for_biggest_cluster = max(clusters, key=lambda k: len(clusters[k]))\n",
    "                if len(clusters[key_for_biggest_cluster]) / total_length < 0.7:\n",
    "                    print(\"Reached base case.\")\n",
    "                    return clusters\n",
    "                else:\n",
    "                    # print(\"We'll go back into subclusters function.\")\n",
    "                    # print(f\"Length of current clusters: {len(clusters)}\")\n",
    "                    # print(\"Current clusters look like this:\")\n",
    "                    # for key, value in clusters.items():\n",
    "                    #      print(f\"{key}: {math.floor(min(value))} - {math.ceil(max(value))}\")\n",
    "\n",
    "                    biggest_cluster = clusters[key_for_biggest_cluster]\n",
    "                    # print(\"Biggest cluster:\")\n",
    "                    # print(biggest_cluster[:3])\n",
    "                    \n",
    "                    del clusters[key_for_biggest_cluster]\n",
    "                    # print(\"Clusters after removing biggest_cluster:\")\n",
    "                    # for key, value in clusters.items():\n",
    "                    #      print(f\"{key}: {math.floor(min(value))} - {math.ceil(max(value))}\")\n",
    "                    \n",
    "                    # K-MEANS CLUSTERING\n",
    "                    k = elbow_method_k_means_clusters(biggest_cluster, 10)\n",
    "                    # print(f\"k = {k}\")\n",
    "                    \n",
    "                    subcl = k_means_clustering(k, biggest_cluster)\n",
    "                    # print(\"subcl - the subclusters dictionary obtained from the biggest_cluster looks like this:\")\n",
    "                    # for key, value in subcl.items():\n",
    "                    #      print(f\"{key}: {math.floor(min(value))} - {math.ceil(max(value))}\")\n",
    "                    \n",
    "                    for key, value in subcl.items():\n",
    "                        clusters[max(clusters.keys())+1] = value\n",
    "                    \n",
    "                    # print(f\"Length of clusters after modifications: {len(clusters)}\")\n",
    "                    # print(\"Clusters look like this:\")\n",
    "                    # for key, value in clusters.items():\n",
    "                    #      print(f\"{key}: {math.floor(min(value))} - {math.ceil(max(value))}\")\n",
    "                    \n",
    "                    subclusters(clusters)\n",
    "            else:\n",
    "                print(\"Clusters dictionary is empty.\")\n",
    "                return clusters\n",
    "            \n",
    "\n",
    "    subclusters(clusters)\n",
    "\n",
    "    if len(clusters) >= number_of_clusters:\n",
    "        for i in range(number_of_clusters):\n",
    "            key_for_biggest_cluster = max(clusters, key=lambda k: len(clusters[k]))\n",
    "            subclusters_for_plot[i] = clusters[key_for_biggest_cluster]\n",
    "            del clusters[key_for_biggest_cluster]\n",
    "    else:\n",
    "        subclusters_for_plot = clusters\n",
    "    \n",
    "    return subclusters_for_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reaction_times(events_path, deltas_type):\n",
    "    all_clusters = clusters_all_events(events_path, deltas_type, clustering_algorithm)\n",
    "\n",
    "    for event_name, clusters in all_clusters.items():\n",
    "        print(f\"Event: {event_name}\")\n",
    "        clusters_to_be_plotted = clusters_for_plot(clusters, 5)\n",
    "\n",
    "        final_clusters_for_plot = {}\n",
    "        for key, value in clusters_to_be_plotted.items():\n",
    "            final_clusters_for_plot[f\"{math.floor(min(value))} - {math.ceil(max(value))}\"] = len(value)\n",
    "\n",
    "        intervals_unsorted = list(final_clusters_for_plot.keys())\n",
    "        values_unsorted = list(final_clusters_for_plot.values())\n",
    "        df_plot = pd.DataFrame(\n",
    "            dict(\n",
    "                Interval=intervals_unsorted,\n",
    "                Value=values_unsorted\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        df_plot['Start of interval'] = df_plot['Interval'].str.split(' - ').str[0]\n",
    "        df_plot['Start of interval'] = df_plot['Start of interval'].apply(pd.to_numeric) \n",
    "        df_plot_sorted = df_plot.sort_values('Start of interval')\n",
    "        intervals = list(df_plot_sorted['Interval'])\n",
    "        values = list(df_plot_sorted['Value'])\n",
    "\n",
    "\n",
    "        plt.bar(range(len(final_clusters_for_plot)), values, tick_label=intervals)\n",
    "        # Rotate the x-axis labels by 45 degrees\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(f'Distribution of reaction times in {deltas_type}')\n",
    "        plt.xlabel(f'Reaction times (between x and y {deltas_type})')\n",
    "        plt.ylabel('Number of reactions')\n",
    "        plt.savefig(pheme_reaction_times_graphs + f\"/{event_name}_reaction_times.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the cell below runs for ~22 sec on my machine if the session has just started. If you run the code again, it takes ~18 sec, due to probably cached information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: germanwings-crash\n",
      "Reached base case.\n",
      "Event: ottawashooting\n",
      "Reached base case.\n",
      "Event: ferguson\n",
      "Reached base case.\n",
      "Event: sydneysiege\n",
      "Reached base case.\n",
      "Event: charliehebdo\n",
      "Reached base case.\n",
      "Event: prince-toronto\n",
      "Reached base case.\n",
      "Event: ebola-essien\n",
      "Reached base case.\n",
      "Event: putinmissing\n",
      "Reached base case.\n"
     ]
    }
   ],
   "source": [
    "plot_reaction_times(events_path, deltas_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: see how to make K-Means have all uninterfering deltas (if I can even do that)\n",
    "# (e.g. in cluser 1, centroid 2, there is a delta of 4.4 and in cluster 2, centroid 15, there is a delta of 3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: document new functions, explain how I sorted the columns in graphs for all events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: see what DBSCAN does exactly and how it is different than K-Means (we can see sub clusters, we usually have more clusters etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: explain the function for creating subclusters of the biggest clusters and choosing the most important ones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reactions counts for the source tweets - the number of reactions per tweet posted at hour x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reactions_count_for_source_tweet(story_path):\n",
    "    \"\"\"Function that generates the number of reactions (replies or retweets) of a single source tweet \n",
    "    (a story contains one source tweet).\n",
    "\n",
    "    Args:\n",
    "        story_path (str): path to the root folder of a story (e.g. /your/path/to/charliehebdo/552783667052167168)\n",
    "\n",
    "    Returns:\n",
    "        int: number of reactions for a source tweet\n",
    "    \"\"\"    \n",
    "    reactions_paths_list = reaction_tweets_paths(story_path)\n",
    "    reactions_count = len(reactions_paths_list)\n",
    "\n",
    "    retweets_path = story_path + \"/retweets.json\"\n",
    "    if os.path.exists(retweets_path):\n",
    "        format_retweets_json(retweets_path)\n",
    "        with open(retweets_path, 'r') as file:\n",
    "            retweets_list = json.load(file)\n",
    "        reactions_count += len(retweets_list)\n",
    "    \n",
    "    return reactions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaction_counts_per_tweet_event(event_path):\n",
    "    \"\"\"Function that generates the dictionary of reaction counts per tweet per hour.\n",
    "     Based on this dictionary, a plot can be made.\n",
    "\n",
    "    Args:\n",
    "        event_path (str): path to the root folder of an event (e.g. /your/path/to/charliehebdo)\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary with keys as posting hours of source tweets and values as the number of reactions per tweet per key(hour)\n",
    "    \"\"\"    \n",
    "    number_of_source_tweets_per_hour = {}\n",
    "    reactions_count_per_hour = {}\n",
    "    for story_id in os.listdir(event_path):\n",
    "        story_path = event_path + \"/\" + story_id\n",
    "        source_path = source_tweet_path(story_path)\n",
    "        source_hour = tweet_hour(source_path)\n",
    "        if source_hour in number_of_source_tweets_per_hour:\n",
    "            number_of_source_tweets_per_hour[source_hour] += 1\n",
    "        else:\n",
    "            number_of_source_tweets_per_hour[source_hour] = 1\n",
    "        if source_hour in reactions_count_per_hour:\n",
    "            reactions_count_per_hour[source_hour] += reactions_count_for_source_tweet(story_path)\n",
    "        else:\n",
    "            reactions_count_per_hour[source_hour] = reactions_count_for_source_tweet(story_path)\n",
    "\n",
    "    reactions_per_tweet = {}\n",
    "    for source_hour in number_of_source_tweets_per_hour:\n",
    "        reactions_per_tweet[source_hour] = round(reactions_count_per_hour[source_hour] / number_of_source_tweets_per_hour[source_hour])\n",
    "\n",
    "    return reactions_per_tweet\n",
    "    # COMMENT THE FIRST RETURN STATEMENT AND UNCOMMENT THE SECOND TO RETURN A LIST WITH 2 DICTIONARIES:\n",
    "    # - the reaction counts per tweet\n",
    "    # - the absolute reaction counts\n",
    "    # return [reactions_per_tweet, reactions_count_per_hour]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add value labels - adds the value of y\n",
    "def add_labels_y_value(x,y):\n",
    "    \"\"\"Function that takes the x and y-axis to be passed onto a plot function and generates labels,\n",
    "    such that on top of each y value, it is displayed centrally.\n",
    "\n",
    "    Args:\n",
    "        x (list): list of labels for x-axis of a plot\n",
    "        y (list): list of values for y-axis of a plot\n",
    "    \"\"\"    \n",
    "    for i in range(len(x)):\n",
    "        plt.text(i, y[i], y[i], ha = 'center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_labels_percentage(x, y):\n",
    "    \"\"\"Function that takes the x and y-axis to be passed onto a plot function and generates labels,\n",
    "    such that on top of each y value, the percentage of the y value out of the sum of all y-s is displayed as a red bounding box.\n",
    "    Useful when y-s represent the counts of occurences for some values.\n",
    "\n",
    "    Args:\n",
    "        x (list): list of labels for x-axis of a plot\n",
    "        y (list): list of values for y-axis of a plot\n",
    "    \"\"\"    \n",
    "    for i in range(len(x)):\n",
    "        percentage = y[i] / sum(y) * 100\n",
    "        plt.text(i, y[i], f\"{round(percentage, 1)}%\", ha = 'center',\n",
    "                 bbox = dict(facecolor = 'red', alpha =.7, pad=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reaction_counts_per_tweet_event(event_path):\n",
    "    \"\"\"Function that creates and saves locally a plot for an event in the dataset. The plots shows the number of reaction counts\n",
    "    (replies or retweets) per tweet posted at an hour indicated by the x-axis labels.\n",
    "\n",
    "    Args:\n",
    "        event_path (str): path to the root folder of an event (e.g. /your/path/to/charliehebdo)\n",
    "    \"\"\"    \n",
    "    reactions_per_tweet_dict = reaction_counts_per_tweet_event(event_path)\n",
    "\n",
    "    # sort the dictionary by keys\n",
    "    sorted_reactions_per_tweet_dict = sorted(reactions_per_tweet_dict.items())\n",
    "\n",
    "    # extract the sorted keys and values\n",
    "    sorted_posting_hours = [k for k, v in sorted_reactions_per_tweet_dict]\n",
    "    sorted_counts = [v for k, v in sorted_reactions_per_tweet_dict]\n",
    "\n",
    "    plt.bar(range(len(reactions_per_tweet_dict)), sorted_counts, tick_label=sorted_posting_hours)\n",
    "    # Rotate the x-axis labels by 45 degrees\n",
    "    plt.xticks(rotation=45)\n",
    "    # calling the function to add value labels\n",
    "    add_labels_y_value(sorted_posting_hours, sorted_counts)\n",
    "    graph_type = 'per tweet'\n",
    "    plt.title(f\"Reaction counts {graph_type} - {os.path.basename(event_path)} event\")\n",
    "    plt.xlabel(\"Posting hour of source tweet\")\n",
    "    plt.ylabel(f\"Number of reactions {graph_type}\")\n",
    "    plt.savefig(pheme_reaction_counts_graphs + f\"/{os.path.basename(event_path)}_reaction_counts_{graph_type}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # COMMENT THE FIRST PART OF THE FUNCTION AND UNCOMMENT THIS PART TO CREATE 2 GRAPHS:\n",
    "    # - reaction counts per tweet per hour\n",
    "    # - absolute reaction counts per hour\n",
    "    # NOTE: you should also return a list in the reaction_counts_per_tweet_event() function \n",
    "    # (comment out the current return statement and uncomment the other)\n",
    "    \n",
    "    # [reactions_per_tweet_dict, reactions_count_per_hour_dict] = reactions_per_tweet_event(event_path)\n",
    "    # results = [reactions_per_tweet_dict, reactions_count_per_hour_dict]\n",
    "    # for i in range(len(results)):\n",
    "    #     dictionary = results[i]\n",
    "    #     # sort the dictionary by keys\n",
    "    #     sorted_reactions_per_tweet_dict = sorted(dictionary.items())\n",
    "\n",
    "    #     # extract the sorted keys and values\n",
    "    #     sorted_posting_hours = [k for k, v in sorted_reactions_per_tweet_dict]\n",
    "    #     sorted_counts = [v for k, v in sorted_reactions_per_tweet_dict]\n",
    "\n",
    "    #     plt.bar(range(len(reactions_per_tweet_dict)), sorted_counts, tick_label=sorted_posting_hours)\n",
    "    #     # Rotate the x-axis labels by 45 degrees\n",
    "    #     plt.xticks(rotation=45)\n",
    "    #     # calling the function to add value labels\n",
    "    #     add_labels_y_value(sorted_posting_hours, sorted_counts)\n",
    "    #     graph_type = 'absolute'\n",
    "    #     graph_type = 'per tweet' if i == 0 else graph_type\n",
    "    #     plt.title(f\"Reaction counts {graph_type} - {os.path.basename(event_path)} event\")\n",
    "    #     plt.xlabel(\"Posting hour of source tweet\")\n",
    "    #     plt.ylabel(f\"Number of reactions {graph_type}\")\n",
    "    #     plt.savefig(pheme_reaction_counts_graphs + f\"/{os.path.basename(event_path)}_reaction_counts_{graph_type}.png\")\n",
    "    #     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reaction_counts_per_tweet_all_events(events_path):\n",
    "    \"\"\"Function that saves all plots, corresponding to all events, locally.\n",
    "\n",
    "    Args:\n",
    "        events_path (str): path to the root folder of all events in the dataset(e.g. /your/path/to/threads/en)\n",
    "    \"\"\"    \n",
    "    for event_name in os.listdir(events_path):\n",
    "        event_path = events_path + \"/\" + event_name\n",
    "        plot_reaction_counts_per_tweet_event(event_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reaction_counts_per_tweet_all_events(events_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243101c165aceacaf115b49fc146265224cf91574f24df3021157e0d2dabdb2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
